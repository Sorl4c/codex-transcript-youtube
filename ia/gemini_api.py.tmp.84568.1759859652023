# ia/gemini_api.py
import os
import time
import google.generativeai as genai
from typing import Dict, Any, Tuple, Optional
import requests
import logging

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- Available Gemini Models (for reference) ---
# Flash Models (faster, lower cost):
# - models/gemini-1.5-flash
# - models/gemini-1.5-flash-latest
# - models/gemini-1.5-flash-002
# - models/gemini-2.5-flash
#
# Pro Models (more powerful, higher cost):
# - models/gemini-1.5-pro
# - models/gemini-1.5-pro-latest
# - models/gemini-2.5-pro
#
# Vision Models (for image/video input):
# - models/gemini-pro-vision
#
# --- Future Implementation Idea ---
# TODO: If a rate limit error (429) is detected, automatically attempt the call
# with a fallback model (e.g., from a 'pro' model to a 'flash' model).
# This would require modifying `make_api_call_with_retry` to handle a list of models.

# Pricing for Gemini models (per 1000 tokens)
# Source: https://ai.google.dev/pricing (accessed 2025-07-08)
MODEL_PRICING = {
    "gemini-1.5-flash": {
        "input": 0.00035, # $0.35 / 1M tokens
        "output": 0.00105 # $1.05 / 1M tokens
    },
    "gemini-1.5-flash-latest": {
        "input": 0.00035,
        "output": 0.00105
    },
    "gemini-1.5-pro": {
        "input": 0.0, # Currently free
        "output": 0.0  # Currently free
    },
    "gemini-pro": {  # Alias for backward compatibility
        "input": 0.0,
        "output": 0.0
    },
    "default": {
        "input": 0.0,
        "output": 0.0
    }
}

# Default model
GEMINI_MODEL_NAME = "gemini-2.0-flash-exp"  # Modelo experimental más reciente

def configure_gemini(api_key: str = None) -> bool:
    """Configures the Gemini API with the provided key or from environment variable."""
    if api_key is None:
        api_key = os.environ.get("GEMINI_API_KEY")
    
    if not api_key:
        print("❌ Error: GEMINI_API_KEY not found. Please set it as an environment variable or pass it as an argument.")
        return False
    
    try:
        genai.configure(api_key=api_key)
        return True
    except Exception as e:
        print(f"❌ Error configuring Gemini API: {e}")
        return False

def count_tokens_gemini(model_name: str, text: str, api_key: str = None) -> int:
    """Counts the number of tokens in a given text for a specific Gemini model."""
    if not configure_gemini(api_key):
        return 0 # Configuration failed
    try:
        model = genai.GenerativeModel(model_name)
        return model.count_tokens(text).total_tokens
    except Exception as e:
        print(f"⚠️ Warning: Could not count tokens using Gemini API for model {model_name}: {e}")
        # Fallback: estimate based on characters (very rough)
        return len(text) // 4 


def make_api_call_with_retry(model, prompt: str, max_retries: int = 3, initial_delay: float = 2.0) -> Any:
    """Realiza una llamada a la API con reintentos y backoff exponencial."""
    delay = initial_delay
    last_exception = None
    
    for attempt in range(max_retries):
        try:
            response = model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.7,
                    top_p=1.0,
                    top_k=32,
                    max_output_tokens=8192
                )
            )
            return response
            
        except Exception as e:
            last_exception = e
            logger.warning(f"Intento {attempt + 1} fallido: {str(e)}")
            if attempt < max_retries - 1:
                time.sleep(delay)
                delay *= 2  # Backoff exponencial
    
    # Si llegamos aquí, todos los intentos fallaron
    raise last_exception if last_exception else Exception("Error desconocido en la llamada a la API")

def summarize_text_gemini(
    text_content: str,
    prompt_template: str,
    model_name: str = GEMINI_MODEL_NAME,
    api_key: str = None,
    generation_config_override: Dict[str, Any] = None,
    safety_settings_override: Dict[str, Any] = None,
    timeout: int = 120  # Tiempo máximo en segundos para la llamada a la API
) -> Dict[str, Any]:
    """
    Summarizes text using the Gemini API with improved error handling and timeouts.

    Args:
        text_content: The text to summarize.
        prompt_template: The prompt template (must include '{text}').
        model_name: The Gemini model to use.
        api_key: The Gemini API key.
        generation_config_override: Optional dictionary to override default generation config.
        safety_settings_override: Optional dictionary to override default safety settings.
        timeout: Maximum time in seconds to wait for the API response.

    Returns:
        A dictionary containing the summary, token counts, time, and cost.
    """
    start_time = time.time()
    
    if not configure_gemini(api_key):
        error_msg = "Error: Gemini API not configured."
        logger.error(error_msg)
        return {
            "summary_text": error_msg,
            "input_tokens": 0,
            "output_tokens": 0,
            "api_call_time_seconds": time.time() - start_time,
            "cost": 0.0,
            "error": "API configuration failed"
        }

    full_prompt = prompt_template.format(text=text_content)
    logger.info(f"Iniciando generación de resumen con modelo {model_name}")
    
    try:
        # Configurar el modelo
        model = genai.GenerativeModel(model_name)
        
        # Configuración de generación
        generation_config = {
            "temperature": 0.7,
            "top_p": 1.0,
            "top_k": 32,
            "max_output_tokens": 8192,
        }
        if generation_config_override:
            generation_config.update(generation_config_override)

        # Configuración de seguridad
        safety_settings = {}
        if safety_settings_override:
            safety_settings.update(safety_settings_override)
        
        # Realizar la llamada con timeout
        api_call_start_time = time.time()
        
        # Usar un hilo para implementar el timeout
        from concurrent.futures import ThreadPoolExecutor, TimeoutError as ThreadTimeoutError
        
        with ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(
                make_api_call_with_retry, 
                model=model,
                prompt=full_prompt
            )
            
            try:
                response = future.result(timeout=timeout)
                api_call_time_seconds = time.time() - api_call_start_time
                
                if not hasattr(response, 'text') or not response.text:
                    raise ValueError("La respuesta de la API no contiene texto")
                    
                summary_text = response.text
                
                # Obtener conteo de tokens
                input_tokens = model.count_tokens(full_prompt).total_tokens
                output_tokens = model.count_tokens(summary_text).total_tokens
                
                # Calcular costo dinámicamente
                pricing = MODEL_PRICING.get(model_name, MODEL_PRICING['default'])
                cost = (input_tokens / 1000 * pricing['input']) + \
                       (output_tokens / 1000 * pricing['output'])
                
                logger.info(f"Resumen generado exitosamente en {api_call_time_seconds:.2f}s. "
                           f"Tokens: {input_tokens} in, {output_tokens} out")
                
                return {
                    "summary_text": summary_text,
                    "input_tokens": input_tokens,
                    "output_tokens": output_tokens,
                    "api_call_time_seconds": api_call_time_seconds,
                    "cost": cost,
                    "model_name": model_name,
                    "error": None
                }
                
            except ThreadTimeoutError:
                future.cancel()
                error_msg = f"Tiempo de espera agotado ({timeout}s) para la API de Gemini"
                logger.error(error_msg)
                raise TimeoutError(error_msg)
                
    except Exception as e:
        error_msg = f"Error en la llamada a la API de Gemini: {str(e)}"
        logger.error(error_msg, exc_info=True)
        
        # Calcular tokens de entrada si es posible
        input_tokens = 0
        if 'full_prompt' in locals():
            try:
                input_tokens = model.count_tokens(full_prompt).total_tokens
            except:
                pass
        
        return {
            "summary_text": f"Error: {str(e)}",
            "input_tokens": input_tokens,
            "output_tokens": 0,
            "api_call_time_seconds": time.time() - start_time,
            "cost": 0.0,
            "model_name": model_name,
            "error": str(e)
        }

if __name__ == '__main__':
    print("--- Testing Gemini API ---")
    sample_text = """
    Artificial intelligence (AI) is intelligence demonstrated by machines,
    as opposed to the natural intelligence displayed by humans and animals.
    Leading AI textbooks define the field as the study of "intelligent agents":
    any device that perceives its environment and takes actions that maximize its
    chance of successfully achieving its goals. Some popular accounts use the
    term "artificial intelligence" to describe machines that mimic "cognitive"
    functions that humans associate with the human mind, such as "learning"
    and "problem solving", however, this definition is rejected by major AI researchers.
    """
    sample_prompt = "Summarize the following text in one sentence: {text}"

    # Ensure GEMINI_API_KEY is set as an environment variable for this test
    if not os.environ.get("GEMINI_API_KEY"):
        print("\n⚠️ GEMINI_API_KEY environment variable not set. Skipping live API test.")
        print("Please set it to run the test against the Gemini API.")
    else:
        print(f"Attempting to use model: {GEMINI_MODEL_NAME}")
        
        # Test token counting
        # input_tok_count = count_tokens_gemini(GEMINI_MODEL_NAME, sample_prompt.format(text=sample_text))
        # print(f"Estimated input tokens for sample: {input_tok_count}")

        result = summarize_text_gemini(sample_text, sample_prompt)

        print("\n--- Summary Result ---")
        if result.get("error"):
            print(f"Error: {result['error']}")
        else:
            print(f"Summary: {result['summary_text']}")
        
        print(f"Input Tokens: {result['input_tokens']}")
        print(f"Output Tokens: {result['output_tokens']}")
        print(f"API Call Time: {result['api_call_time_seconds']:.2f}s")
        print(f"Estimated Cost: ${result['cost']:.6f}")
        print(f"Model Used: {result.get('model_name', 'N/A')}")

    print("\n--- Gemini API test finished ---")
