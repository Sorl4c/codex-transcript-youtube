# M√≥dulo de IA - YouTube Subtitle Downloader

Este documento proporciona instrucciones para configurar y utilizar el m√≥dulo de IA para resumen de texto dentro del proyecto YouTube Subtitle Downloader.

**Estado Actual (v2.7.1):**
- ‚úÖ M√≥dulo RAG (Retrieval-Augmented Generation) con soporte para b√∫squeda sem√°ntica
- ‚úÖ Almacenamiento vectorial eficiente con SQLite + sqlite-vec
- ‚úÖ Generaci√≥n de embeddings con modelos locales (sentence-transformers)
- ‚úÖ Soporte para CPU/GPU con CUDA (RTX 5070 Ti verificada)
- ‚úÖ Procesamiento por lotes de documentos
- ‚úÖ Sistema avanzado de chunking con m√∫ltiples estrategias:
  - ‚úÖ Chunking por caracteres (tradicional) con solapamiento configurable
  - ‚úÖ Chunking por palabras respetando l√≠mites l√©xicos
  - ‚úÖ Chunking sem√°ntico basado en estructura natural (p√°rrafos/frases)
  - ‚úÖ **Chunking Agentic con LLMs (Gemini/Local) totalmente funcional y depurado**

- ‚úÖ Soporte para m√∫ltiples modelos GGUF (TinyLlama, Mistral, Mixtral, LLaMA 2)
- ‚úÖ Integraci√≥n con Gemini API para res√∫menes en la nube
- ‚úÖ C√°lculo autom√°tico de costos por token para Gemini
- ‚úÖ Pipeline nativo optimizado con soporte para chunking configurable
- ‚úÖ Integraci√≥n completa con LangChain
- ‚úÖ Sistema de benchmarking comparativo entre modelos locales y en la nube
- ‚úÖ Generaci√≥n de informes detallados en Markdown con m√©tricas de costo-efectividad
- ‚úÖ Sistema de pruebas unitarias completo
- ‚úÖ Manejo de errores y validaci√≥n robustos
- ‚úÖ Soporte multiling√ºe mejorado
- ‚úÖ Gesti√≥n avanzada de v√≠deos con interfaz intuitiva

> **Nota sobre GPU**: Se ha verificado el funcionamiento con CUDA 12.9. Para m√°s detalles, consulta la [Gu√≠a de configuraci√≥n CUDA/WSL2](./GUIA_CUDA_WSL.md).

## Modos de Uso: CLI y GUI

Actualmente existen dos formas principales de interacci√≥n:

### CLI (Command Line Interface)
- **Estado:** Experimental. El script `summarize_transcript.py` sirve para pruebas de prompt y ajuste de par√°metros.
- **Limitaci√≥n:** No soporta procesamiento por lotes (solo un archivo/transcripci√≥n por ejecuci√≥n).
- **Ejemplo:**
  ```bash
  python ia/summarize_transcript.py -i entrada.txt -o salida.txt --max-tokens 2048
  ```
- **Uso t√≠pico:** Ajuste de prompts, pruebas r√°pidas desde terminal.

### Interfaz Web con Streamlit
- **Estado:** Implementaci√≥n principal y recomendada para uso interactivo
- **Caracter√≠sticas principales:**
  - Interfaz web moderna y responsiva accesible desde cualquier navegador
  - Gesti√≥n avanzada de v√≠deos con vista previa en tiempo real
  - Soporte para procesamiento por lotes con modelos de IA (Gemini, modelos locales)
  - Panel de depuraci√≥n integrado para diagn√≥stico
  - Filtros por canal y b√∫squeda de v√≠deos
  - Visualizaci√≥n y edici√≥n de metadatos y res√∫menes
  - Exportaci√≥n de datos en formato JSON o CSV

> **Nota:** La interfaz Streamlit est√° orientada a la gesti√≥n y an√°lisis de v√≠deos. El benchmarking de modelos solo est√° disponible por CLI, no desde la interfaz web.

### GUI Legacy (tkinter)
- **Estado:** Mantenimiento, se recomienda migrar a la interfaz Streamlit
- **Caracter√≠sticas:**
  - Aplicaci√≥n de escritorio independiente
  - Soporte b√°sico para gesti√≥n de v√≠deos
  - Ordenaci√≥n por columnas
  - Procesamiento por lotes

---

> **NOTA:** El pr√≥ximo objetivo es unificar la UX: al procesar un v√≠deo, guardar el transcript completo (como ya se hace) y, adem√°s, guardar el resumen generado en un nuevo campo de la base de datos. 
> 
> **PR√ìXIMO OBJETIVO: Implementaci√≥n de RAG**
> 
> El siguiente gran hito del proyecto es la implementaci√≥n de un sistema **Retrieval-Augmented Generation (RAG)**. El objetivo es crear un motor de b√∫squeda sem√°ntica que permita a los usuarios hacer preguntas en lenguaje natural sobre el contenido de los v√≠deos almacenados y obtener respuestas precisas y contextualizadas, citando las fuentes originales.
> 
> **Nota:** Con la estabilizaci√≥n de la interfaz de Streamlit, el trabajo en la UI se considera pausado para centrar todos los esfuerzos en esta nueva funcionalidad.

## Arquitectura Modular y Flujo T√≠pico

1. Descarga de v√≠deo/audio (yt-dlp)
2. Transcripci√≥n autom√°tica (whisper/whisper.cpp)
3. Procesamiento/resumen (CLI o GUI, ambos usan microservicio LLM local)
4. Exportaci√≥n y visualizaci√≥n de resultados

---

## Microservicio LLM Local (OpenAI Compatible)

Servicio de alto rendimiento para servir modelos GGUF locales a trav√©s de una API HTTP compatible con OpenAI. Optimizado para procesamiento de contexto largo y uso eficiente de GPU.

**üöÄ Caracter√≠sticas Principales**
- **API 100% compatible con OpenAI** - Usa los mismos endpoints y formatos de petici√≥n/respuesta
- **Soporte para modelos avanzados** - Qwen2-7B, Mistral, TinyLlama y m√°s
- **Contexto extenso** - Hasta 10,000 tokens probados sin p√©rdida de calidad
- **Alto rendimiento** - 300-400 tokens/segundo en RTX 5070 Ti
- **Streaming eficiente** - Respuestas token por token con baja latencia
- **Configuraci√≥n flexible** - Ajustes v√≠a variables de entorno

**üìä Rendimiento Verificado**
- **Respuestas cortas**: < 2 segundos
- **Textos largos (8000+ tokens)**: ~6 segundos
- **Uso de GPU**: Optimizado para carga completa en VRAM
- **Concurrencia**: M√∫ltiples peticiones simult√°neas con gesti√≥n de recursos

**üîß Casos de Uso Verificados**
- Res√∫menes de texto (50-300 palabras)
- Clasificaci√≥n tem√°tica
- Procesamiento de documentos largos
- Generaci√≥n de contenido estructurado

**üìö Documentaci√≥n Detallada**
Para configuraci√≥n avanzada, ejemplos de uso y soluci√≥n de problemas, consulta:
‚û°Ô∏è **[Documentaci√≥n Completa del Microservicio](../llm_service/README.md)**

## 1.1 Requisitos para Modelos Locales

Antes de comenzar, aseg√∫rate de tener lo siguiente:

- **Python**: Versi√≥n 3.8 o superior.
- **WSL (Windows Subsystem for Linux)**: **Recomendado para aceleraci√≥n GPU**.
    - Para verificar si WSL est√° instalado y tu distribuci√≥n, ejecuta en PowerShell o CMD: `wsl --list --verbose`
    - Aseg√∫rate de usar WSL 2.
- **GPU NVIDIA (opcional)**: Para aceleraci√≥n GPU, necesitar√°s una GPU compatible con CUDA (ej: GeForce RTX 5070 Ti).
- **Controladores NVIDIA**: √öltimos controladores instalados en Windows.
- **Toolkit CUDA (solo si usas GPU)**: Debe instalarse dentro de WSL.
    - Verifica con: `nvidia-smi` y `nvcc --version`
    - Sigue las gu√≠as oficiales de NVIDIA para instalar CUDA Toolkit en WSL si es necesario.

### 1.2 Requisitos para Gemini API

Para utilizar la integraci√≥n con Gemini API:

1. **Cuenta de Google Cloud**: Necesitar√°s una cuenta de Google Cloud con facturaci√≥n habilitada.
2. **Clave de API de Gemini**: 
   - Ve a [Google AI Studio](https://makersuite.google.com/app/apikey)
   - Crea una nueva clave de API
   - Configura la variable de entorno:
     ```bash
     export GEMINI_API_KEY='tu-clave-api-aqu√≠'
     ```
   O proporci√≥nala como argumento al ejecutar el script.

3. **L√≠mites de cuota**: Revisa y ajusta los l√≠mites de cuota seg√∫n sea necesario en Google Cloud Console.

## 2. Configuraci√≥n e Instalaci√≥n

### 2.1 Configuraci√≥n B√°sica (CPU/GPU)

Antes de comenzar, aseg√∫rate de tener lo siguiente:

- **Python**: Versi√≥n 3.8 o superior.
- **WSL (Windows Subsystem for Linux)**: **Recomendado para aceleraci√≥n GPU**.
    - Para verificar si WSL est√° instalado y tu distribuci√≥n, ejecuta en PowerShell o CMD: `wsl --list --verbose`
    - Aseg√∫rate de usar WSL 2.
- **GPU NVIDIA (opcional)**: Para aceleraci√≥n GPU, necesitar√°s una GPU compatible con CUDA (ej: GeForce RTX 5070 Ti).
- **Controladores NVIDIA**: √öltimos controladores instalados en Windows.
- **Toolkit CUDA (solo si usas GPU)**: Debe instalarse dentro de WSL.
    - Verifica con: `nvidia-smi` y `nvcc --version`
    - Sigue las gu√≠as oficiales de NVIDIA para instalar CUDA Toolkit en WSL si es necesario.

## 2. Configuraci√≥n e Instalaci√≥n

### Configuraci√≥n B√°sica (CPU)

1. **Navegar al directorio del proyecto**:
   ```bash
   cd /mnt/c/local/tools/tools/yt-dlp
   ```

2. **Crear y activar entorno virtual**:
   ```bash
   python3 -m venv venv-yt-ia
   source venv-yt-ia/bin/activate
   ```

3. **Instalar dependencias b√°sicas**:
   ```bash
   pip install llama-cpp-python numpy
   ```

### Configuraci√≥n Avanzada (GPU - Pendiente)

Para habilitar la aceleraci√≥n por GPU con una RTX 5070 Ti:

1. **Instalar compiladores y CMake**:
   ```bash
   sudo apt update && sudo apt install -y build-essential cmake
   ```

2. **Reinstalar llama-cpp-python con soporte CUDA**:
   ```bash
   CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir
   ```

3. **Verificar instalaci√≥n CUDA**:
   ```bash
   nvidia-smi
   nvcc --version
   ```

## 3. Configuraci√≥n de Modelos

El m√≥dulo de IA requiere modelos en formato GGUF. Hasta ahora hemos probado con √©xito TinyLlama en CPU.

### Modelos Probados

1. **TinyLlama (1.1B par√°metros)**
   - **Estado**: ‚úÖ Funciona correctamente en CPU/GPU
   - **Tama√±o**: ~700MB (versi√≥n cuantizada)
   - **Rendimiento**: Aceptable para pruebas, recomendado usar GPU para producci√≥n
   - **Ubicaci√≥n recomendada**: `C:\local\modelos\tinyllama.gguf`
   - **Uso con CUDA**: Configura `n_gpu_layers=-1` para m√°xima aceleraci√≥n

2. **Configuraci√≥n de modelos**
   - Los modelos se cargan bajo demanda
   - Se soportan tanto el pipeline nativo como LangChain
   - Los par√°metros de generaci√≥n son configurables en tiempo de ejecuci√≥n

### Estructura de Directorios

```
yt-dlp/
‚îú‚îÄ‚îÄ ia/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ ia_models.py    # Carga de modelos
‚îÇ   ‚îú‚îÄ‚îÄ ia_processor.py # Procesamiento de texto
‚îÇ   ‚îú‚îÄ‚îÄ test_ia.py      # Pruebas del m√≥dulo
‚îÇ   ‚îî‚îÄ‚îÄ prompts/        # Plantillas de prompts
‚îÇ       ‚îî‚îÄ‚îÄ summary.txt
‚îî‚îÄ‚îÄ modelos/            # Directorio para modelos GGUF
    ‚îî‚îÄ‚îÄ tinyllama.gguf  # Modelo actual
```

### Uso del M√≥dulo de IA

#### Inicializaci√≥n b√°sica
```python
from ia.core import initialize_llm, map_summarize_chunk, reduce_summaries

# Inicializar el modelo (CPU/GPU autom√°tico)
llm = initialize_llm(
    model_path="/ruta/al/modelo.gguf",
    pipeline_type="native",  # o "langchain"
    n_ctx=2048,             # contexto m√°ximo
    n_gpu_layers=-1         # -1 para usar todas las capas en GPU
)

# Configuraci√≥n de generaci√≥n
generation_params = {
    "temperature": 0.1,
    "max_tokens": 512,
    "top_p": 0.9
}

# Resumir un fragmento de texto
summary = map_summarize_chunk(
    llm,
    "Texto a resumir...",
    "Resume el siguiente texto: {text}",
    generation_params
)

# Combinar m√∫ltiples res√∫menes
combined = reduce_summaries(
    llm,
    ["Resumen 1", "Resumen 2"],
    "Combina estos res√∫menes: {text}",
    generation_params
)
```

### A√±adir Nuevos Modelos

1. Descarga el modelo GGUF (recomendado desde [TheBloke en Hugging Face](https://huggingface.co/TheBloke))
2. Col√≥calo en `C:\local\modelos\`
3. El sistema detectar√° autom√°ticamente modelos en esta ubicaci√≥n

## 4. Pruebas del M√≥dulo

El m√≥dulo incluye un conjunto completo de pruebas unitarias que verifican:

- Carga de modelos (nativo y LangChain)
- Procesamiento de texto con diferentes configuraciones
- Manejo de errores y casos l√≠mite
- Compatibilidad con diferentes tama√±os de contexto

### Ejecutar pruebas

```bash
# Navegar al directorio del proyecto
cd /mnt/c/local/tools/tools/yt-dlp

# Activar entorno virtual (si no est√° activo)
source venv-yt-ia/bin/activate

# Ejecutar pruebas unitarias
python -m unittest ia.tests.test_core

# Ejecutar pruebas con cobertura (opcional)
pip install coverage
coverage run -m unittest ia.tests.test_core
coverage report -m
```

### Estructura de pruebas

- `test_core.py`: Pruebas unitarias para las funciones principales
  - Carga de plantillas de prompts
  - Inicializaci√≥n de modelos
  - Funciones de mapeo y reducci√≥n
  - Manejo de errores

**Salida esperada**:
```
=== PRUEBA DE MODELO TINYLLAMA ===
Iniciando prueba con el modelo...

1. Cargando modelo (esto puede tardar un momento)...
‚úì Modelo cargado en 18.0 segundos

2. Probando generaci√≥n de texto...
Prompt: 'Ponme un t√≠tulo creativo para un video sobre programaci√≥n en Python'

‚úì Resultado generado:
--------------------------------------------------
y Python.
--------------------------------------------------

Tiempo de generaci√≥n: 0.21 segundos
```

### test_ia.py

Prueba m√°s completa que utiliza el m√≥dulo de IA para procesar texto de ejemplo.

```bash
python test_ia.py
```

## 5. Sistema Avanzado de Chunking

El sistema ahora incluye un avanzado m√≥dulo de chunking con m√∫ltiples estrategias implementadas mediante el patr√≥n Strategy.

### 5.1 Estrategias Disponibles

- **Chunking por Caracteres**: Estrategia tradicional que divide el texto en fragmentos de tama√±o fijo.
  ```python
  from rag_engine.chunker import chunk_text
  chunks = chunk_text(texto, strategy='caracteres', chunk_size=1000, chunk_overlap=200)
  ```

- **Chunking por Palabras**: Divide respetando l√≠mites de palabras para evitar cortes abruptos.
  ```python
  chunks = chunk_text(texto, strategy='palabras', chunk_size=1000, chunk_overlap=200)
  ```

- **Chunking Sem√°ntico**: Divide respetando la estructura natural del texto (p√°rrafos y frases).
  ```python
  # Mantiene p√°rrafos juntos cuando es posible y preserva la coherencia sem√°ntica
  chunks = chunk_text(texto, strategy='semantico', chunk_size=1000, chunk_overlap=200)
  ```

- **Chunking Agentic (con LLMs)**: Sistema de divisi√≥n de texto **totalmente funcional y robusto** que utiliza LLMs (Google Gemini o un modelo local) para realizar un chunking inteligente basado en el contenido sem√°ntico.
  - **Selecci√≥n Directa de Proveedor**: Elige expl√≠citamente entre "Google Gemini" y "LLM Local" en la GUI.
  - **Sin Fallbacks**: El sistema ya no recurre a otras estrategias si un LLM falla. Los errores de API se muestran directamente para un diagn√≥stico claro.
  - **Alta Coherencia Sem√°ntica**: Los LLMs analizan el texto para identificar los puntos de divisi√≥n m√°s l√≥gicos, creando chunks con un alto grado de coherencia interna.

  ```python
  # En la GUI, simplemente selecciona la estrategia "Agentic"
  # y el proveedor deseado (Gemini o Local).
  
  # Ejemplo de uso directo (como en la GUI):
  from rag_engine.agentic_chunking import chunk_text_with_gemini, chunk_text_with_local_llm

  # Usar Gemini
  chunks_gemini = chunk_text_with_gemini(texto, chunk_size=1024, chunk_overlap=200)

  # Usar LLM Local
  chunks_local = chunk_text_with_local_llm(texto, chunk_size=1024, chunk_overlap=200)
  ```

### 5.2 Chunking Playground

El sistema incluye una interfaz gr√°fica para experimentar con diferentes estrategias de chunking:

```bash
python rag_engine/chunking_playground.py
```

Funcionalidades:
- Carga de texto desde archivos o entrada manual
- Selecci√≥n de estrategia de chunking (caracteres, palabras, sem√°ntico, agentic)
- Configuraci√≥n de tama√±o y solapamiento
- Visualizaci√≥n de chunks generados con metadatos (ID, T√≠tulo, Resumen, Contenido, Chars)
- Ventana emergente con detalles completos del chunk (doble clic)
- Almacenamiento en base de datos vectorial
- Visualizaci√≥n de embeddings

### 5.3 Prueba de Estrategias

Para comparar el rendimiento de las diferentes estrategias:

```bash
python rag_engine/test_chunking_strategies.py
```

Este script demuestra las diferencias entre cada estrategia utilizando textos de ejemplo con estructura clara.

## 6. Uso de Gemini API

### 6.1 Configuraci√≥n B√°sica

Para utilizar Gemini API, sigue estos pasos:

1. **Configura tu clave de API** (puedes usar variable de entorno o par√°metro):
   ```bash
   # Usando variable de entorno
   export GEMINI_API_KEY='tu-clave-api-aqu√≠'
   
   # O como par√°metro
   python bench.py --pipeline gemini --gemini-api-key 'tu-clave-api-aqu√≠'
   ```

2. **Ejecuta el benchmark con Gemini**:
   ```bash
   python bench.py \
     --input-file ia/sample_text.txt \
     --prompt-dir ia/prompts \
     --pipeline gemini \
     --gemini-model gemini-1.5-flash-latest  # Modelo por defecto
   ```

### 5.2 Modelos Disponibles

- `gemini-1.5-flash`: Equilibrio entre velocidad y costo (recomendado)
- `gemini-1.5-pro`: Mayor capacidad, mayor costo
- `gemini-1.0-pro`: Versi√≥n anterior

### 5.3 Control de Costos

El costo se calcula autom√°ticamente basado en:
- Tokens de entrada: $0.35 por mill√≥n de tokens
- Tokens de salida: $1.05 por mill√≥n de tokens

Ejemplo de salida de costos:
```
Tokens de entrada: 1,234 (Costo: $0.00043)
Tokens de salida: 456 (Costo: $0.00048)
Costo total: $0.00091
```

## 6. Soluci√≥n de Problemas

### Problemas Comunes con Modelos Locales

#### Modelo no encontrado
```
Error: No se encontr√≥ el modelo en /mnt/c/local/modelos/tinyllama.gguf
```
**Soluci√≥n**:
- Verifica que el archivo del modelo existe en la ruta especificada
- Aseg√∫rate de tener permisos de lectura

#### Errores de GPU / CUDA
```
Could not load library cublas
CUDA error: no kernel image is available for execution
```
**Soluci√≥n**:
1. Verifica los controladores de NVIDIA en Windows
2. Instala CUDA Toolkit en WSL
3. Reinstala llama-cpp-python con soporte CUDA:
   ```bash
   CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir
   ```

#### Errores de importaci√≥n
```
ModuleNotFoundError: No module named 'ia.ia_models'
```
**Soluci√≥n**:
- Aseg√∫rate de ejecutar los scripts desde el directorio ra√≠z del proyecto
- Verifica que el directorio `ia` tenga un archivo `__init__.py`

### Problemas con Gemini API

**Problema:** Errores de cuota (`429 Too Many Requests`).

**Soluci√≥n (Implementada):**
- **Modelo por defecto cambiado**: El sistema ahora usa `gemini-1.5-flash-latest` por defecto, que tiene un nivel gratuito m√°s generoso.
- **Configuraci√≥n centralizada**: Todas las partes de la aplicaci√≥n (GUI, CLI, benchmarks) ahora usan la configuraci√≥n del modelo definida en `ia/gemini_api.py`, asegurando consistencia.

#### Error de autenticaci√≥n
```
google.api_core.exceptions.PermissionDenied: 403 Permission denied on resource project _
```
**Soluci√≥n**:
- Verifica que la clave de API sea correcta y tenga permisos suficientes
- Aseg√∫rate de que la facturaci√≥n est√© habilitada en tu proyecto de Google Cloud

#### L√≠mite de cuota excedido
```
429 Resource has been exhausted
```
**Soluci√≥n**:
- Espera unos minutos antes de realizar m√°s solicitudes
- Verifica y ajusta los l√≠mites de cuota en Google Cloud Console
- Considera implementar un sistema de reintentos con backoff exponencial

## 7. M√≥dulo RAG (Retrieval-Augmented Generation)

El m√≥dulo RAG permite realizar b√∫squedas sem√°nticas sobre las transcripciones de los v√≠deos, encontrando fragmentos relevantes basados en el significado en lugar de solo palabras clave.

### Caracter√≠sticas Principales

- **Chunking Inteligente**: Divide el texto en fragmentos con solapamiento para mantener el contexto
- **Embeddings Locales**: Usa modelos sentence-transformers (all-MiniLM-L6-v2 por defecto)
- **Almacenamiento Eficiente**: Vectores almacenados en SQLite con extensi√≥n sqlite-vec
- **B√∫squeda Sem√°ntica**: Encuentra contenido relevante basado en similitud de significado
- **Soporte GPU**: Aceleraci√≥n con CUDA para generaci√≥n de embeddings

### Uso B√°sico

```python
from rag_engine.ingestor import RAGIngestor

# Inicializar el ingestor (crea la base de datos si no existe)
ingestor = RAGIngestor()

# Ingresar texto (puede ser una transcripci√≥n de v√≠deo)
texto = """
[Contenido de la transcripci√≥n...]
"""

# Procesar y almacenar el texto
ingestor.ingest_text(texto)

# Buscar contenido similar
resultados = ingestor.search("tema de b√∫squeda", top_k=3)
for contenido, similitud in resultados:
    print(f"Similitud: {similitud:.2f}")
    print(contenido)
    print("-" * 80)
```

### Configuraci√≥n Avanzada

Puedes personalizar el comportamiento del m√≥dulo RAG modificando `rag_engine/config.py`:

- `CHUNK_SIZE`: Tama√±o de los fragmentos de texto (en tokens)
- `CHUNK_OVERLAP`: Solapamiento entre fragmentos
- `EMBEDDING_MODEL`: Modelo de embeddings a utilizar
- `DB_PATH`: Ubicaci√≥n del archivo de base de datos
- `DB_TABLE_NAME`: Nombre de la tabla para almacenar los vectores

### RAG Chunking Playground

Herramienta de desarrollo con interfaz gr√°fica Tkinter para experimentar y ajustar par√°metros de chunking:

```bash
# Ejecutar desde el directorio del proyecto
python rag_engine/chunking_playground.py
```

**Caracter√≠sticas:**
- **Panel de Configuraci√≥n**: Sliders para ajustar `chunk_size` y `chunk_overlap` en tiempo real
- **Selector de Modo**: Chunking por caracteres o por palabras
- **Vista de Resultados**: Tabla con √≠ndice, preview y longitud de cada chunk
- **Base de Datos en Tiempo Real**: Estad√≠sticas y contenido de `rag_database.db`
- **Exportaci√≥n**: Guardar resultados en formato CSV
- **Manejo de Errores**: La aplicaci√≥n nunca se cierra por errores, muestra feedback

**Casos de Uso:**
- Ajustar par√°metros de chunking antes de procesar documentos grandes
- Visualizar c√≥mo diferentes configuraciones afectan la divisi√≥n del texto
- Inspeccionar el contenido actual de la base de datos vectorial
- Exportar configuraciones de chunking para documentaci√≥n

### Chunking Agentic Avanzado con Metadatos Enriquecidos (En Desarrollo)

La pr√≥xima gran mejora del sistema RAG se centrar√° en la implementaci√≥n de una estrategia de **chunking agentic avanzada**. A diferencia del placeholder actual, esta nueva versi√≥n no solo dividir√° el texto, sino que lo enriquecer√° con una capa de metadatos estructurados generados por un LLM. El objetivo es crear chunks "inteligentes" que tengan un conocimiento profundo de su contenido y su relaci√≥n con los chunks vecinos.

#### Caracter√≠sticas Clave:

- **Modelo de Datos de Chunk Enriquecido**: Cada chunk ser√° un objeto con:
  - `content`: El texto del chunk.
  - `metadata`:
    - `index`: √çndice secuencial.
    - `char_start_index` / `char_end_index`: Posici√≥n exacta en el texto original.
    - `semantic_title`: Un t√≠tulo corto generado por el LLM que resume la idea principal.
    - `summary`: Un resumen breve del contenido.
    - `prev_chunk_id` / `next_chunk_id`: Enlaces para reconstruir el contexto completo.
    - `semantic_overlap`: Descripci√≥n de la relaci√≥n sem√°ntica con los chunks adyacentes (ej. "contin√∫a la explicaci√≥n de...", "introduce un nuevo concepto sobre...").

- **Integraci√≥n con LLMs**: La l√≥gica de chunking y generaci√≥n de metadatos ser√° manejada por una funci√≥n LLM inyectable, permitiendo el uso de modelos locales (v√≠a microservicio) o APIs externas (Gemini, OpenAI).

- **Fallback Inteligente**: Si el LLM falla, el sistema recurrir√° al chunking sem√°ntico y generar√° una estructura de metadatos b√°sica para mantener la consistencia.

- **Actualizaci√≥n de la Base de Datos**: El esquema de la base de datos vectorial se ampliar√° para almacenar y consultar estos metadatos enriquecidos, permitiendo b√∫squedas m√°s contextuales.

- **Visualizaci√≥n en la GUI**: El `Chunking Playground` ser√° actualizado para mostrar los metadatos de cada chunk, facilitando el an√°lisis y la depuraci√≥n.

Esta funcionalidad transformar√° el sistema de chunking de una simple herramienta de divisi√≥n de texto a un motor de procesamiento de conocimiento, sentando las bases para un sistema RAG mucho m√°s potente y preciso.

### Pr√≥ximos Pasos y Mejoras Futuras

- [ ] Integraci√≥n con la interfaz de Streamlit existente
- [ ] Soporte para m√∫ltiples colecciones/namespaces
- [ ] B√∫squeda h√≠brida (sem√°ntica + keywords)
- [ ] Indexado incremental de nuevos documentos

## 8. Pr√≥ximos Pasos

1. **Optimizaci√≥n de rendimiento**
   - Sintonizaci√≥n fina de par√°metros de generaci√≥n
   - Soporte para batch processing
   - Mejora del manejo de memoria para modelos grandes

2. **Nuevas funcionalidades**
   - Integraci√≥n con el sistema de subt√≠tulos
   - Resumen autom√°tico de transcripciones
   - Extracci√≥n de palabras clave
   - Generaci√≥n de t√≠tulos y descripciones
   - Soporte para m√°s modelos y formatos

3. **Mejoras en la API**
   - Interfaz m√°s intuitiva
   - Soporte para streaming de respuestas
   - Callbacks para monitoreo de progreso

4. **Documentaci√≥n ampliada**
   - Gu√≠as de inicio r√°pido
   - Ejemplos de casos de uso
   - Referencia de API completa

## 8. Contribuciones

Este m√≥dulo es experimental y se encuentra en desarrollo activo. Las contribuciones son bienvenidas. Algunas √°reas de mejora incluyen:

- Mejora en la evaluaci√≥n de calidad de res√∫menes
- Soporte para m√°s proveedores de modelos en la nube
- Optimizaci√≥n de costos para uso en producci√≥n
- Mejora en la documentaci√≥n y ejemplos

## 9. Limitaciones y Futuro Desarrollo

### Limitaciones Actuales
- Los modelos locales tienen limitaciones de contexto (ej: 4096 tokens para Mistral 7B)
- La calidad de los res√∫menes puede variar significativamente entre modelos
- El rendimiento en GPU depende fuertemente de la configuraci√≥n del sistema

### Pr√≥ximas Caracter√≠sticas
- [ ] Integraci√≥n con m√°s proveedores de modelos en la nube
- [ ] Soporte para procesamiento por lotes
- [ ] Interfaz web para configuraci√≥n y monitoreo
- [ ] Sistema de cach√© para reducir costos
- [ ] Soporte para m√°s tareas de procesamiento de lenguaje natural

### Notas de Versi√≥n
Para ver el historial completo de cambios, consulta el archivo [CHANGELOG.md](./CHANGELOG.md).
