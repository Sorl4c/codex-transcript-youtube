# Plan MVP RAG + Roadmap Multi-Sesi√≥n

## üéØ SESI√ìN 1: MVP CLI RAG Query ‚úÖ COMPLETADA

**Fecha:** 30 Septiembre 2025
**Commits:** `80b7440`, `0d6131e`, `21c796b`

### Objetivo
Crear CLI m√≠nimo funcional para probar recuperaci√≥n RAG sin llamadas API continuas

### ‚úÖ Implementado

1. **`rag_engine/retriever.py`** - SimpleRetriever con b√∫squeda sem√°ntica
   - Clase `SimpleRetriever` con m√©todo `query(question, top_k)`
   - Usa `database.search_similar()` existente
   - Retorna `List[SearchResult]` con contenido y score

2. **`rag_engine/rag_cli.py`** - CLI completo con 3 comandos
   - `stats` - Estad√≠sticas de BD (88 documentos, 1.03 MB)
   - `query "<pregunta>" --top-k N` - B√∫squeda sem√°ntica
   - `ingest <file> --mock` - Ingesta con chunking simple (sin LLM)

3. **LocalEmbedder configurado**
   - Modelo: `all-MiniLM-L6-v2` (sentence-transformers)
   - Forzado a CPU para evitar problemas CUDA sm_120 (RTX 5070 Ti)
   - Sin dependencias de APIs externas

4. **Dependencias agregadas a `requirements.txt`**
   - `sentence-transformers>=5.1.0`
   - `sqlite-vec>=0.1.6`
   - `scikit-learn>=1.7.0`

5. **Venv recreado correctamente**
   - Eliminado venv roto (symlinks Linux/Windows)
   - Creado nuevo venv con Python 3.13.5
   - Todas las deps instaladas en `venv-yt-ia/`

### ‚úÖ Probado y funcionando
```bash
# Activar venv PRIMERO
venv-yt-ia\Scripts\activate

# Comandos CLI
python -m rag_engine.rag_cli stats
python -m rag_engine.rag_cli query "What is deep learning?" --top-k 3
python -m rag_engine.rag_cli ingest transcripts_for_rag/sample.txt --mock
```

### üìù Notas importantes
- ‚ö†Ô∏è **SIEMPRE activar venv antes de cualquier comando**
- Embeddings 100% locales, sin costos de API
- Base de datos ya tiene 88 documentos de sesiones anteriores
- CLI compatible con Windows (sin emojis, encoding UTF-8)

---

## üìã SESI√ìN 2: RAG H√≠brido + BM25 ‚úÖ COMPLETADA

**Fecha:** 01 Octubre 2025

### Objetivo
Implementar b√∫squeda h√≠brida combinando retrieval sem√°ntico (vector) y lexical (BM25) con fusi√≥n RRF

### ‚úÖ Implementado

1. **`rag_engine/hybrid_retriever.py`** - HybridRetriever con 3 modos
   - Clase `HybridRetriever` extendiendo `SimpleRetriever`
   - **Vector mode**: B√∫squeda sem√°ntica pura (embeddings)
   - **Keyword mode**: B√∫squeda lexical BM25 (rank-bm25)
   - **Hybrid mode**: Reciprocal Rank Fusion (RRF) de ambas
   - RRF con k=60 (paper original)
   - Provenance tracking (vector_rank, bm25_rank, scores individuales)

2. **`rag_engine/rag_cli.py`** - CLI actualizado con flag `--mode`
   - `query "<pregunta>" --mode vector|keyword|hybrid`
   - Modo vector como default (compatibilidad con Sesi√≥n 1)
   - Display de provenance para resultados h√≠bridos

3. **`rag_engine/test_queries.json`** - 12 queries de evaluaci√≥n
   - Queries de fitness (triceps, brazos, hipertrofia)
   - Queries de AI/tech (deep learning, inteligencia artificial)
   - Queries irrelevantes (clima, cambio clim√°tico)
   - Cada query con relevant_keywords para evaluaci√≥n autom√°tica

4. **`rag_engine/evaluate.py`** - Sistema de evaluaci√≥n con m√©tricas IR
   - **Recall@5**: Proporci√≥n de documentos relevantes en top-5
   - **MRR (Mean Reciprocal Rank)**: Promedio de reciprocal rank del primer relevante
   - Evaluaci√≥n autom√°tica con keyword matching como ground truth
   - Reporte comparativo de los 3 modos
   - Guardado de resultados en JSON

5. **Dependencia agregada**: `rank-bm25>=0.2.0` en `requirements.txt`

### üìä Resultados de Evaluaci√≥n

**Comparativa sobre 88 documentos y 12 test queries:**

| Modo    | Recall@5 | MRR    | Observaciones |
|---------|----------|--------|---------------|
| Vector  | 0.8333   | 0.9583 | Buena sem√°ntica, pierde algunos t√©rminos exactos |
| Keyword | 0.9667   | 1.0000 | **MEJOR** - Excelente para matching exacto |
| Hybrid  | 0.9333   | 1.0000 | Segundo mejor, combina ambas fortalezas |

**Conclusiones:**
- ‚úÖ **BM25 keyword search es superior** en este dataset espec√≠fico
- El corpus actual tiene mucha terminolog√≠a espec√≠fica (nombres de ejercicios, conceptos t√©cnicos)
- BM25 captura mejor los t√©rminos exactos ("press franc√©s", "cabeza larga")
- Hybrid mantiene MRR perfecto (1.0) pero Recall@5 ligeramente menor que keyword puro
- Vector search (sem√°ntico) es el menos efectivo en este caso espec√≠fico
- **Recomendaci√≥n**: Usar `--mode keyword` o `--mode hybrid` para mejor rendimiento

### ‚úÖ Probado y funcionando

```bash
# Activar venv PRIMERO
venv-yt-ia\Scripts\activate

# Queries con diferentes modos
python -m rag_engine.rag_cli query "ejercicios para triceps" --mode vector
python -m rag_engine.rag_cli query "ejercicios para triceps" --mode keyword
python -m rag_engine.rag_cli query "ejercicios para triceps" --mode hybrid

# Evaluaci√≥n completa
python -m rag_engine.evaluate
```

### ‚ö†Ô∏è Nota importante sobre sintaxis CLI
El flag `--mode` requiere un **ESPACIO** antes de los guiones:
- ‚úÖ **CORRECTO**: `query "pregunta" --mode vector` (con espacio)
- ‚ùå **INCORRECTO**: `query "pregunta"--mode vector` (sin espacio - error de parsing)

### üìù Notas importantes
- ‚ö†Ô∏è **SIEMPRE activar venv antes de cualquier comando**
- BM25 requiere cargar todo el corpus en memoria (acceptable para <10k docs)
- RRF usa k=60 (configurable en HybridRetriever)
- Evaluation usa keyword matching como proxy de relevancia (no requiere anotaciones manuales)
- Results saved to `rag_engine/evaluation_results.json`

---

## üìã SESI√ìN 3: Schema v2 Multi-Chunking ‚úÖ COMPLETADA

**Fecha:** 01 Octubre 2025

### Objetivo
Implementar schema de base de datos mejorado (v2) que almacene metadata de chunking strategy, permitiendo comparar estrategias semantic vs agentic en el mismo sistema RAG.

### ‚úÖ Implementado

1. **`rag_engine/database.py`** - Schema v2 con metadata completo
   - Tabla `vector_store` con 14 columnas:
     - Tracking: `source_document`, `source_hash`, `chunking_strategy`
     - Posicionamiento: `chunk_index`, `char_start`, `char_end`
     - Metadata agentic: `semantic_title`, `semantic_summary`, `semantic_overlap`
     - Extensible: `metadata_json` (JSON para keywords y otros)
     - Timestamp: `created_at`
   - M√©todo nuevo: `add_documents_with_metadata()`
   - M√©todo legacy: `add_documents()` (backward compatible)
   - 4 √≠ndices para b√∫squedas eficientes (strategy, source, hash, content)

2. **`rag_engine/ingestor.py`** - Pipeline mejorado con metadata tracking
   - Par√°metro `source_document` para tracking de origen
   - Calcula `source_hash` (MD5) para deduplicaci√≥n
   - Extrae autom√°ticamente metadata de Chunk objects
   - Guarda keywords en `metadata_json`
   - Summary mejorado con strategy y hash info

3. **`rag_engine/rag_cli.py`** - CLI actualizado
   - Pasa filepath absoluto como `source_document`
   - Display mejorado mostrando strategy, source_hash
   - Compatible con schema v2

4. **`rag_engine/agentic_chunking.py`** - Metadata enriquecido v√≠a LLM
   - Prompt mejorado para extraer:
     - `title`: T√≠tulo descriptivo (5-10 palabras)
     - `summary`: Resumen conciso (2-3 frases)
     - `keywords`: Lista de 5-7 palabras clave
     - `semantic_overlap`: Conexi√≥n con chunk anterior
   - Parser actualizado para procesar keywords
   - Keywords guardados en `additional_metadata['keywords']`
   - Soporte para LLM local (llm_service) y Gemini API

5. **`rag_engine/inspect_db.py`** - Herramienta de inspecci√≥n completa
   - Estad√≠sticas por chunking_strategy
   - Lista de source_documents con chunk counts
   - Detecci√≥n de duplicados por source_hash
   - Samples de metadata agentic (title, summary, keywords)
   - An√°lisis de diversidad de contenido (top 20 palabras)

6. **Corpus de prueba diverso** - 5 topics + 1 test
   - `transcripts_for_rag/test_agentic.txt` (prueba r√°pida de IA/ML)
   - `transcripts_for_rag/test_corpus/fitness.txt` (triceps, entrenamiento)
   - `transcripts_for_rag/test_corpus/ai_ml.txt` (IA, deep learning, GPT)
   - `transcripts_for_rag/test_corpus/history.txt` (Renacimiento italiano)
   - `transcripts_for_rag/test_corpus/cooking.txt` (cocina italiana)
   - `transcripts_for_rag/test_corpus/science.txt` (mec√°nica cu√°ntica)

### ‚úÖ Base de datos limpiada
- Backup creado: `rag_database.db.backup`
- BD antigua eliminada (88 documentos duplicados con datos de prueba)
- Nueva BD se crear√° autom√°ticamente con schema v2 en primera ingesta

### üìù Notas importantes
- ‚ö†Ô∏è **Schema v2 es autom√°tico**: Primera ingesta crea tabla nueva
- ‚ö†Ô∏è **Embeddings 100% locales**: `all-MiniLM-L6-v2` (sentence-transformers)
- ‚ö†Ô∏è **Agentic chunking NO probado a√∫n**: Requiere llm_service activo (Sesi√≥n 4)
- ‚úÖ **Backward compatible**: C√≥digo antiguo sigue funcionando
- üîÆ **Futuro**: Opci√≥n de cambiar modelo embeddings y re-generar todos los chunks

### üéØ Siguiente paso
Pasar a **Sesi√≥n 4** para probar agentic chunking con LLM local end-to-end.

---

## üìã SESI√ìN 4: Testing LLM Local + Agentic Chunking (PR√ìXIMA)

**Objetivo:** Validar agentic chunking completo con LLM local y comparar con semantic chunking

### Features

1. **Verificar llm_service funcional**
   - Puerto 8000 accesible
   - Modelo GGUF cargado correctamente
   - Test de endpoint `/v1/chat/completions`
   - Revisar `llm_service/config.py` y `main.py`

2. **Testing agentic chunking con archivo peque√±o**
   - Ingestar `test_agentic.txt` con `--strategy agentic`
   - Verificar que usa LLM local (no Gemini fallback)
   - Inspeccionar metadata extra√≠do: title, summary, keywords
   - Comando: `python -m rag_engine.inspect_db`

3. **Comparaci√≥n semantic vs agentic**
   - Ingestar mismo archivo con `--strategy semantico`
   - Comparar metadata: semantic (b√°sico) vs agentic (rico)
   - Evaluar calidad de t√≠tulos/res√∫menes/keywords del LLM
   - Decisi√≥n: ¬øVale la pena el costo computacional de agentic?

4. **Ingesta corpus completo** (si agentic funciona bien)
   - 5 documentos √ó 2 strategies = 10 entradas en BD
   - Script batch para ingesta automatizada
   - Validaci√≥n de diversidad con `inspect_db.py`
   - Verificar NO hay duplicados por source_hash

5. **Testing retrieval multi-topic**
   - Queries diversas cubriendo los 5 topics
   - Confirmar NO hay hardcoding de resultados (problema original)
   - Verificar que devuelve chunks de diferentes sources
   - Probar los 3 modos: vector, keyword, hybrid

### Archivos a revisar/modificar
- `llm_service/main.py` - Verificar configuraci√≥n del servicio
- `llm_service/config.py` - Path al modelo GGUF
- `rag_engine/agentic_chunking.py` - Ajustar prompt si la calidad es baja
- Crear script batch de ingesta (opcional)

### ‚ö†Ô∏è Decisiones importantes
- **Embeddings**: SIEMPRE local (`all-MiniLM-L6-v2`)
- **LLM local**: SOLO para agentic chunking metadata
- **Gemini API**: Fallback opcional, no requerido
- **Estrategia default**: Semantic (r√°pido, sin LLM)
- **Estrategia avanzada**: Agentic (lento, metadata rico)

---

## üìã SESI√ìN 5: Reranking con Cross-Encoder (POSPUESTO)

### Features
- Implementar reranking con `sentence-transformers` cross-encoder
- Modelo: `cross-encoder/ms-marco-MiniLM-L-6-v2`
- Reordenar top-k resultados por relevancia sem√°ntica profunda
- CLI: `--rerank` flag

### Archivos nuevos
- `rag_engine/reranker.py`

---

## üìã SESI√ìN 4: Integraci√≥n Streamlit

### Features
- Pesta√±a "RAG Search" en `gui_streamlit.py`
- Input de pregunta + resultados visuales
- Mostrar chunks + scores + metadata
- Configuraci√≥n: top-k, modo (vector/h√≠brido), reranking

---

## üìã SESI√ìN 5: M√©tricas de Evaluaci√≥n ‚ö†Ô∏è PARCIALMENTE COMPLETADA

### ‚úÖ Ya implementado (Sesi√≥n 2)
- **Recall@5** ‚úÖ - Proporci√≥n de documentos relevantes en top-5
- **MRR (Mean Reciprocal Rank)** ‚úÖ - Promedio de reciprocal ranks
- Sistema de evaluaci√≥n autom√°tico con test queries ‚úÖ
- Archivo: `rag_engine/evaluate.py`

### ‚ùå Faltante (por implementar)
- **NDCG** (Normalized Discounted Cumulative Gain)
- Dataset de evaluaci√≥n manual m√°s extenso (actualmente 12 queries)
- CLI: comando `benchmark` dedicado (actualmente es `evaluate`)
- Reportes comparativos en diferentes formatos (CSV, HTML)
- M√©tricas de diversidad de resultados
- A/B testing framework

---

## üìã SESI√ìN 6: Migraci√≥n de Transcripciones Existentes

### Features
- Script para procesar `subtitles.db` ‚Üí `rag_database.db`
- Procesamiento por lotes con progress bar
- Opci√≥n de agentic chunking real con LLM
- Preservar metadata (t√≠tulo, canal, fecha)

---

## üîß Estructura de Archivos

### Sesi√≥n 1 (MVP)
```
rag_engine/
‚îú‚îÄ‚îÄ rag_cli.py          ‚Üê NUEVO (CLI principal)
‚îú‚îÄ‚îÄ retriever.py        ‚Üê NUEVO (SimpleRetriever)
‚îú‚îÄ‚îÄ database.py         ‚úÖ (ya existe search_similar)
‚îú‚îÄ‚îÄ embedder.py         ‚úÖ (LocalEmbedder ya configurado)
‚îú‚îÄ‚îÄ chunker.py          ‚úÖ (4 estrategias disponibles)
‚îú‚îÄ‚îÄ ingestor.py         ‚úÖ (pipeline completo)
‚îî‚îÄ‚îÄ config.py           ‚úÖ (configuraci√≥n centralizada)
```

### Sesi√≥n 2 (Hybrid Search)
```
rag_engine/
‚îú‚îÄ‚îÄ hybrid_retriever.py    ‚Üê NUEVO (HybridRetriever con BM25 + RRF)
‚îú‚îÄ‚îÄ test_queries.json      ‚Üê NUEVO (12 queries de evaluaci√≥n)
‚îú‚îÄ‚îÄ evaluate.py            ‚Üê NUEVO (m√©tricas Recall@5 y MRR)
‚îú‚îÄ‚îÄ evaluation_results.json ‚Üê GENERADO (resultados cuantitativos)
‚îî‚îÄ‚îÄ rag_cli.py             ‚úÖ ACTUALIZADO (flag --mode)
```

---

## ‚ö° Ventajas del Enfoque

1. **Testing sin API**: Embeddings 100% locales, sin costos
2. **Iteraci√≥n r√°pida**: CLI permite experimentar sin GUI
3. **Mock data reutilizable**: Chunks pre-generados en BD
4. **Modular**: Cada sesi√≥n a√±ade features independientes
5. **Git worktrees friendly**: Features aisladas por rama

---

## üìä Estado Actual del Sistema RAG

### ‚úÖ Implementado
- Chunking (4 estrategias: caracteres, palabras, sem√°ntico, agentic)
- Embeddings (100% local con `all-MiniLM-L6-v2` via sentence-transformers)
- Base de datos vectorial (SQLite + sqlite-vec)
- **‚úÖ Schema v2 con metadata tracking** (chunking_strategy, source_document, source_hash, etc.)
- **‚úÖ Agentic chunking con metadata enriquecido** (title, summary, keywords v√≠a LLM)
- B√∫squeda por similitud (`search_similar()`)
- Pipeline de ingesta completo con metadata extraction
- **‚úÖ Interfaz de consulta/retriever de alto nivel** (SimpleRetriever + HybridRetriever)
- **‚úÖ B√∫squeda h√≠brida** (vector + keyword/BM25 con RRF)
- **‚úÖ M√©tricas de evaluaci√≥n** (Recall@5, MRR)
- **‚úÖ CLI completo** con 3 modos de b√∫squeda + tracking de metadata
- **‚úÖ Herramienta de inspecci√≥n** (`inspect_db.py` para debugging)
- **‚úÖ Corpus de prueba diverso** (5 topics: fitness, AI, history, cooking, science)

### ‚ùå Faltante (por implementar)
- Testing completo de agentic chunking con LLM local (Sesi√≥n 4)
- Evaluaci√≥n cuantitativa semantic vs agentic chunking
- Reranking despu√©s del top-k (cross-encoder) - Sesi√≥n 5
- Integraci√≥n con Streamlit (pesta√±a RAG Search) - Sesi√≥n 6
- M√©tricas adicionales (NDCG)
- Migraci√≥n masiva de subtitles.db ‚Üí rag_database.db
- Sistema de re-embedding con modelos alternativos (opci√≥n futura)

---

## üìñ REPASO: Estado del Sistema RAG (Octubre 2025)

### üéØ Progreso General: 50% completado (3/6 sesiones core)

#### ‚úÖ SESIONES COMPLETADAS

**Sesi√≥n 1 - MVP CLI RAG Query** ‚úÖ (30 Sept 2025)
- CLI funcional con 3 comandos (`stats`, `query`, `ingest`)
- B√∫squeda sem√°ntica con embeddings locales (`all-MiniLM-L6-v2`)
- 88 documentos en BD vectorial
- Commits: `80b7440`, `0d6131e`, `21c796b`

**Sesi√≥n 2 - RAG H√≠brido + BM25** ‚úÖ (01 Oct 2025)
- 3 modos de b√∫squeda (vector, keyword, hybrid)
- Reciprocal Rank Fusion (RRF) con k=60
- Sistema de evaluaci√≥n con Recall@5 y MRR
- **Keyword search superior** para corpus con terminolog√≠a t√©cnica
- Commit: `123e232`

**Sesi√≥n 3 - Schema v2 Multi-Chunking** ‚úÖ (01 Oct 2025)
- Schema v2 con 14 columnas de metadata
- Agentic chunking con metadata enriquecido (title, summary, keywords)
- Herramienta `inspect_db.py` para debugging
- Corpus de prueba diverso (5 topics)
- BD antigua limpiada (backup creado)

#### üöß PR√ìXIMAS SESIONES

**Sesi√≥n 4 - Testing LLM Local + Agentic Chunking** (pr√≥xima inmediata)
- Verificar llm_service funcional
- Testing agentic chunking end-to-end
- Comparaci√≥n semantic vs agentic
- Ingesta corpus completo + validaci√≥n retrieval

**Sesi√≥n 5 - Reranking** (pospuesto)
- Cross-encoder para reranking profundo (`cross-encoder/ms-marco-MiniLM-L-6-v2`)
- Mejora de precisi√≥n en top-k
- Flag `--rerank` en CLI

**Sesi√≥n 6 - Integraci√≥n Streamlit**
- Pesta√±a "RAG Search" en `gui_streamlit.py`
- Visualizaci√≥n de resultados con metadata agentic
- Selector de chunking strategy
- Configuraci√≥n interactiva (top-k, modo, reranking)

**Sesi√≥n 7 - Migraci√≥n Masiva** (futuro)
- Script para procesar `subtitles.db` ‚Üí `rag_database.db`
- Migraci√≥n de datos hist√≥ricos con metadata

---

### üì¶ Inventario de Componentes RAG

#### Core RAG Engine

| Componente | Estado | Archivo | Descripci√≥n |
|------------|--------|---------|-------------|
| **Chunking** | ‚úÖ Completo | `chunker.py` | 4 estrategias: caracteres, palabras, sem√°ntico, agentic |
| **Agentic Chunking** | ‚ö†Ô∏è Implementado | `agentic_chunking.py` | Metadata LLM (title, summary, keywords) - NO probado |
| **Embeddings** | ‚úÖ Completo | `embedder.py` | 100% Local (`all-MiniLM-L6-v2`) |
| **Vector DB v2** | ‚úÖ Completo | `database.py` | Schema v2 con 14 columnas + metadata tracking |
| **Ingesta v2** | ‚úÖ Completo | `ingestor.py` | Pipeline con metadata extraction + source tracking |
| **Retrieval Vector** | ‚úÖ Completo | `retriever.py` | SimpleRetriever con b√∫squeda sem√°ntica |
| **Retrieval H√≠brido** | ‚úÖ Completo | `hybrid_retriever.py` | BM25 + Vector + RRF fusion |
| **Evaluaci√≥n** | ‚úÖ Completo | `evaluate.py` | Recall@5, MRR con test queries |
| **Inspecci√≥n DB** | ‚úÖ Completo | `inspect_db.py` | Debugging, stats por strategy, metadata samples |
| **Reranking** | ‚ùå Pendiente | - | Cross-encoder para reordenamiento |

#### Interfaces de Usuario

| Interfaz | Estado | Archivo | Descripci√≥n |
|----------|--------|---------|-------------|
| **CLI RAG v2** | ‚úÖ Completo | `rag_cli.py` | 3 comandos + 3 modos + metadata display |
| **GUI Streamlit Principal** | ‚úÖ Completo | `gui_streamlit.py` | Library de videos con transcripts |
| **GUI Streamlit RAG** | ‚ùå Pendiente | - | Pesta√±a dedicada a b√∫squeda RAG |

#### Datos de Prueba

| Dataset | Estado | Ubicaci√≥n | Descripci√≥n |
|---------|--------|-----------|-------------|
| **Test Agentic** | ‚úÖ Creado | `test_agentic.txt` | Archivo peque√±o para prueba r√°pida (IA/ML) |
| **Corpus Fitness** | ‚úÖ Creado | `test_corpus/fitness.txt` | Entrenamiento triceps, hipertrofia |
| **Corpus AI/ML** | ‚úÖ Creado | `test_corpus/ai_ml.txt` | Deep learning, GPT, transformers |
| **Corpus History** | ‚úÖ Creado | `test_corpus/history.txt` | Renacimiento italiano, Leonardo, M√©dici |
| **Corpus Cooking** | ‚úÖ Creado | `test_corpus/cooking.txt` | Cocina italiana, pasta, pizza |
| **Corpus Science** | ‚úÖ Creado | `test_corpus/science.txt` | Mec√°nica cu√°ntica, entrelazamiento |
| **Test Queries** | ‚ö†Ô∏è Limitado | `test_queries.json` | 12 queries (necesita expansi√≥n a 30+) |

---

### üî¢ Estad√≠sticas del Sistema

**Base de Datos RAG (`rag_database.db`):**
- **Estado**: Limpiada (backup en `rag_database.db.backup`)
- **Documentos actuales**: 0 (BD nueva con schema v2)
- **Documentos previos**: 88 chunks (datos de prueba duplicados - eliminados)
- **Schema**: v2 con 14 columnas + 4 √≠ndices
- **Modelo de embeddings**: `all-MiniLM-L6-v2` (384 dimensiones, 100% local)
- **Vector DB**: SQLite 3.49.1 + sqlite-vec v0.1.6

**Corpus de Prueba Disponible:**
- **Total archivos**: 6 (1 test + 5 corpus completo)
- **Topics cubiertos**: 5 (Fitness, AI/ML, History, Cooking, Science)
- **Tama√±o promedio**: ~1500 palabras por documento
- **Listo para ingesta**: ‚úÖ S√≠ (Sesi√≥n 4)

**Rendimiento de Retrieval (evaluado sobre 12 test queries):**

| Modo | Recall@5 | MRR | Observaci√≥n |
|------|----------|-----|-------------|
| **Keyword (BM25)** | **96.67%** | **100%** | üèÜ Mejor para terminolog√≠a exacta |
| Hybrid (RRF) | 93.33% | 100% | ü•à Balance entre ambos |
| Vector (Semantic) | 83.33% | 95.83% | ü•â Bueno para b√∫squeda conceptual |

**Conclusi√≥n**: BM25 keyword search es superior para corpus con terminolog√≠a t√©cnica espec√≠fica (nombres de ejercicios, conceptos t√©cnicos). Hybrid mantiene MRR perfecto.

---

### üé¨ Comandos R√°pidos de Referencia

```bash
# ========================================
# ACTIVAR ENTORNO (SIEMPRE PRIMERO)
# ========================================
venv-yt-ia\Scripts\activate

# ========================================
# ESTAD√çSTICAS DE LA BASE DE DATOS
# ========================================
python -m rag_engine.rag_cli stats

# ========================================
# B√öSQUEDA RAG (3 MODOS)
# ========================================
# Modo 1: Vector (sem√°ntico)
python -m rag_engine.rag_cli query "ejercicios para triceps" --mode vector

# Modo 2: Keyword (BM25 - mejor para t√©rminos exactos)
python -m rag_engine.rag_cli query "ejercicios para triceps" --mode keyword

# Modo 3: Hybrid (RRF - balance entre ambos)
python -m rag_engine.rag_cli query "ejercicios para triceps" --mode hybrid

# Con top-k personalizado
python -m rag_engine.rag_cli query "deep learning" --mode hybrid --top-k 10

# ========================================
# INGESTA DE DOCUMENTOS
# ========================================
# Mock mode (r√°pido, sin LLM)
python -m rag_engine.rag_cli ingest archivo.txt --mock

# Con estrategia espec√≠fica
python -m rag_engine.rag_cli ingest archivo.txt --strategy semantico
python -m rag_engine.rag_cli ingest archivo.txt --strategy agentic

# ========================================
# EVALUACI√ìN COMPLETA
# ========================================
python -m rag_engine.evaluate

# ========================================
# GUI PRINCIPAL (CON LIBRARY DE VIDEOS)
# ========================================
streamlit run gui_streamlit.py
```

---

### üöÄ Pr√≥ximos Pasos Recomendados

#### üî¥ Inmediato (Sesi√≥n 4 - Testing LLM)
1. **Verificar llm_service funcional**
   - Iniciar servicio en puerto 8000
   - Test endpoint `/v1/chat/completions`
   - Confirmar modelo GGUF cargado

2. **Testing agentic chunking end-to-end**
   - Ingestar `test_agentic.txt` con `--strategy agentic`
   - Verificar metadata extra√≠do (title, summary, keywords)
   - Usar `inspect_db.py` para validaci√≥n

3. **Comparaci√≥n semantic vs agentic**
   - Ingestar mismo archivo con ambas estrategias
   - Evaluar calidad de metadata del LLM
   - Decisi√≥n: ¬øVale la pena el costo computacional?

4. **Ingesta corpus completo + validaci√≥n retrieval**
   - 5 docs √ó 2 strategies = 10 entradas
   - Queries diversas en todos los topics
   - Confirmar NO hay hardcoding de resultados

#### üü° Corto Plazo (Sesi√≥n 5-6)
5. **Reranking con cross-encoder** (Sesi√≥n 5)
   - Modelo: `cross-encoder/ms-marco-MiniLM-L-6-v2`
   - Flag `--rerank` en CLI

6. **Integraci√≥n Streamlit** (Sesi√≥n 6)
   - Pesta√±a RAG Search en GUI
   - Selector de chunking strategy
   - Visualizaci√≥n de metadata agentic

7. **Expandir test queries**
   - De 12 a 30+ queries
   - Cubrir los 5 topics del corpus
   - Queries irrelevantes (control negativo)

#### üü¢ Medio Plazo
8. **M√©tricas avanzadas**: Implementar NDCG
9. **Migraci√≥n masiva**: Script para `subtitles.db` ‚Üí `rag_database.db`
10. **Optimizaci√≥n**: Cache de embeddings para queries frecuentes

#### üîµ Largo Plazo
11. **Sistema de re-embedding**: Cambiar modelo y regenerar chunks
12. **Unit tests**: Suite de pruebas automatizadas
13. **Monitoreo**: Logging y m√©tricas de uso
14. **Escalabilidad**: Evaluaci√≥n con corpus >10k documentos

---

### üìö Referencias T√©cnicas

#### Algoritmos Implementados
- **BM25**: Okapi BM25 ranking function (lexical matching)
- **RRF**: Reciprocal Rank Fusion (Cormack et al. 2009) - k=60
- **Embeddings**: Sentence-BERT (all-MiniLM-L6-v2, 384 dims)

#### M√©tricas de Evaluaci√≥n
- **Recall@K**: Proporci√≥n de documentos relevantes en top-k
- **MRR**: Mean Reciprocal Rank (promedio de 1/rank del primer relevante)
- **NDCG**: Normalized Discounted Cumulative Gain (pendiente)

#### Tecnolog√≠as
- **Vector DB**: `sqlite-vec` (SQLite extension para vectores)
- **Embeddings**: `sentence-transformers` (Hugging Face)
- **BM25**: `rank-bm25` (implementaci√≥n Python)
- **Backend**: Python 3.13.5
- **GUI**: Streamlit

#### Papers de Referencia
1. BM25: Robertson & Zaragoza (2009) - "The Probabilistic Relevance Framework: BM25 and Beyond"
2. RRF: Cormack, Clarke & B√ºttcher (2009) - "Reciprocal Rank Fusion outperforms Condorcet and individual Rank Learning"
3. Sentence-BERT: Reimers & Gurevych (2019) - "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"

---

### üí° Lecciones Aprendidas

#### ‚úÖ Decisiones Acertadas
1. **Embeddings locales**: Sin costos de API, reproducible
2. **SQLite + sqlite-vec**: Simple, portable, sin servidor externo
3. **CLI primero**: Permite iteraci√≥n r√°pida sin GUI
4. **Evaluaci√≥n autom√°tica**: Ground truth con keyword matching
5. **M√∫ltiples modos**: Flexibilidad para diferentes tipos de queries

#### ‚ö†Ô∏è Consideraciones y Mejoras (Actualizadas Sesi√≥n 3)
1. **BM25 en memoria**: Limita escalabilidad a <10k docs
2. **Falta de reranking**: Top-k inicial podr√≠a mejorarse (Sesi√≥n 5)
3. **Test queries limitadas**: 12 queries es insuficiente para conclusiones robustas
4. **Sin cache**: Queries repetidas recalculan embeddings
5. ~~**Metadata limitada**: Sin tracking de fuente, fecha, autor~~ ‚Üí ‚úÖ **RESUELTO en Sesi√≥n 3** (schema v2)
6. **Agentic chunking no probado**: Requiere testing con LLM local (Sesi√≥n 4)
7. **BD anterior con duplicados**: ‚úÖ **RESUELTO** (limpiada, backup creado)

---

### üîó Enlaces √ötiles

- **Repositorio**: https://github.com/Sorl4c/codex-transcript-youtube
- **sqlite-vec docs**: https://github.com/asg017/sqlite-vec
- **sentence-transformers**: https://www.sbert.net/
- **rank-bm25**: https://github.com/dorianbrown/rank_bm25

---

## üìå RESUMEN EJECUTIVO: Sesi√≥n 3 (01 Oct 2025)

### üéØ Objetivo Cumplido
Implementar arquitectura multi-chunking con metadata tracking completo, permitiendo comparar estrategias semantic vs agentic en el mismo sistema RAG.

### ‚úÖ Logros Principales

1. **Schema v2 Implementado**
   - 14 columnas de metadata (vs 3 anteriores)
   - Tracking completo: source_document, source_hash, chunking_strategy
   - 4 √≠ndices para b√∫squedas eficientes
   - Backward compatible con c√≥digo antiguo

2. **Agentic Chunking Mejorado**
   - Prompt actualizado para extraer: title, summary, keywords
   - Keywords guardados en metadata_json
   - Soporte LLM local + Gemini API fallback

3. **Herramienta de Inspecci√≥n**
   - `inspect_db.py` para debugging completo
   - Stats por strategy, detecci√≥n duplicados, an√°lisis diversidad
   - Samples de metadata agentic

4. **Corpus de Prueba Diverso**
   - 5 topics diferentes (Fitness, AI/ML, History, Cooking, Science)
   - 6 archivos listos para ingesta
   - Elimina el problema de "solo resultados de triceps"

5. **BD Limpiada**
   - Backup creado de BD antigua
   - 88 documentos duplicados eliminados
   - Lista para empezar con datos limpios

### üîÑ Cambios en Pipeline de Ingesta

**Antes (Sesi√≥n 1-2):**
```python
ingestor.ingest_text(text)
‚Üí DB: (content, embedding)  # Solo 2 campos
```

**Ahora (Sesi√≥n 3):**
```python
ingestor = RAGIngestor(..., source_document=filepath)
ingestor.ingest_text(text)
‚Üí DB: (content, embedding, source_document, source_hash,
      chunking_strategy, chunk_index, char_start, char_end,
      semantic_title, semantic_summary, keywords, ...)  # 14 campos
```

### üìä Comparaci√≥n de Schemas

| Campo | Schema v1 | Schema v2 |
|-------|-----------|-----------|
| `id` | ‚úÖ | ‚úÖ |
| `content` | ‚úÖ | ‚úÖ |
| `embedding` | ‚úÖ | ‚úÖ |
| `source_document` | ‚ùå | ‚úÖ NUEVO |
| `source_hash` | ‚ùå | ‚úÖ NUEVO |
| `chunking_strategy` | ‚ùå | ‚úÖ NUEVO |
| `chunk_index` | ‚ùå | ‚úÖ NUEVO |
| `char_start/end` | ‚ùå | ‚úÖ NUEVO |
| `semantic_title` | ‚ùå | ‚úÖ NUEVO |
| `semantic_summary` | ‚ùå | ‚úÖ NUEVO |
| `semantic_overlap` | ‚ùå | ‚úÖ NUEVO |
| `metadata_json` | ‚ùå | ‚úÖ NUEVO |
| `created_at` | ‚ùå | ‚úÖ NUEVO |

### üöÄ Pr√≥ximo Paso Inmediato

**Sesi√≥n 4**: Probar agentic chunking con LLM local end-to-end

**Comandos clave:**
```bash
# 1. Test r√°pido con semantic
python -m rag_engine.rag_cli ingest test_agentic.txt --strategy semantico

# 2. Test con agentic (requiere llm_service activo)
python -m rag_engine.rag_cli ingest test_agentic.txt --strategy agentic

# 3. Inspeccionar resultados
python -m rag_engine.inspect_db

# 4. Comparar metadata
# ‚Üí Semantic: title/summary/keywords = NULL
# ‚Üí Agentic: title/summary/keywords = extra√≠dos por LLM
```

### üí° Decisi√≥n Pendiente (Sesi√≥n 4)

¬øAgentic chunking vale la pena el costo computacional?

**Evaluaremos:**
- Calidad de t√≠tulos generados por LLM
- Utilidad de res√∫menes autom√°ticos
- Relevancia de keywords extra√≠dos
- Tiempo de procesamiento vs semantic

**Resultado esperado:**
- ‚úÖ Si la calidad es alta ‚Üí Usar agentic para corpus importantes
- ‚ö†Ô∏è Si la calidad es baja/media ‚Üí Semantic como default (m√°s r√°pido)
