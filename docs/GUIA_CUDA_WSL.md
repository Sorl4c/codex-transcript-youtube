# GuÃ­a completa para configurar llama-cpp-python con soporte CUDA en WSL2 (NVIDIA RTX 5070 Ti)

## ğŸ“Œ Objetivo
Instalar y compilar llama-cpp-python con soporte GPU para poder ejecutar modelos .gguf de forma local y acelerada por CUDA, dentro de WSL2 en Windows, usando una GPU NVIDIA RTX 5070 Ti.

---

## ğŸ§¾ Resumen de pasos realizados

### 1. âœ… Requisitos previos
- Windows 10/11 con WSL2 activado
- DistribuciÃ³n Ubuntu instalada y funcionando bajo WSL2
- Driver NVIDIA actualizado en Windows
- GPU RTX 5070 Ti
- Python 3.12+
- pip, venv, build-essential, cmake, etc.

### 2. ğŸ§ª Verificaciones iniciales
**2.1 Confirmar que Ubuntu usa WSL2:**
- Ejecutar `wsl.exe --list --verbose` en Windows
- âœ… Debe indicar que Ubuntu estÃ¡ en versiÃ³n 2 (Running Version 2)

**2.2 Verificar acceso a GPU desde WSL:**
- Ejecutar `nvidia-smi` en WSL
- âœ… Muestra la GPU desde WSL

### 3. âš™ï¸ Crear entorno virtual Python
- Navegar al directorio del proyecto
- Crear y activar entorno virtual con `python3 -m venv venv-yt-ia` y `source venv-yt-ia/bin/activate`

### 4. âŒ Primer intento fallido de instalaciÃ³n
- Se intentÃ³ compilar con:
  - `CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python`
- âŒ Error: LLAMA_CUBLAS estÃ¡ deprecated. El mensaje sugerÃ­a usar LLAMA_CUDA=on.

### 5. ğŸš¨ Fallo por falta de nvcc
- Ejecutar `nvcc --version`
- âŒ Error: Command 'nvcc' not found
- Esto significaba que el compilador CUDA no estaba instalado en WSL.

### 6. âœ… InstalaciÃ³n del CUDA Toolkit 12.9 en Ubuntu WSL2
**6.1 AÃ±adir repositorio de CUDA:**
- Descargar e instalar el keyring de CUDA
- Actualizar repositorios

**6.2 Instalar toolkit:**
- Instalar `cuda-toolkit-12-9`
- âš ï¸ Aparecieron errores de dependencias

**6.3 Solucionar dependencias:**
- Ejecutar `sudo apt --fix-broken install`
- âœ… Esto descargÃ³ e instalÃ³ +90 paquetes (~4 GB)

### 7. âœ… AÃ±adir CUDA al entorno (.bashrc)
- Comprobar que `nvcc` estÃ¡ instalado en `/usr/local/cuda-12.9/bin/nvcc`
- AÃ±adir variables de entorno en `~/.bashrc`:
  - `export CUDA_HOME=/usr/local/cuda-12.9`
  - `export PATH="$CUDA_HOME/bin:$PATH"`
  - `export LD_LIBRARY_PATH="$CUDA_HOME/lib64:$LD_LIBRARY_PATH"`
- Recargar entorno con `source ~/.bashrc`

### 8. âœ… VerificaciÃ³n de nvcc
- Ejecutar `nvcc --version`
- âœ… Cuda compilation tools, release 12.9, V12.9.86

### 9. ğŸ§© Compilar llama-cpp-python con soporte CUDA
- Ejecutar:
  - `CMAKE_ARGS="-DLLAMA_CUDA=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir`
- âœ… Esta vez no hubo errores, y la compilaciÃ³n se realizÃ³ correctamente.

### 10. ğŸ§ª VerificaciÃ³n desde Python
- Importar en Python:
  - `from llama_cpp import Llama`
  - `print("Llama importado correctamente âœ…")`
- âœ… ImportaciÃ³n exitosa

### 11. ğŸš€ Prueba con modelo real (tinyllama.gguf)
**11.1 Descargar modelo:**
- Ubicado en: `C:\local\modelos\tinyllama.gguf`
- Acceso desde WSL2: `/mnt/c/local/modelos/tinyllama.gguf`

**11.2 Inferencia simple:**
```python
from llama_cpp import Llama
llm = Llama(
    model_path="/mnt/c/local/modelos/tinyllama.gguf",
    n_gpu_layers=-1
)
respuesta = llm("Â¿QuÃ© es una estrella?", max_tokens=50)
print(respuesta["choices"][0]["text"])
```
- âœ… Salida exitosa, acelerada por GPU
- ğŸ•’ Tiempo: ~398 ms
- ğŸ’¬ Respuesta generada correctamente

### ğŸ“¦ RecomendaciÃ³n: guardar como ia/test_modelo.py
```python
from llama_cpp import Llama
llm = Llama(
    model_path="/mnt/c/local/modelos/tinyllama.gguf",
    n_gpu_layers=-1
)
respuesta = llm("Â¿QuÃ© es una estrella?", max_tokens=50)
print(respuesta["choices"][0]["text"])
```

## ğŸ“ Estructura recomendada del proyecto
```
/yt-dlp/
â”œâ”€â”€ main.py
â”œâ”€â”€ gui.py
â”œâ”€â”€ ia/
â”‚   â”œâ”€â”€ test_modelo.py
â”‚   â”œâ”€â”€ ia_processor.py
â”‚   â””â”€â”€ prompts/
â””â”€â”€ venv-yt-ia/
```

---

## ğŸ§  ConclusiÃ³n
El entorno local con llama-cpp-python + CUDA estÃ¡ correctamente configurado.
Puedes usar cualquier modelo .gguf y aprovechar tu GPU para acelerar inferencias.
