# Sistema de Benchmarking para Pipelines de IA

## Visi칩n General

Este m칩dulo permite comparar el rendimiento y la calidad de diferentes pipelines de generaci칩n de res칰menes de texto, incluyendo:

- **Pipeline nativo** (usando directamente `llama-cpp-python`)
- **Pipeline LangChain** (para procesamiento estructurado)
- **Gemini API** (modelos en la nube de Google)

El sistema genera informes detallados en formato Markdown con comparativas visuales f치ciles de interpretar.

## Caracter칤sticas

- **Comparaci칩n de rendimiento** entre m칰ltiples pipelines
- **M칠tricas detalladas**:
  - Tiempo de carga del modelo (local)
  - Latencia de generaci칩n
  - Tokens por segundo
  - Tokens generados y de entrada
  - Costo estimado (para Gemini API)
  - Ratio de compresi칩n (tokens entrada/salida)
  - Longitud del resumen (caracteres y palabras)
  - Riqueza de vocabulario (palabras 칰nicas / total palabras)
- **Visualizaciones integradas** con barras comparativas
- **Informe ejecutivo** con resumen de resultados
- **An치lisis detallado** por prompt
- Soporte para m칰ltiples modelos (locales y en la nube)


### 1. Pipeline Nativo (Local)

```bash
# Configuraci칩n b치sica
export MODEL_PATH="/ruta/al/modelo.gguf"
python bench.py --pipeline native --model_path $MODEL_PATH

# Personalizaci칩n avanzada
python bench.py --pipeline native \
  --model_path "$MODEL_PATH" \
  --n-ctx 4096 \
  --max-tokens 1024 \
  --chunk-size 0  # Desactivar chunking para modelos con contexto amplio
```

### 2. Pipeline LangChain (Local)

```bash
# Configuraci칩n b치sica
python bench.py --pipeline langchain --model_path "$MODEL_PATH"

# Con par치metros personalizados
python bench.py --pipeline langchain \
  --model_path "$MODEL_PATH" \
  --n-ctx 4096 \
  --max-tokens 1024
```

### 3. Gemini API (Nube)

```bash
# Usando variable de entorno
export GEMINI_API_KEY='tu-clave-api'
python bench.py --pipeline gemini

# O especificando la clave directamente
python bench.py --pipeline gemini \
  --gemini-api-key 'tu-clave-api' \
  --gemini-model gemini-1.5-flash-latest  # Modelo por defecto (configurable en ia/gemini_api.py)
```

### 4. Comparaci칩n M칰ltiple

```bash
# Ejecutar m칰ltiples pipelines y comparar
python bench.py --pipeline all \
  --model_path "$MODEL_PATH" \
  --gemini-api-key 'tu-clave-api'
```

### 3. Modelos Soportados
El sistema detecta autom치ticamente los siguientes modelos:

- **TinyLlama**: 2048 tokens de contexto
- **Mistral/Mixtral**: 8192 tokens de contexto
- **LLaMA 2**: 4096 tokens de contexto
- **Otros**: 2048 tokens por defecto

### 3. Generar Informe Comparativo

El script `compare.py` analiza los resultados de los benchmarks y genera un informe detallado en Markdown:

```bash
# Generar informe comparativo
python compare.py

# Especificar archivo de resultados espec칤fico
python compare.py --input bench_results/benchmark_combined_20240618.json
```

### Caracter칤sticas del Informe

1. **Resumen Ejecutivo**
   - Promedios de todas las m칠tricas
   - Ganadores globales por categor칤a
   - Estad칤sticas generales

2. **An치lisis Detallado por Prompt**
   - Tablas comparativas con m칠tricas
   - Barras visuales para comparaci칩n
   - Texto completo de prompts y respuestas
   - Secciones plegables para mejor legibilidad

3. **M칠tricas Incluidas**
   - Tiempos de procesamiento
   - Rendimiento en tokens/segundo
   - Estad칤sticas de tokens
   - Calidad del resumen (longitud, vocabulario)
   - Ratios de compresi칩n

4. **Visualizaciones**
   - Barras comparativas
   - Indicadores de ganador
   - Formato de tabla para f치cil lectura

El informe se guarda en `bench_results/comparison_report.md` por defecto.

## Estructura del Proyecto

- `bench.py`: Script principal para ejecutar benchmarks
- `compare.py`: Genera informes comparativos
- `ia/native_pipeline.py`: Implementaci칩n del pipeline nativo
- `ia/langchain_pipeline.py`: Implementaci칩n del pipeline LangChain
- `ia/core.py`: Funcionalidad compartida
- `bench_results/`: Directorio con resultados e informes

## Requisitos

- Python 3.8+
- Dependencias listadas en `requirements.txt`
- Modelos GGUF compatibles

## Personalizaci칩n

### Par치metros Ajustables

- `--temperature`: Controla la aleatoriedad (0.0 a 1.0)
- `--max_tokens`: M치ximo de tokens por generaci칩n
- `--top_p`: Muestreo de n칰cleo (nucleus sampling)
- `--chunk_size`: Tama침o de fragmentos para procesamiento

### Prompts Personalizados

Crea archivos en `ia/prompts/`:
- `map_summary.txt`: Para la fase de mapeo
- `reduce_summary.txt`: Para la fase de reducci칩n

## Interpretaci칩n de Resultados

### M칠tricas Clave

1. **Tiempo de Carga**: Tiempo que tarda en cargar el modelo en memoria (solo local).
2. **Latencia**: Tiempo total de generaci칩n del resumen.
3. **Tokens/seg**: Velocidad de generaci칩n del modelo.
4. **Costo (Gemini)**: Costo estimado en USD basado en tokens E/S.
5. **Tokens (E/S)**: Relaci칩n entre tokens de entrada y salida.
6. **Compresi칩n**: Porcentaje de reducci칩n de tama침o.
7. **Longitud**: Tama침o del resumen generado.
8. **Vocabulario**: Riqueza l칠xica del resumen.

### Ejemplo de Salida

#### Modelo Local
```
=== RESULTADOS DEL BENCHMARK ===

Modelo: mistral-7b-instruct-v0.1.Q5_K_M.gguf
Pipeline: native

M칠tricas:
- Tiempo de carga: 5.23s
- Latencia: 18.76s
- Tokens/s: 38.4
- Tokens (E/S): 1,245/312 (4.0x compresi칩n)
- Longitud: 2,145 caracteres (415 palabras)
- Vocabulario: 0.62 (palabras 칰nicas/total)
```

#### Gemini API
```
=== RESULTADOS DEL BENCHMARK ===

Modelo: gemini-1.5-flash-latest
Pipeline: gemini

M칠tricas:
- Latencia: 3.21s
- Tokens (E/S): 1,856/429 (4.3x compresi칩n)
- Costo: $0.00091 USD
  - Entrada (1,856 tokens): $0.00065
  - Salida (429 tokens): $0.00026
- Longitud: 2,845 caracteres (512 palabras)
- Vocabulario: 0.68 (palabras 칰nicas/total)
```

### S칤mbolos en los Informes

- **Native**: Pipeline nativo obtuvo mejor resultado
- **LangChain**: Pipeline de LangChain obtuvo mejor resultado
- **Gemini**: Pipeline de Gemini obtuvo mejor resultado
- **Empate**: Ambos pipelines obtuvieron resultados equivalentes
- 游볟 **LangChain**: Pipeline de LangChain obtuvo mejor resultado
- 游뱋 **Empate**: Ambos pipelines obtuvieron resultados equivalentes

## Soluci칩n de Problemas

### Problemas Comunes con Modelos Locales
- **Modelo no encontrado**: Verifica la ruta al archivo GGUF
- **Falta de memoria**: Reduce el tama침o de los fragmentos o usa un modelo m치s peque침o
- **Errores de CUDA**: Verifica la instalaci칩n de los controladores NVIDIA

### Problemas con Gemini API
- **Error 403 (Permiso denegado)**: Verifica que la clave de API sea correcta y tenga permisos
- **Error 429 (L칤mite de cuota)**: Espera o aumenta los l칤mites en Google Cloud Console
- **Tiempos de espera**: Verifica tu conexi칩n a Internet o aumenta el timeout

## Mejores Pr치cticas

1. **Para desarrollo local**:
   - Usa modelos m치s peque침os (TinyLlama, 7B cuantizados)
   - Limita el tama침o del contexto con `--n-ctx`
   - Usa `--chunk-size 0` para modelos con contexto amplio

2. **Para producci칩n con Gemini**:
   - Monitorea el uso de la API y costos
   - Implementa cach칠 para reducir llamadas repetitivas
   - Usa `gemini-1.5-flash-latest` para mejor relaci칩n costo-rendimiento

3. **Optimizaci칩n de costos**:
   - Procesa lotes de texto juntos cuando sea posible
   - Usa el modelo m치s peque침o que cumpla con tus requisitos
   - Considera procesamiento h칤brido (local + nube)

## Pr칩ximas Mejoras

- Soporte para m칠tricas de calidad autom치ticas (ROUGE, BLEU)
- Integraci칩n con TensorBoard para visualizaci칩n
- Pruebas de estr칠s y carga
- Soporte para m치s modelos y frameworks
