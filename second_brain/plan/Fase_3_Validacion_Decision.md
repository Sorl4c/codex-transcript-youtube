# Fase 3: Validaci√≥n Comparativa y Decisi√≥n Final

## Objetivo del D√≠a

Ejecutar queries de prueba para cada embedder, comparar calidad de retrieval y tomar decisi√≥n final sobre el embedder a utilizar.

**Duraci√≥n estimada:** 4-5 horas
**Resultado esperado:** Matriz de decisi√≥n completa con embedder final seleccionado y justificaci√≥n objetiva.

## Checklist Completo

### ‚úÖ Paso 1: Preparaci√≥n y Configuraci√≥n (30 minutos)

#### Checklist:
- [ ] Validar que Fase 2 complet√≥ exitosamente
- [ ] Cargar matriz de ingesti√≥n de Fase 2
- [ ] Seleccionar embedder con mejor rendimiento de Fase 2
- [ ] Configurar base de datos con el embedder seleccionado
- [ ] Preparar 5 queries de referencia

#### Comandos:
```bash
# 1.1 Validar que Fase 2 complet√≥
if [ ! -f "second_brain/plan/results/fase_2_summary.json" ]; then
    echo "‚ùå Fase 2 no complet√≥. Ejecutar Fase 2 primero."
    exit 1
fi

# 1.2 Analizar resultados de Fase 2
echo "üìä Analizando resultados de Fase 2..."
python -c "
import json
with open('second_brain/plan/results/ingestion_matrix.json', 'r') as f:
    matrix = json.load(f)

print('üèÜ Embedders por rendimiento:')
for row in sorted(matrix['matrix_data'], key=lambda x: x['Performance_Score'], reverse=True):
    print(f'   {row[\"Performance_Score\"]:>3}/100 {row[\"Embedder\"]}')

best = max(matrix['matrix_data'], key=lambda x: x['Performance_Score'])
print(f'\\nüéØ Mejor performer: {best[\"Embedder\"]} ({best[\"Performance_Score\"]}/100)')
print(f'   Modelo: {best[\"Model\"]}')
print(f'   Dimensi√≥n: {best[\"Dimension\"]}')
"

# 1.3 Configurar variables para el mejor embedder
echo "‚öôÔ∏è Configurando para el mejor embedder de Fase 2..."
python -c "
import json
with open('second_brain/plan/results/ingestion_matrix.json', 'r') as f:
    matrix = json.load(f)

best = max(matrix['matrix_data'], key=lambda x: x['Performance_Score'])

# Crear configuraci√≥n para el mejor embedder
config = f'''# PostgreSQL Configuration
DATABASE_BACKEND=postgresql
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=rag_experiments
POSTGRES_USER=postgres
POSTGRES_PASSWORD=tu_password

# Embedding Configuration - Fase 3 (Best Performer)
EMBEDDING_MODEL={best['Model']}
EMBEDDING_DIM={best['Dimension']}

# Experiment Configuration
EXPERIMENT_MODE=true
SAVE_RAW_OUTPUTS=true
LOG_LEVEL=DEBUG
PHASE=fase_3
BEST_PERFORMER={best['Embedder']}
'''

with open('.env.fase3', 'w') as f:
    f.write(config)

print(f'‚úÖ Configuraci√≥n creada para: {best[\"Embedder\"]}')
print(f'   Modelo: {best[\"Model\"]}')
print(f'   Dimensi√≥n: {best[\"Dimension\"]}')
"

# 1.4 Usar configuraci√≥n de Fase 3
export $(cat .env.fase3 | xargs)
echo "‚úÖ Variables configuradas:"
echo "   EMBEDDING_MODEL: $EMBEDDING_MODEL"
echo "   EMBEDDING_DIM: $EMBEDDING_DIM"
echo "   BEST_PERFORMER: $BEST_PERFORMER"
```

### ‚úÖ Paso 2: Definir Queries de Referencia (30 minutos)

#### Checklist:
- [ ] Crear archivo con 5 queries estandarizadas
- [ ] Definir expected results para cada query
- [ ] Validar que los conceptos existen en el dataset
- [ ] Preparar sistema de evaluaci√≥n
- [ ] Crear plantilla para guardar resultados

#### Archivo: `queries_referencia.py`
```python
#!/usr/bin/env python3
"""
Queries de referencia para validaci√≥n de retrieval
"""

QUERIES_REFERENCIA = [
    {
        "id": 1,
        "query": "¬øQu√© es Docker y c√≥mo funciona?",
        "category": "tecnologia_definicion",
        "expected_concepts": ["docker", "contenedores", "virtualizaci√≥n"],
        "expected_min_chunks": 1,
        "expected_max_chunks": 5,
        "language": "espa√±ol",
        "difficulty": "b√°sico"
    },
    {
        "id": 2,
        "query": "¬øC√≥mo se instala PostgreSQL?",
        "category": "proceso_instalacion",
        "expected_concepts": ["postgresql", "instalaci√≥n", "base de datos"],
        "expected_min_chunks": 1,
        "expected_max_chunks": 4,
        "language": "espa√±ol",
        "difficulty": "b√°sico"
    },
    {
        "id": 3,
        "query": "¬øQu√© son los embeddings?",
        "category": "concepto_tecnico",
        "expected_concepts": ["embeddings", "vectores", "representaci√≥n"],
        "expected_min_chunks": 1,
        "expected_max_chunks": 5,
        "language": "espa√±ol",
        "difficulty": "intermedio"
    },
    {
        "id": 4,
        "query": "¬øPara qu√© sirve el chunking?",
        "category": "aplicacion_practica",
        "expected_concepts": ["chunking", "trozos", "procesamiento"],
        "expected_min_chunks": 1,
        "expected_max_chunks": 4,
        "language": "espa√±ol",
        "difficulty": "intermedio"
    },
    {
        "id": 5,
        "query": "Sistemas de bases de datos",
        "category": "tema_general",
        "expected_concepts": ["base de datos", "sistemas", "almacenamiento"],
        "expected_min_chunks": 2,
        "expected_max_chunks": 6,
        "language": "espa√±ol",
        "difficulty": "variable"
    }
]

def save_queries():
    """Guardar queries en archivo JSON"""
    import json
    with open("second_brain/plan/queries_referencia.json", "w", encoding="utf-8") as f:
        json.dump(QUERIES_REFERENCIA, f, indent=2, ensure_ascii=False)
    print("‚úÖ Queries de referencia guardadas")

def load_queries():
    """Cargar queries desde archivo"""
    import json
    with open("second_brain/plan/queries_referencia.json", "r", encoding="utf-8") as f:
        return json.load(f)

if __name__ == "__main__":
    save_queries()
    print(f"\nüìã Queries de referencia preparadas:")
    for query in QUERIES_REFERENCIA:
        print(f"   {query['id']}. {query['query']}")
        print(f"      Categor√≠a: {query['category']}")
        print(f"      Conceptos esperados: {', '.join(query['expected_concepts'])}")
```

#### Comandos:
```bash
# 2.1 Crear y guardar queries
python queries_referencia.py
# Esperado: ‚úÖ Queries de referencia guardadas

# 2.2 Validar que las queries est√°n listas
cat second_brain/plan/queries_referencia.json | python -m json.tool | head -20
```

### ‚úÖ Paso 3: Ejecutar Queries con Cada Embedder (90 minutos)

#### Checklist:
- [ ] Para cada embedder (MiniLM, MPNet, Multiling√ºe):
  - [ ] Cargar modelo y configurar BD
  - [ ] Ejecutar las 5 queries
  - [ ] Guardar resultados crudos
  - [ ] Medir tiempos de respuesta
  - [ ] Evaluar relevancia subjetiva

#### Archivo: `execute_validation.py`
```python
#!/usr/bin/env python3
"""
Ejecutar validaci√≥n de queries para todos los embedders
"""
import os
import json
import time
import psutil
from sentence_transformers import SentenceTransformer
import numpy as np

from postgresql_database_experimental import PostgreSQLVectorDatabase
from queries_referencia import load_queries

def clear_model_cache():
    """Limpiar cache de modelos"""
    import torch
    import gc
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()

def configure_for_embedder(embedder_name):
    """Configurar variables de entorno para un embedder espec√≠fico"""
    embedder_configs = {
        'MiniLM (Control)': {
            'model': 'all-MiniLM-L6-v2',
            'dim': 384,
            'phase': 'fase_3_minilm'
        },
        'MPNet (Alta Calidad)': {
            'model': 'all-mpnet-base-v2',
            'dim': 768,
            'phase': 'fase_3_mpnet'
        },
        'Multilingual (Espa√±ol)': {
            'model': 'paraphrase-multilingual-MiniLM-L12-v2',
            'dim': 384,
            'phase': 'fase_3_multilingual'
        }
    }

    config = embedder_configs.get(embedder_name)
    if not config:
        raise ValueError(f"Embedder no reconocido: {embedder_name}")

    # Configurar variables de entorno
    os.environ['EMBEDDING_MODEL'] = config['model']
    os.environ['EMBEDDING_DIM'] = str(config['dim'])
    os.environ['PHASE'] = config['phase']

    return config

def setup_database_for_embedder(embedder_name, config):
    """Configurar base de datos para un embedder espec√≠fico"""
    print(f"üîå Configurando BD para: {embedder_name}")

    # Limpiar tablas existentes
    from postgresql_database_experimental import PostgreSQLVectorDatabase
    temp_db = PostgreSQLVectorDatabase()

    with temp_db.pool.connection() as conn:
        with conn.cursor() as cursor:
            cursor.execute("DROP TABLE IF EXISTS document_embeddings CASCADE;")
            cursor.execute("DROP TABLE IF EXISTS documents CASCADE;")

    # Recrear schema con dimensi√≥n correcta
    os.system(f"EMBEDDING_DIM={config['dim']} python setup_schema.py")

    # Cargar datos para este embedder desde logs de Fase 2
    phase_files = {
        'MiniLM (Control)': 'fase_1_metrics.json',
        'MPNet (Alta Calidad)': 'fase_2a_mpnet_metrics.json',
        'Multilingual (Espa√±ol)': 'fase_2b_multilingual_metrics.json'
    }

    phase_file = phase_files.get(embedder_name)
    if not phase_file:
        raise ValueError(f"No se encontraron datos para {embedder_name}")

    # Cargar dataset original
    with open("second_brain/plan/experimental_dataset.json", 'r', encoding='utf-8') as f:
        dataset = json.load(f)

    # Cargar modelo
    print(f"ü§ñ Cargando modelo: {config['model']}")
    model = SentenceTransformer(config['model'])

    # Preparar documentos
    documents = []
    for i, chunk_data in enumerate(dataset):
        content = chunk_data['content']
        metadata = {
            'source_document': chunk_data['source_document'],
            'source_hash': chunk_data['source_hash'],
            'chunking_strategy': 'experimental',
            'chunk_index': chunk_data['chunk_index'],
            'semantic_title': f"{embedder_name} chunk {i+1}",
            'semantic_summary': content[:100] + "..." if len(content) > 100 else content,
            'additional_metadata': {
                'phase': config['phase'],
                'model': config['model'],
                'dimension': config['dim'],
                'validation_phase': True
            }
        }
        documents.append((content, None, metadata))

    # Generar embeddings
    print(f"üßÆ Generando embeddings para {len(documents)} chunks...")
    texts = [doc[0] for doc in documents]
    embeddings = model.encode(texts, convert_to_numpy=True)

    # Insertar en BD
    documents_with_embeddings = []
    for (content, _, metadata), embedding in zip(documents, embeddings):
        documents_with_embeddings.append((content, embedding.tolist(), metadata))

    db = PostgreSQLVectorDatabase()
    db.add_documents_with_metadata(documents_with_embeddings)

    stats = db.get_stats()
    print(f"‚úÖ BD configurada: {stats}")

    return db, model

def evaluate_relevance(query_text, results, expected_concepts):
    """Evaluar relevancia subjetiva de los resultados"""
    if not results:
        return 0, "No results"

    score = 0
    feedback = []

    # Verificar si los resultados contienen conceptos esperados
    concept_matches = 0
    for content, similarity in results[:3]:  # Evaluar top 3
        content_lower = content.lower()
        found_concepts = [concept for concept in expected_concepts if concept.lower() in content_lower]
        if found_concepts:
            concept_matches += len(found_concepts)
            score += 2  # 2 puntos por concepto encontrado
            feedback.append(f"‚úÖ Conceptos encontrados: {found_concepts}")

    # Bonus por similitud alta
    if results and results[0][1] > 0.7:
        score += 1
        feedback.append(f"‚úÖ Alta similitud: {results[0][1]:.3f}")

    # Penalizar si no hay conceptos esperados
    if concept_matches == 0:
        score -= 2
        feedback.append("‚ùå No se encontraron conceptos esperados")

    # Normalizar score (0-5)
    score = max(0, min(5, score))

    return score, "; ".join(feedback)

def execute_validation_for_embedder(embedder_name):
    """Ejecutar validaci√≥n completa para un embedder"""
    print(f"\nüöÄ Iniciando validaci√≥n para: {embedder_name}")
    print("="*60)

    start_time = time.time()

    # Configurar para este embedder
    config = configure_for_embedder(embedder_name)

    # Limpiar cache antes de cargar modelo
    clear_model_cache()

    # Configurar base de datos
    db, model = setup_database_for_embedder(embedder_name, config)

    # Cargar queries de referencia
    queries = load_queries()
    print(f"üìã Cargadas {len(queries)} queries de referencia")

    # Ejecutar queries
    results = {}
    total_query_time = 0

    for query_info in queries:
        query_id = query_info['id']
        query_text = query_info['query']

        print(f"\nüîç Ejecutando query {query_id}: {query_text}")

        # Medir tiempo de query
        query_start = time.time()

        # Generar embedding de la query
        query_embedding = model.encode([query_text])[0].tolist()

        # Ejecutar b√∫squeda
        search_results = db.search_similar(query_embedding, top_k=5)

        query_time = time.time() - query_start
        total_query_time += query_time

        # Evaluar relevancia
        relevance_score, feedback = evaluate_relevance(
            query_text, search_results, query_info['expected_concepts']
        )

        # Guardar resultados
        results[f"query_{query_id}"] = {
            'query': query_text,
            'category': query_info['category'],
            'expected_concepts': query_info['expected_concepts'],
            'expected_range': (query_info['expected_min_chunks'], query_info['expected_max_chunks']),
            'results_count': len(search_results),
            'query_time_ms': int(query_time * 1000),
            'relevance_score': relevance_score,
            'relevance_feedback': feedback,
            'top_results': [
                {
                    'content': content[:200] + "...",
                    'similarity': float(similarity),
                    'length': len(content)
                }
                for content, similarity in search_results[:3]
            ]
        }

        print(f"   ‚è±Ô∏è Tiempo: {query_time*1000:.0f}ms")
        print(f"   üìä Resultados: {len(search_results)} chunks")
        print(f"   üéØ Relevancia: {relevance_score}/5 - {feedback}")

    # Calcular m√©tricas agregadas
    avg_relevance = np.mean([r['relevance_score'] for r in results.values()])
    avg_query_time = total_query_time / len(queries)
    total_time = time.time() - start_time

    validation_results = {
        'embedder': embedder_name,
        'model': config['model'],
        'dimension': config['dim'],
        'total_time_s': round(total_time, 2),
        'avg_query_time_ms': round(avg_query_time * 1000, 1),
        'avg_relevance_score': round(avg_relevance, 2),
        'total_queries': len(queries),
        'successful_queries': len([r for r in results.values() if r['relevance_score'] > 0]),
        'results': results,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    }

    # Guardar resultados
    phase_name = config['phase']
    results_file = f"second_brain/plan/logs/{phase_name}_validation.json"

    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(validation_results, f, indent=2, ensure_ascii=False)

    print(f"\nüìä Resumen para {embedder_name}:")
    print(f"   Tiempo total: {total_time:.2f}s")
    print(f"   Tiempo promedio por query: {avg_query_time*1000:.1f}ms")
    print(f"   Relevancia promedio: {avg_relevance:.2f}/5")
    print(f"   Queries exitosas: {validation_results['successful_queries']}/{len(queries)}")
    print(f"   üìÑ Resultados guardados en: {results_file}")

    db.close()
    clear_model_cache()

    return validation_results

def run_all_validations():
    """Ejecutar validaciones para todos los embedders"""
    print("üéØ Iniciando validaci√≥n completa para todos los embedders")
    print("="*80)

    embedders = [
        'MiniLM (Control)',
        'MPNet (Alta Calidad)',
        'Multilingual (Espa√±ol)'
    ]

    all_results = {}

    for embedder in embedders:
        try:
            result = execute_validation_for_embedder(embedder)
            all_results[embedder] = result
            print(f"‚úÖ Validaci√≥n completada: {embedder}")
        except Exception as e:
            print(f"‚ùå Error en validaci√≥n {embedder}: {e}")
            all_results[embedder] = {'error': str(e)}

    return all_results

if __name__ == "__main__":
    results = run_all_validations()
    print(f"\nüéâ Todas las validaciones completadas!")
```

#### Comandos:
```bash
# 3.1 Crear script de validaci√≥n
# (Crear el archivo execute_validation.py con el contenido de arriba)

# 3.2 Ejecutar validaci√≥n completa
python execute_validation.py
# Esperado: üéØ Validaci√≥n completada para los 3 embedders con m√©tricas detalladas
```

### ‚úÖ Paso 4: Crear Matriz de Decisi√≥n (60 minutos)

#### Checklist:
- [ ] Cargar resultados de los 3 embedders
- [ ] Calcular scores de decisi√≥n ponderados
- [ ] Crear matriz comparativa final
- [ ] Analizar trade-offs entre calidad y rendimiento
- [ ] Generar recomendaci√≥n final

#### Archivo: `create_decision_matrix.py`
```python
#!/usr/bin/env python3
"""
Crear matriz de decisi√≥n final basada en resultados de validaci√≥n
"""
import json
import os
import numpy as np
from datetime import datetime

def load_validation_results():
    """Cargar resultados de validaci√≥n de todos los embedders"""
    results = {}

    validation_files = {
        'MiniLM (Control)': 'second_brain/plan/logs/fase_3_minilm_validation.json',
        'MPNet (Alta Calidad)': 'second_brain/plan/logs/fase_3_mpnet_validation.json',
        'Multilingual (Espa√±ol)': 'second_brain/plan/logs/fase_3_multilingual_validation.json'
    }

    for embedder, file_path in validation_files.items():
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                results[embedder] = json.load(f)
            print(f"‚úÖ Cargado: {embedder}")
        else:
            print(f"‚ùå No encontrado: {file_path}")

    return results

def load_ingestion_metrics():
    """Cargar m√©tricas de ingesti√≥n de Fase 2"""
    with open('second_brain/plan/results/ingestion_matrix.json', 'r', encoding='utf-8') as f:
        matrix = json.load(f)

    ingestion_metrics = {}
    for row in matrix['matrix_data']:
        ingestion_metrics[row['Embedder']] = row

    return ingestion_metrics

def calculate_decision_scores(validation_results, ingestion_metrics):
    """Calcular scores de decisi√≥n ponderados"""

    # Definir ponderaciones (total = 100%)
    weights = {
        'relevance': 0.35,      # 35% - Calidad de resultados
        'response_time': 0.25, # 25% - Velocidad de respuesta
        'resource_usage': 0.15, # 15% - Uso de memoria/CPU
        'spanish_support': 0.15, # 15% - Soporte para espa√±ol
        'stability': 0.10       # 10% - Consistencia y errores
    }

    decision_matrix = []

    for embedder, validation_data in validation_results.items():
        if 'error' in validation_data:
            print(f"‚ö†Ô∏è Omitiendo {embedder} por errores")
            continue

        ingestion_data = ingestion_metrics.get(embedder, {})

        # Calcular scores normalizados (0-100)

        # 1. Relevancia (35%)
        relevance_score = validation_data.get('avg_relevance_score', 0) * 20  # 0-5 -> 0-100

        # 2. Tiempo de respuesta (25%) - menor es mejor
        avg_time_ms = validation_data.get('avg_query_time_ms', 1000)
        # Normalizar: 100ms = 100 puntos, 1000ms = 0 puntos
        time_score = max(0, 100 - (avg_time_ms - 100) * 100 / 900)

        # 3. Uso de recursos (15%) - menor es mejor
        memory_mb = ingestion_data.get('Model_Memory_MB', 100)
        # Normalizar: 100MB = 100 puntos, 1000MB = 0 puntos
        resource_score = max(0, 100 - (memory_mb - 100) * 100 / 900)

        # 4. Soporte espa√±ol (15%)
        if 'multilingual' in embedder.lower():
            spanish_score = 100
        elif 'mpnet' in embedder.lower():
            spanish_score = 70  # Bueno pero no especializado
        else:
            spanish_score = 50  # B√°sico

        # 5. Estabilidad (10%) - queries exitosas
        successful_queries = validation_data.get('successful_queries', 0)
        total_queries = validation_data.get('total_queries', 5)
        stability_score = (successful_queries / total_queries) * 100

        # Calcular score total ponderado
        total_score = (
            relevance_score * weights['relevance'] +
            time_score * weights['response_time'] +
            resource_score * weights['resource_usage'] +
            spanish_score * weights['spanish_support'] +
            stability_score * weights['stability']
        )

        # Redondear a 1 decimal
        total_score = round(total_score, 1)

        decision_matrix.append({
            'Embedder': embedder,
            'Model': ingestion_data.get('Model', 'N/A'),
            'Dimension': ingestion_data.get('Dimension', 'N/A'),

            # M√©tricas individuales (normalizadas 0-100)
            'Relevance_Score': round(relevance_score, 1),
            'Response_Time_Score': round(time_score, 1),
            'Resource_Score': round(resource_score, 1),
            'Spanish_Support_Score': spanish_score,
            'Stability_Score': round(stability_score, 1),

            # Score total
            'Total_Score': total_score,

            # M√©tricas originales para referencia
            'Original_Relevance': round(validation_data.get('avg_relevance_score', 0), 2),
            'Original_Time_ms': round(avg_time_ms, 1),
            'Original_Memory_MB': round(memory_mb, 1),
            'Successful_Queries': f"{successful_queries}/{total_queries}",

            # Metadata
            'Weights': weights
        })

    return decision_matrix

def create_final_decision_matrix(decision_matrix):
    """Crear matriz de decisi√≥n final con an√°lisis"""

    print("üìä CREANDO MATRIZ DE DECISI√ìN FINAL")
    print("="*100)

    # Ordenar por score total
    decision_matrix.sort(key=lambda x: x['Total_Score'], reverse=True)

    # Mostrar matriz
    header = [
        "Embedder",
        "Relevance",
        "Time",
        "Resource",
        "Spanish",
        "Stability",
        "TOTAL"
    ]

    print(f"{header[0]:<25} {header[1]:<10} {header[2]:<8} {header[3]:<10} {header[4]:<10} {header[5]:<10} {header[6]:<8}")
    print("-"*100)

    for row in decision_matrix:
        print(f"{row['Embedder']:<25} {row['Relevance_Score']:<10} "
              f"{row['Response_Time_Score']:<8} {row['Resource_Score']:<10} "
              f"{row['Spanish_Support_Score']:<10} {row['Stability_Score']:<10} {row['Total_Score']:<8}")

    print("="*100)

    # An√°lisis detallado
    print(f"\nüîç AN√ÅLISIS DETALLADO")

    winner = decision_matrix[0]
    print(f"üèÜ GANADOR: {winner['Embedder']}")
    print(f"   Score Total: {winner['Total_Score']}/100")
    print(f"   Modelo: {winner['Model']}")
    print(f"   Dimensi√≥n: {winner['Dimension']}")

    print(f"\nüìà PUNTUACIONES DETALLADAS:")
    print(f"   Relevancia: {winner['Relevance_Score']}/100 "
          f"(original: {winner['Original_Relevance']}/5)")
    print(f"   Tiempo Respuesta: {winner['Response_Time_Score']}/100 "
          f"(original: {winner['Original_Time_ms']}ms)")
    print(f"   Uso Recursos: {winner['Resource_Score']}/100 "
          f"(original: {winner['Original_Memory_MB']}MB)")
    print(f"   Soporte Espa√±ol: {winner['Spanish_Support_Score']}/100")
    print(f"   Estabilidad: {winner['Stability_Score']}/100 "
          f"(original: {winner['Successful_Queries']})")

    # Comparaci√≥n con otros
    print(f"\nüîÑ COMPARACI√ìN CON ALTERNATIVAS:")
    for i, row in enumerate(decision_matrix[1:], 1):
        diff = row['Total_Score'] - winner['Total_Score']
        print(f"   {i}. {row['Embedder']}: {row['Total_Score']} "
              f"({diff:+.1f} vs ganador)")

    # Recomendaciones basadas en uso
    print(f"\nüí° RECOMENDACIONES POR CASO DE USO:")

    # Mejor relevancia
    best_relevance = max(decision_matrix, key=lambda x: x['Relevance_Score'])
    print(f"   üéØ Mejor Relevancia: {best_relevance['Embedder']} "
          f"({best_relevance['Relevance_Score']}/100)")

    # M√°s r√°pido
    fastest = max(decision_matrix, key=lambda x: x['Response_Time_Score'])
    print(f"   ‚ö° M√°s R√°pido: {fastest['Embedder']} "
          f"({fastest['Response_Time_Score']}/100)")

    # Mejor para espa√±ol
    best_spanish = max(decision_matrix, key=lambda x: x['Spanish_Support_Score'])
    print(f"   üåç Mejor para Espa√±ol: {best_spanish['Embedder']} "
          f"({best_spanish['Spanish_Support_Score']}/100)")

    # M√°s ligero
    lightest = max(decision_matrix, key=lambda x: x['Resource_Score'])
    print(f"   üíæ M√°s Ligero: {lightest['Embedder']} "
          f"({lightest['Resource_Score']}/100)")

    return decision_matrix

def generate_final_report(decision_matrix, validation_results, ingestion_metrics):
    """Generar reporte final completo"""

    winner = decision_matrix[0]

    report = {
        'experiment_completed': True,
        'generated_at': datetime.now().isoformat(),
        'final_decision': {
            'selected_embedder': winner['Embedder'],
            'model': winner['Model'],
            'dimension': winner['Dimension'],
            'total_score': winner['Total_Score'],
            'recommendation_strength': 'ALTA' if winner['Total_Score'] > 75 else 'MEDIA',
            'confidence': 'ALTO' if winner['Stability_Score'] > 80 else 'MEDIO'
        },
        'decision_matrix': decision_matrix,
        'weights_used': {
            'relevance': 0.35,
            'response_time': 0.25,
            'resource_usage': 0.15,
            'spanish_support': 0.15,
            'stability': 0.10
        },
        'validation_summary': validation_results,
        'ingestion_summary': ingestion_metrics,
        'next_steps': {
            'implement_selected_embedder': True,
            'proceed_to_refactor': winner['Total_Score'] > 70,
            'monitor_performance': True,
            'document_learnings': True
        }
    }

    # Guardar reporte final
    report_file = "second_brain/plan/results/final_decision_report.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    print(f"\nüìÑ Reporte final guardado en: {report_file}")

    # Crear resumen ejecutivo
    executive_summary = f"""
# üéØ DECISI√ìN FINAL - EMBEDDER SELECCIONADO

## Ganador: {winner['Embedder']}

### Configuraci√≥n Recomendada:
- **Modelo:** {winner['Model']}
- **Dimensi√≥n:** {winner['Dimension']}
- **Score Total:** {winner['Total_Score']}/100
- **Confianza:** {report['final_decision']['confidence']}

### M√©tricas Clave:
- **Relevancia:** {winner['Original_Relevance']}/5 ‚≠ê
- **Tiempo Respuesta:** {winner['Original_Time_ms']}ms ‚ö°
- **Memoria:** {winner['Original_Memory_MB']}MB üíæ
- **Queries Exitosas:** {winner['Successful_Queries']} ‚úÖ

### Justificaci√≥n:
Este embedder obtuvo el mejor balance entre calidad de resultados, rendimiento y uso de recursos seg√∫n la ponderaci√≥n definida.

### Recomendaci√≥n:
Implementar este embedder como backend definitivo del sistema RAG.
"""

    summary_file = "second_brain/plan/results/executive_summary.md"
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(executive_summary)

    print(f"üìÑ Resumen ejecutivo guardado en: {summary_file}")

    return report

def main():
    """Funci√≥n principal de creaci√≥n de matriz de decisi√≥n"""
    print("üöÄ Iniciando creaci√≥n de matriz de decisi√≥n final...")

    # Cargar resultados
    validation_results = load_validation_results()
    ingestion_metrics = load_ingestion_metrics()

    if len(validation_results) < 2:
        print("‚ùå Se necesitan resultados de al menos 2 embedders para comparar")
        return

    # Calcular matriz de decisi√≥n
    decision_matrix = calculate_decision_scores(validation_results, ingestion_metrics)

    # Crear matriz final
    final_matrix = create_final_decision_matrix(decision_matrix)

    # Generar reporte final
    final_report = generate_final_report(final_matrix, validation_results, ingestion_metrics)

    print(f"\nüéâ Matriz de decisi√≥n completada exitosamente!")

    return final_report

if __name__ == "__main__":
    main()
```

#### Comandos:
```bash
# 4.1 Crear script de decisi√≥n
# (Crear el archivo create_decision_matrix.py con el contenido de arriba)

# 4.2 Ejecutar an√°lisis de decisi√≥n
python create_decision_matrix.py
# Esperado: üìä Matriz de decisi√≥n final con embedder ganador
```

### ‚úÖ Paso 5: Validaci√≥n y Documentaci√≥n (30 minutos)

#### Checklist:
- [ ] Verificar que la matriz de decisi√≥n se cre√≥
- [ ] Validar que hay un ganador claro
- [ ] Revisar que todos los archivos est√°n guardados
- [ ] Crear documentaci√≥n final
- [ ] Preparar recomendaciones para Fase 4

#### Comandos:
```bash
# 5.1 Validar archivos finales
echo "üîç Validando archivos finales del experimento..."
echo "M√©tricas de ingesti√≥n:"
ls -la second_brain/plan/results/ingestion_matrix.json

echo -e "\nM√©tricas de validaci√≥n:"
ls -la second_brain/plan/logs/fase_*_validation.json

echo -e "\nDecisi√≥n final:"
ls -la second_brain/plan/results/final_decision_report.json

# 5.2 Validar que hay un ganador claro
echo -e "\nüèÜ Validando ganador..."
python -c "
import json
with open('second_brain/plan/results/final_decision_report.json', 'r') as f:
    report = json.load(f)

winner = report['final_decision']['selected_embedder']
score = report['final_decision']['total_score']
confidence = report['final_decision']['confidence']

print(f'‚úÖ Ganador: {winner}')
print(f'‚úÖ Score: {score}/100')
print(f'‚úÖ Confianza: {confidence}')

if score > 75:
    print('üéØ DECISI√ìN CLARA: Ganador con score alto')
elif score > 60:
    print('‚öñÔ∏è DECISI√ìN MODERADA: Ganador con score medio')
else:
    print('‚ùì DECISI√ìN DUDOSA: Ning√∫n embedder claramente superior')
"

# 5.3 Resumen de todo el experimento
echo -e "\nüìä RESUMEN COMPLETO DEL EXPERIMENTO"
python -c "
import json
import os

# Contar archivos generados
logs = len([f for f in os.listdir('second_brain/plan/logs') if f.endswith('.json')])
results = len([f for f in os.listdir('second_brain/plan/results') if f.endswith('.json')])

print(f'üìÅ Archivos de logs generados: {logs}')
print(f'üìÅ Archivos de resultados: {results}')

# Cargar decisi√≥n final
with open('second_brain/plan/results/final_decision_report.json', 'r') as f:
    final = json.load(f)

print(f'\\nüéØ ESTADO FINAL:')
print(f'   ‚úÖ Experimento completado: {final[\"experiment_completed\"]}')
print(f'   ‚úÖ Embedder seleccionado: {final[\"final_decision\"][\"selected_embedder\"]}')
print(f'   ‚úÖ Proceder a refactor: {final[\"next_steps\"][\"proceed_to_refactor\"]}')
"

# 5.4 Crear resumen para documentaci√≥n
cat > second_brain/plan/EXPERIMENTO_COMPLETO.md << EOF
# üéØ Experimento RAG PostgreSQL - Resumen Completo

## Objetivo Alcanzado
‚úÖ Evaluar 3 embedders diferentes para seleccionar el mejor para el sistema RAG

## Embedders Probados
1. **all-MiniLM-L6-v2** (Control - 384 dimensiones)
2. **all-mpnet-base-v2** (Alta calidad - 768 dimensiones)
3. **paraphrase-multilingual-MiniLM-L12-v2** (Multiling√ºe - 384 dimensiones)

## M√©tricas Evaluadas
- **Calidad de retrieval** (relevancia de resultados)
- **Tiempo de respuesta** (latencia)
- **Uso de recursos** (memoria/CPU)
- **Soporte para espa√±ol** (cobertura ling√º√≠stica)
- **Estabilidad** (consistencia de resultados)

## Dataset
- **15 chunks** seleccionados de transcripts_for_rag
- **5 queries** de referencia estandarizadas
- **Evaluaci√≥n subjetiva** 0-5 puntos por query

## Decision Key
Ponderaci√≥n: Relevancia(35%) + Tiempo(25%) + Recursos(15%) + Espa√±ol(15%) + Estabilidad(10%)

## Resultados
Consultar el archivo \`final_decision_report.json\` para detalles completos.

## Pr√≥ximos Pasos
1. Implementar embedder ganador en producci√≥n
2. Refactorizar sistema RAG actual
3. Monitorear rendimiento en producci√≥n
4. Documentar aprendizajes

---

*Experimento completado en $(date)*
EOF

echo "üìÑ Resumen del experimento guardado en: second_brain/plan/EXPERIMENTO_COMPLETO.md"
```

## Criterios de √âxito de Fase 3

### ‚úÖ √âxito si:
- Las 5 queries se ejecutan para cada embedder
- Matriz de decisi√≥n creada con scores ponderados
- Hay un ganador claro con score > 60/100
- M√©tricas de calidad y rendimiento documentadas
- Recomendaci√≥n final justificada
- Todos los archivos de resultados guardados

### ‚ùå Fracaso si:
- Menos de 2 embedders completan validaci√≥n
- No hay un ganador claro (scores muy similares)
- Las m√©tricas de calidad no son fiables
- Errores en las queries de prueba
- No se puede tomar decisi√≥n informada

## Entregables de Fase 3

1. **Resultados de validaci√≥n** por embedder - `fase_*_validation.json`
2. **Matriz de decisi√≥n** con scores ponderados - `final_decision_report.json`
3. **Resumen ejecutivo** con recomendaci√≥n - `executive_summary.md`
4. **An√°lisis comparativo** detallado - incluido en reporte final
5. **Documentaci√≥n completa** del experimento - `EXPERIMENTO_COMPLETO.md`

## Decisi√≥n Final Esperada

Basado en el an√°lisis, el embedder seleccionado deber√≠a tener:

### Score > 75/100 (Decisi√≥n Clara)
- **Mejor balance** entre calidad y rendimiento
- **Relevancia** > 3.0/5 en promedio
- **Tiempo respuesta** < 500ms promedio
- **Estabilidad** > 80% (queries exitosas)

### Score 60-75/100 (Decisi√≥n Moderada)
- **Trade-offs identificados** claramente
- **Ventajas espec√≠ficas** para ciertos casos de uso
- **Recomendaciones condicionales** seg√∫n prioridades

### Score < 60/100 (Decisi√≥n Dudosa)
- **Ning√∫n embedder claramente superior**
- **Problemas fundamentales** en retrieval
- **Necesita m√°s investigaci√≥n** o ajustes

## Preparaci√≥n para Fase 4 (Opcional)

Al completar Fase 3 exitosamente, tendr√°s:
- **Embedder ganador** seleccionado objetivamente
- **Configuraci√≥n √≥ptima** definida y probada
- **M√©tricas baseline** para comparaci√≥n futura
- **Confianza** en la decisi√≥n t√©cnica
- **Roadmap claro** para implementaci√≥n

**Siguiente paso:** Si se decide proceder, implementar `PostgreSQLVectorDatabase` definitiva con el embedder ganador y refactorizar el sistema actual.

## Impacto del Experimento

Este experimento proporciona:
1. **Base objetiva** para decisiones t√©cnicas
2. **Experiencia pr√°ctica** con diferentes embedders
3. **M√©tricas reales** de rendimiento y calidad
4. **Proceso reproducible** para futuras evaluaciones
5. **Confianza** en la arquitectura seleccionada

El conocimiento adquirido es directamente aplicable al proyecto RAG del ERP en tu trabajo.