# Golden Corpus para Evaluaci√≥n RAG - Plan de Implementaci√≥n

**Fecha:** 2025-10-18
**Archivo:** `second_brain/plan/Golden_Corpus_Plan.md`
**Tipo:** [Investigaci√≥n + Plan] Dificultad (‚≠ê‚≠ê‚≠ê)
**Estado:** üìã Planificado para Fase 2.5+

## Objetivo

Definir e implementar un golden corpus est√°ndar para evaluaci√≥n de sistemas RAG, basado en mejores pr√°cticas de la industria y referencias acad√©micas.

## Fuentes Principales Investigadas

### 1. Hugging Face Cookbook - RAG Evaluation ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- **URL**: https://huggingface.co/learn/cookbook/rag_evaluation
- **Enfoque**: S√≠ntesis con LLMs + filtros de calidad
- **Key Insight**: Chunk size tuning es "f√°cil y muy impactante"

### 2. RAGAS Documentation ‚≠ê‚≠ê‚≠ê‚≠ê
- **URL**: https://docs.ragas.io
- **Enfoque**: M√©tricas comprehensivas para RAG
- **Key Insight**: Framework de evaluaci√≥n con m√©tricas espec√≠ficas

### 3. A Comprehensive Review on RAG (arXiv:2402.19473) ‚≠ê‚≠ê‚≠ê‚≠ê
- **URL**: https://arxiv.org/abs/2402.19473
- **Enfoque**: Framework de evaluaci√≥n actualizado
- **Key Insight**: "No single good recipe" cuando se tunea RAG

---

## Mejores Pr√°cticas Identificadas

### üèóÔ∏è **Construcci√≥n del Dataset**

#### **S√≠ntesis con LLMs**
- Usar LLMs para generar QA pairs desde la knowledge base
- Implementar filtros de calidad con agentes LLM

#### **Filtros de Calidad (Escala 1-5)**
- **‚úÖ Groundedness**: "¬øLa pregunta puede responderse desde el contexto?"
- **‚úÖ Relevance**: "¬ø√ötil para practicantes de ML construyendo apps NLP?"
- **‚úÖ Standalone**: "Claridad de la pregunta independiente del contexto"

### üìè **Criterios de Representatividad**

#### **Chunk Size Recomendado**
- **200+ tokens** (seg√∫n Hugging Face)
- Chunk size tuning es "f√°cil y muy impactante"

#### **Query Selection**
- Queries representativas del use case real
- Diversidad de tipos de preguntas
- Diferentes niveles de dificultad

### üéØ **Evaluaci√≥n de Retrieval**

#### **M√©tricas Clave**
- **Answer Correctness** como m√©trica end-to-end
- **GPT-4 como juez** con rubric detallada (1-5 scale)
- **Rationale antes del scoring** para mejor consistencia

#### **Tests de Configuraci√≥n**
- Chunk sizes (200+ tokens recomendado)
- Embedding models
- Reranking on/off

### üé® **Principios de Dise√±o**

#### **Caracter√≠sticas del Golden Corpus**
- **Reducido pero representativo**: No necesita ser masivo
- **Variabilidad tem√°tica**: Cubrir dominios diferentes
- **Dificultad graduada**: Queries f√°ciles y dif√≠ciles
- **Ground truth verificable**: Respuestas conocidas

---

## Estado Actual vs Ideal

### üìä **Dataset Actual (18 chunks) - Evaluaci√≥n**

#### ‚úÖ **Fortalezas**
- Temas t√©cnicos relevantes (Docker, PostgreSQL, embeddings)
- Tama√±o manejable para experimentaci√≥n
- Queries representativas ya probadas
- Performance < 10ms por b√∫squeda
- Scores de similitud 0.60-0.71 funcionales

#### üîß **Mejoras Aplicables**
1. **A√±adir QA pairs** con Ground truth known
2. **Implementar filtros** de groundedness (1-5)
3. **Diversificar queries** para diferentes dominios
4. **Documentar metadata** de cada chunk
5. **Expandir a 30-50 chunks** con m√°s diversidad tem√°tica

---

## Plan de Implementaci√≥n

### üöÄ **Fase 2.5: Golden Corpus Enhancement**

#### **Paso 1: An√°lisis del Dataset Actual**
```python
# Evaluaci√≥n de chunks existentes
- Analizar diversidad tem√°tica actual
- Identificar gaps de cobertura
- Evaluar calidad de contenido
- Documentar metadata existente
```

#### **Paso 2: Expansi√≥n Controlada con Corpus Sint√©tico**
```python
# Crear corpus sint√©tico multitem√°tico: 15 chunks por categor√≠a = 45 total
- Distribuci√≥n: 15 chunks tecnolog√≠a + 15 chunks aprendizaje/productividad + 15 chunks deporte/bienestar
- Formato: document_id + chunk_index (mantiener compatibilidad)
- Chunk size: 200+ tokens optimizado para RAG
- Metadata enriquecida por chunk:
  * categoria (tecnologia/aprendizaje/deporte)
  * fuente (ficticia pero realista)
  * titulo (descriptivo del contenido)
  * resumen (breve descripci√≥n)
- Preservar estructura actual para compatibilidad
- Objetivo: Medir relevancia cruzada entre dominios
```

#### **Paso 3: Generaci√≥n QA Pairs por Chunk**
```python
# S√≠ntesis con LLMs - 3 QA pairs por chunk = 135 QA pairs totales
- Tipos de preguntas por chunk:
  * 1 pregunta factual (qu√©, qui√©n, d√≥nde, cu√°ndo)
  * 1 pregunta procedimental (c√≥mo, paso a paso)
  * 1 pregunta reflexiva (por qu√©, qu√© pasar√≠a si, comparaci√≥n)
- Sistema de scoring 1-5 por criterio:
  * Groundedness: ¬øSe puede responder desde el chunk?
  * Relevance: ¬øEs √∫til para usuarios reales?
  * Claridad: ¬øEs independiente y comprensible?
- Validaci√≥n con GPT-4 + filtros autom√°ticos
- Crear respuestas ground truth con citas textuales
- Total: 45 chunks √ó 3 QA pairs = 135 pares QA completos
```

#### **Paso 4: M√©tricas de Evaluaci√≥n**
```python
# Framework completo
- Retrieval metrics (precision, recall, F1)
- Answer correctness (GPT-4 como juez)
- Relevance scoring (1-5)
- Groundedness assessment
```

### üìà **Fase 4: Refactorizaci√≥n con Golden Corpus**

#### **Integraci√≥n con Nuevo Schema**
- Aplicar principios de golden corpus al schema normalizado
- Mantener compatibilidad con queries existentes
- A√±adir vistas para evaluaci√≥n espec√≠fica
- Documentar best practices para futuros datasets

---

## M√©tricas de Success

### üìä **KPIs del Golden Corpus**

#### **Calidad del Dataset**
- **Diversidad tem√°tica**: ‚â• 5 dominios diferentes
- **Complejidad variada**: Queries f√°ciles, medias, dif√≠ciles
- **Ground truth verificable**: 100% de QA pairs validados
- **Filtros de calidad**: Promedio ‚â• 4.0/5.0

#### **Performance de Retrieval**
- **Coverage**: ‚â• 80% de queries con resultados relevantes
- **Precision**: Top-3 relevance ‚â• 0.7
- **Speed**: < 50ms por query
- **Consistency**: Scores estables across runs

#### **Evaluci√≥n End-to-End**
- **Answer correctness**: ‚â• 4.0/5.0 (GPT-4 judge)
- **Groundedness**: ‚â• 4.0/5.0
- **Relevance**: ‚â• 4.0/5.0
- **User satisfaction**: M√©tricas cualitativas

---

## Dataset de Referencia Propuesto

### üèÜ **Estructura Ideal del Golden Corpus**

#### **Categor√≠as Tem√°ticas (30-50 chunks)**
1. **DevOps & Infrastructure** (20%)
   - Docker, Kubernetes, CI/CD
   - Cloud platforms (AWS, GCP, Azure)
   - Monitoring & observability

2. **Machine Learning & AI** (25%)
   - Modelos y algoritmos
   - Frameworks (TensorFlow, PyTorch)
   - MLOps y deployment

3. **Software Architecture** (20%)
   - Microservicios y distributed systems
   - Design patterns
   - API development

4. **Data Engineering** (15%)
   - Databases (SQL, NoSQL, vector)
   - Data pipelines
   - Streaming y real-time

5. **Security & Best Practices** (10%)
   - Cybersecurity fundamentals
   - Code security
   - Compliance

6. **Domain Specific** (10%)
   - Industry verticals
   - Specialized applications
   - Emerging technologies

#### **QA Pairs por Chunk**
- **3-5 preguntas** por chunk
- **Diferentes tipos**: Factual, conceptual, procedimental
- **Complejidad graduada**: F√°cil ‚Üí Dif√≠cil
- **Ground truth documentado**

---

## Herramientas y Frameworks

### üõ†Ô∏è **Tecnolog√≠as Recomendadas**

#### **Generaci√≥n y Evaluaci√≥n**
- **RAGAS**: M√©tricas comprehensivas
- **Hugging Face**: S√≠ntesis con LLMs
- **GPT-4**: Judge para answer correctness
- **Custom filters**: Groundedness y relevance

#### **Almacenamiento y Retrieval**
- **PostgreSQL + pgvector**: Nuestra implementaci√≥n actual
- **Metadata enriquecida**: JSON schema extendido
- **Versioning**: Track de cambios en dataset

#### **Testing y Validaci√≥n**
- **Unit tests**: Validaci√≥n de QA pairs
- **Integration tests**: End-to-end evaluation
- **Benchmarking**: Comparaci√≥n de configuraciones

---

## Plan de Testing y Fixtures

### üß™ **Roadmap de Testing para Corpus Sint√©tico**

#### **Unit Tests**
```python
# Test de generaci√≥n de corpus sint√©tico
def test_corpus_generation():
    - Validar 45 chunks totales (15 por categor√≠a)
    - Verificar metadata requerida (categoria, fuente, titulo, resumen)
    - Comprobar chunk size >= 200 tokens
    - Validar formatos document_id + chunk_index

# Test de QA pairs generation
def test_qa_pairs_quality():
    - Verificar 3 QA pairs por chunk
    - Validar tipos: factual, procedimental, reflexiva
    - Comprobar scoring >= 4.0/5.0 en todos los criterios
    - Test de ground truth citations
```

#### **Integration Tests**
```python
# Test de ingesti√≥n multitem√°tica
def test_multitematic_ingestion():
    - Ingerir corpus sint√©tico completo
    - Validar distribuci√≥n por categor√≠as
    - Verificar b√∫squeda cruzada entre dominios
    - Comprobar performance < 50ms

# Test de evaluaci√≥n RAG
def test_rag_evaluation():
    - Ejecutar 135 QA pairs queries
    - Medir answer correctness
    - Validar groundedness y relevance
    - Comprobar cross-domain relevance
```

#### **Fixtures Generados**
```python
# Fixtures por categor√≠a
@pytest.fixture
def tecnologia_corpus():
    return generate_synthetic_chunks("tecnologia", count=15)

@pytest.fixture
def aprendizaje_corpus():
    return generate_synthetic_chunks("aprendizaje", count=15)

@pytest.fixture
def deporte_corpus():
    return generate_synthetic_chunks("deporte", count=15)

# Fixture completo del corpus
@pytest.fixture
def corpus_multitematico():
    return {
        "tecnologia": tecnologia_corpus(),
        "aprendizaje": aprendizaje_corpus(),
        "deporte": deporte_corpus()
    }

# Fixture de QA pairs
@pytest.fixture
def qa_pairs_by_category():
    return generate_qa_pairs(corpus_multitematico())
```

### üéØ **Objetivos de Testing**

1. **Validar generaci√≥n consistente** de corpus sint√©tico
2. **Asegurar calidad de QA pairs** con scoring autom√°tico
3. **Verificar cross-domain relevance** entre categor√≠as
4. **Medir rendimiento** con dataset multitem√°tico real
5. **Comprobar backward compatibility** con estructura actual

## Riesgos y Mitigaci√≥n

### ‚ö†Ô∏è **Riesgos Identificados**

#### **Quality Control**
- **Riesgo**: QA pairs de baja calidad
- **Mitigaci√≥n**: Filtros LLM + validaci√≥n humana

#### **Bias y Representatividad**
- **Riesgo**: Dataset sesgado hacia dominios espec√≠ficos
- **Mitigaci√≥n**: Diversificaci√≥n intencional y m√©tricas de balance

#### **Scalability**
- **Riesgo**: Dataset crece demasiado
- **Mitigaci√≥n**: Principios de minimalidad efectiva

#### **Maintainability**
- **Riesgo**: Dataset se vuelve obsoleto
- **Mitigaci√≥n**: Versioning y actualizaci√≥n peri√≥dica

---

## Timeline Sugerido

### üìÖ **Fechas Estimadas**

#### **Sprint 1 (Week 1-2): Analysis & Planning**
- Evaluar dataset actual
- Definir requerimientos espec√≠ficos
- Dise√±ar estructura del golden corpus

#### **Sprint 2 (Week 3-4): Expansion**
- Generar chunks adicionales
- Implementar diversidad tem√°tica
- A√±adir metadata enriquecida

#### **Sprint 3 (Week 5-6): QA Generation**
- S√≠ntesis de QA pairs con LLMs
- Implementar filtros de calidad
- Validaci√≥n iterativa

#### **Sprint 4 (Week 7-8): Integration**
- Integrar con pipeline RAG existente
- Implementar m√©tricas de evaluaci√≥n
- Documentaci√≥n completa

---

## Success Metrics Dashboard

### üìä **KPIs para Seguimiento**

#### **Dataset Quality**
- [ ] Total chunks: 30-50
- [ ] Dominios cubiertos: ‚â•5
- [ ] QA pairs totales: ‚â•100
- [ ] Quality score promedio: ‚â•4.0/5.0

#### **Performance Metrics**
- [ ] Retrieval precision@3: ‚â•0.7
- [ ] Answer correctness: ‚â•4.0/5.0
- [ ] Query response time: <50ms
- [ ] Coverage rate: ‚â•80%

#### **Evaluation Framework**
- [ ] M√©tricas automatizadas funcionando
- [ ] Dashboard de monitoreo activo
- [ ] Reports generados autom√°ticamente
- [ ] Benchmark baseline establecido

---

## Pr√≥ximos Pasos

### üéØ **Acciones Inmediatas**

1. **Validaci√≥n del Plan**: Review con stakeholder
2. **Priorizaci√≥n**: Definir qu√© dominios expandir primero
3. **Tool Setup**: Preparar entorno de generaci√≥n QA
4. **Baseline Capture**: Documentar estado actual como referencia

### üîÑ **Proceso Iterativo**

1. **Sprint Planning**: Definir objetivos por sprint
2. **Development**: Implementaci√≥n incremental
3. **Testing**: Validaci√≥n continua
4. **Review**: Retroalimentaci√≥n y ajustes

---

## Referencias Adicionales

### üìö **Material de Estudio**

#### **Papers Acad√©micos**
- RAG Survey: https://arxiv.org/abs/2402.19473
- RAG Evaluation: https://arxiv.org/abs/2312.10997

#### **Documentaci√≥n T√©cnica**
- RAGAS: https://docs.ragas.io
- Hugging Face Cookbook: https://huggingface.co/learn/cookbook/rag_evaluation

#### **Industry Best Practices**
- Pinecone RAG Evaluation Series
- Weaviate RAG Best Practices
- IBM RAG Evaluation Guidelines

---

## Conclusiones

### üí° **Key Takeaways**

1. **Dataset actual es buen punto de partida** pero necesita expansi√≥n
2. **Calidad sobre cantidad**: 30-50 chunks bien curados > miles sin validar
3. **Filtros LLM son esenciales** para mantener calidad
4. **M√©tricas estandarizadas** cruciales para comparaci√≥n
5. **Iteraci√≥n continua** para mantener relevancia

### üöÄ **Ready for Implementation**

El golden corpus propuesto seguir√° las mejores pr√°cticas de la industria y proporcionar√° una base s√≥lida para evaluaci√≥n de sistemas RAG, permitiendo comparaciones justas y mejoras medibles en nuestro pipeline.

---

*√öltima actualizaci√≥n: 2025-10-18*
*Status: Planificado para implementaci√≥n en Fase 2.5*
*Corpus Sint√©tico: 45 chunks (15√ó3 categor√≠as) con 135 QA pairs*