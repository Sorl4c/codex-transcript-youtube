# Fase 1: Setup PostgreSQL + Ingestión Controlada

## Objetivo del Día

Crear infraestructura PostgreSQL limpia y validar ingesta con el modelo base (all-MiniLM-L6-v2).

**Duración estimada:** 3-4 horas
**Resultado esperado:** PostgreSQL funcionando con pgvector, schema paramétrico creado, y 10-20 chunks ingeridos sin duplicados.

## Checklist Completo

### ✅ Paso 1: Verificación de Instalación PostgreSQL (30 minutos)

#### Checklist:
- [ ] Verificar versión PostgreSQL instalada
- [ ] Confirmar pgvector disponible
- [ ] Crear base de datos experimental
- [ ] Configurar usuario y permisos
- [ ] Probar conexión básica

#### Comandos:
```bash
# 1.1 Verificar instalación PostgreSQL
psql --version
# Esperado: PostgreSQL 13+ (recomendado 14+)

# 1.2 Verificar si pgvector está disponible
psql -d postgres -c "SELECT * FROM pg_available_extensions WHERE name = 'vector';"
# Esperado: vector | 1.0+ | ... (disponible)

# 1.3 Crear base de datos experimental
createdb rag_experiments
# Esperado: CREATE DATABASE

# 1.4 Probar conexión a nueva BD
psql -d rag_experiments -c "SELECT current_database(), current_user;"
# Esperado: rag_experiments | tu_usuario

# 1.5 Instalar pgvector si no está disponible
# (Ejecutar solo si paso 1.2 no muestra pgvector)
# Consultar documentación oficial para tu SO
```

#### Validación:
```bash
# Verificar que todo funciona
psql -d rag_experiments -c "
CREATE EXTENSION IF NOT EXISTS vector;
SELECT version(), extversion FROM pg_extension WHERE extname = 'vector';
"
# Esperado: PostgreSQL version + pgvector version
```

### ✅ Paso 2: Configuración de Entorno (20 minutos)

#### Checklist:
- [ ] Activar virtual environment
- [ ] Configurar variables de entorno
- [ ] Instalar dependencias necesarias
- [ ] Crear directorios para logs y resultados
- [ ] Verificar imports de Python

#### Comandos:
```bash
# 2.1 Activar entorno virtual
source .venv/Scripts/activate
# O para Linux/Mac:
# source .venv/bin/activate

# 2.2 Configurar variables de entorno
cat >> .env << EOF
# PostgreSQL Configuration
DATABASE_BACKEND=postgresql
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=rag_experiments
POSTGRES_USER=postgres
POSTGRES_PASSWORD=tu_password

# Embedding Configuration - Fase 1
EMBEDDING_MODEL=all-MiniLM-L6-v2
EMBEDDING_DIM=384

# Experiment Configuration
EXPERIMENT_MODE=true
SAVE_RAW_OUTPUTS=true
LOG_LEVEL=DEBUG
EOF

# 2.3 Instalar dependencias si es necesario
pip install psycopg[binary,pool] sentence-transformers

# 2.4 Crear directorios para logs
mkdir -p second_brain/plan/logs
mkdir -p second_brain/plan/results
mkdir -p second_brain/plan/outputs
```

#### Validación:
```bash
# 2.5 Verificar imports
python -c "
import psycopg
import psycopg_pool
import sentence_transformers
import numpy as np
import os
print('✅ Todas las dependencias importadas correctamente')
print(f'✅ EMBEDDING_DIM: {os.getenv(\"EMBEDDING_DIM\")}')
print(f'✅ POSTGRES_DB: {os.getenv(\"POSTGRES_DB\")}')
"
```

### ✅ Paso 3: Schema Paramétrico PostgreSQL (45 minutos)

#### Checklist:
- [ ] Crear script de schema SQL
- [ ] Implementar configuración paramétrica en Python
- [ ] Crear tablas con validaciones
- [ ] Configurar índices vectoriales
- [ ] Validar constraints funcionan

#### Archivo: `create_schema.sql`
```sql
-- Crear extensión pgvector
CREATE EXTENSION IF NOT EXISTS vector;

-- Tabla de documentos con hash único
CREATE TABLE IF NOT EXISTS documents (
    id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    content TEXT NOT NULL,
    source_hash VARCHAR(64) UNIQUE NOT NULL,
    source_document TEXT,
    chunking_strategy VARCHAR(50) DEFAULT 'agentic',
    chunk_index INTEGER,
    char_start INTEGER,
    char_end INTEGER,
    semantic_title VARCHAR(500),
    semantic_summary TEXT,
    semantic_overlap TEXT,
    embedding_model VARCHAR(100),
    metadata_json JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Tabla de embeddings con dimensión paramétrica
CREATE TABLE IF NOT EXISTS document_embeddings (
    id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    document_id BIGINT REFERENCES documents(id) ON DELETE CASCADE,
    embedding vector(384), -- Se reemplazará dinámicamente
    embedding_type VARCHAR(50) DEFAULT 'content',
    embedding_model VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    UNIQUE(document_id)
);

-- Índice vectorial
CREATE INDEX IF NOT EXISTS idx_document_embeddings_vector
    ON document_embeddings
    USING ivfflat (embedding vector_cosine_ops)
    WITH (lists = 100);

-- Índices de soporte
CREATE INDEX IF NOT EXISTS idx_documents_source_hash ON documents(source_hash);
CREATE INDEX IF NOT EXISTS idx_documents_strategy_created ON documents(chunking_strategy, created_at DESC);
CREATE INDEX IF NOT EXISTS idx_documents_metadata_jsonb ON documents USING gin(metadata_json);
```

#### Archivo: `setup_schema.py`
```python
#!/usr/bin/env python3
"""
Script para crear schema PostgreSQL con dimensión paramétrica
"""
import os
import psycopg
from psycopg_pool import ConnectionPool

def create_schema_with_dimension():
    """Crear schema con dimensión configurada desde variables de entorno"""

    # Leer configuración
    embedding_dim = int(os.getenv('EMBEDDING_DIM', '384'))
    db_name = os.getenv('POSTGRES_DB', 'rag_experiments')

    print(f"🔧 Creando schema para PostgreSQL: {db_name}")
    print(f"📏 Dimensión de embeddings: {embedding_dim}")

    # Leer y modificar schema SQL
    with open('create_schema.sql', 'r') as f:
        schema_sql = f.read()

    # Reemplazar dimensión fija con variable
    schema_sql = schema_sql.replace('vector(384)', f'vector({embedding_dim})')

    # Conectar y ejecutar
    conn_string = (
        f"host={os.getenv('POSTGRES_HOST', 'localhost')} "
        f"port={os.getenv('POSTGRES_PORT', 5432)} "
        f"dbname={db_name} "
        f"user={os.getenv('POSTGRES_USER', 'postgres')} "
        f"password={os.getenv('POSTGRES_PASSWORD', '')}"
    )

    with psycopg.connect(conn_string) as conn:
        with conn.cursor() as cur:
            cur.execute(schema_sql)
        conn.commit()

    print(f"✅ Schema creado exitosamente con dimensión {embedding_dim}")

if __name__ == "__main__":
    create_schema_with_dimension()
```

#### Comandos:
```bash
# 3.1 Crear archivo de schema
# (Crear el archivo create_schema.sql con el contenido de arriba)

# 3.2 Crear script de setup
# (Crear el archivo setup_schema.py con el contenido de arriba)

# 3.3 Ejecutar creación de schema
python setup_schema.py
# Esperado: 🔧 Creando schema... ✅ Schema creado exitosamente
```

#### Validación:
```bash
# 3.4 Validar que las tablas se crearon correctamente
psql -d rag_experiments -c "
SELECT table_name, column_name, data_type, is_nullable
FROM information_schema.columns
WHERE table_name IN ('documents', 'document_embeddings')
ORDER BY table_name, ordinal_position;
"

# 3.5 Validar que pgvector funciona
psql -d rag_experiments -c "
SELECT
    table_name,
    indexname,
    indexdef
FROM pg_indexes
WHERE tablename LIKE '%embedding%';
"
```

### ✅ Paso 4: Selección y Preparación de Dataset (30 minutos)

#### Checklist:
- [ ] Explorar directorio transcripts_for_rag
- [ ] Seleccionar 10-20 chunks representativos
- [ ] Validar que los chunks tienen contenido variado
- [ ] Crear lista controlada para experimentos
- [ ] Documentar chunks seleccionados

#### Comandos:
```bash
# 4.1 Explorar archivos disponibles
ls -la transcripts_for_rag/
find transcripts_for_rag/ -name "*.md" -o -name "*.txt" | head -10

# 4.2 Contar líneas y contenido de archivos
wc -l transcripts_for_rag/*.md
wc -l transcripts_for_rag/*.txt
```

#### Archivo: `prepare_dataset.py`
```python
#!/usr/bin/env python3
"""
Preparar dataset controlado para experimentos
"""
import os
import hashlib
from pathlib import Path

def prepare_experimental_dataset():
    """Seleccionar y preparar dataset de 10-20 chunks"""

    transcripts_dir = Path("transcripts_for_rag")
    selected_chunks = []

    # Buscar archivos disponibles
    available_files = []
    for ext in ['*.md', '*.txt']:
        available_files.extend(transcripts_dir.glob(ext))

    print(f"📁 Archivos encontrados: {len(available_files)}")

    # Seleccionar contenido variado
    target_chunks = 15  # Entre 10-20 chunks
    content_samples = []

    for file_path in available_files[:3]:  # Máximo 3 archivos
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # Dividir en chunks (simulación simple)
            lines = content.split('\n')
            current_chunk = []

            for line in lines:
                if line.strip():
                    current_chunk.append(line.strip())

                    if len(current_chunk) >= 3 and len(' '.join(current_chunk)) > 100:
                        chunk_text = ' '.join(current_chunk)
                        chunk_hash = hashlib.md5(chunk_text.encode()).hexdigest()

                        content_samples.append({
                            'content': chunk_text,
                            'source_hash': chunk_hash,
                            'source_document': str(file_path.name),
                            'chunk_index': len(content_samples)
                        })

                        current_chunk = []

                        if len(content_samples) >= target_chunks:
                            break

            if len(content_samples) >= target_chunks:
                break

        except Exception as e:
            print(f"⚠️ Error procesando {file_path}: {e}")
            continue

    # Guardar dataset
    import json
    dataset_file = "second_brain/plan/experimental_dataset.json"

    with open(dataset_file, 'w', encoding='utf-8') as f:
        json.dump(content_samples[:target_chunks], f, indent=2, ensure_ascii=False)

    print(f"✅ Dataset preparado: {len(content_samples[:target_chunks])} chunks")
    print(f"📄 Guardado en: {dataset_file}")

    # Mostrar resumen
    for i, chunk in enumerate(content_samples[:5]):
        print(f"\n📝 Chunk {i+1}:")
        print(f"   Fuente: {chunk['source_document']}")
        print(f"   Hash: {chunk['source_hash'][:16]}...")
        print(f"   Content: {chunk['content'][:100]}...")

    return content_samples[:target_chunks]

if __name__ == "__main__":
    prepare_experimental_dataset()
```

#### Comandos:
```bash
# 4.3 Preparar dataset experimental
python prepare_dataset.py
# Esperado: ✅ Dataset preparado: 15 chunks
```

### ✅ Paso 5: Implementación de PostgreSQLVectorDatabase (60 minutos)

#### Checklist:
- [ ] Crear clase PostgreSQLVectorDatabase básica
- [ ] Implementar conexión con pool
- [ ] Implementar método add_documents
- [ ] Implementar método search_similar
- [ ] Validar prevención de duplicados

#### Archivo: `postgresql_database_experimental.py`
```python
#!/usr/bin/env python3
"""
PostgreSQLVectorDatabase para experimentos controlados
"""
import os
import json
import hashlib
import time
from typing import List, Tuple, Dict, Any, Optional
import psycopg
from psycopg_pool import ConnectionPool
import numpy as np

class PostgreSQLVectorDatabase:
    """Implementación PostgreSQL para experimentos con embedders"""

    def __init__(self):
        """Inicializar conexión PostgreSQL"""
        self.embedding_dim = int(os.getenv('EMBEDDING_DIM', '384'))
        self.embedding_model = os.getenv('EMBEDDING_MODEL', 'all-MiniLM-L6-v2')

        # Pool de conexiones
        self.pool = ConnectionPool(
            conninfo=self._build_connection_string(),
            min_size=2,
            max_size=10,
            timeout=30
        )

        print(f"🔌 PostgreSQLVectorDatabase inicializado")
        print(f"📏 Dimensión: {self.embedding_dim}")
        print(f"🤖 Modelo: {self.embedding_model}")

    def _build_connection_string(self) -> str:
        """Construir connection string desde variables de entorno"""
        return (
            f"host={os.getenv('POSTGRES_HOST', 'localhost')} "
            f"port={os.getenv('POSTGRES_PORT', 5432)} "
            f"dbname={os.getenv('POSTGRES_DB', 'rag_experiments')} "
            f"user={os.getenv('POSTGRES_USER', 'postgres')} "
            f"password={os.getenv('POSTGRES_PASSWORD', '')}"
        )

    def add_documents_with_metadata(self, documents: List[Tuple[str, List[float], dict]]):
        """
        Insertar documentos con metadata y embeddings

        Args:
            documents: Lista de (content, embedding, metadata_dict)
        """
        if not documents:
            return

        start_time = time.time()
        print(f"📥 Insertando {len(documents)} documentos...")

        with self.pool.connection() as conn:
            with conn.cursor() as cursor:
                for i, (content, embedding, metadata) in enumerate(documents):
                    # Validar dimensión
                    if len(embedding) != self.embedding_dim:
                        raise ValueError(
                            f"Dimensión incorrecta: esperado {self.embedding_dim}, "
                            f"recibido {len(embedding)}"
                        )

                    # Generar source_hash si no existe
                    source_hash = metadata.get('source_hash') or hashlib.md5(
                        content.encode('utf-8')
                    ).hexdigest()

                    try:
                        # Insertar documento (con ON CONFLICT para evitar duplicados)
                        cursor.execute("""
                            INSERT INTO documents (
                                content, source_hash, source_document,
                                chunking_strategy, chunk_index, char_start, char_end,
                                semantic_title, semantic_summary, semantic_overlap,
                                embedding_model, metadata_json
                            ) VALUES (
                                %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
                            )
                            ON CONFLICT (source_hash) DO UPDATE SET
                                content = EXCLUDED.content,
                                embedding_model = EXCLUDED.embedding_model
                            RETURNING id
                        """, (
                            content,
                            source_hash,
                            metadata.get('source_document'),
                            metadata.get('chunking_strategy', 'agentic'),
                            metadata.get('chunk_index'),
                            metadata.get('char_start'),
                            metadata.get('char_end'),
                            metadata.get('semantic_title'),
                            metadata.get('semantic_summary'),
                            metadata.get('semantic_overlap'),
                            self.embedding_model,
                            json.dumps(metadata.get('additional_metadata', {}))
                            if metadata.get('additional_metadata') else None
                        ))

                        doc_id = cursor.fetchone()[0]

                        # Insertar embedding
                        cursor.execute("""
                            INSERT INTO document_embeddings (
                                document_id, embedding, embedding_model
                            ) VALUES (%s, %s, %s)
                            ON CONFLICT (document_id) DO UPDATE SET
                                embedding = EXCLUDED.embedding,
                                embedding_model = EXCLUDED.embedding_model
                        """, (doc_id, embedding, self.embedding_model))

                        print(f"✅ Documento {i+1}/{len(documents)} insertado (hash: {source_hash[:16]}...)")

                    except Exception as e:
                        print(f"❌ Error insertando documento {i+1}: {e}")
                        continue

            conn.commit()

        elapsed_time = time.time() - start_time
        print(f"🎉 Inserción completada en {elapsed_time:.2f}s")

    def search_similar(self, query_embedding: List[float], top_k: int = 5) -> List[Tuple[str, float]]:
        """
        Buscar documentos similares usando pgvector

        Args:
            query_embedding: Vector de consulta
            top_k: Número de resultados a devolver

        Returns:
            Lista de (content, similarity_score)
        """
        if len(query_embedding) != self.embedding_dim:
            raise ValueError(
                f"Dimensión incorrecta en query: esperado {self.embedding_dim}, "
                f"recibido {len(query_embedding)}"
            )

        start_time = time.time()

        with self.pool.connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute("""
                    SELECT
                        d.content,
                        d.source_hash,
                        1 - (de.embedding <=> %s::vector) as similarity
                    FROM document_embeddings de
                    JOIN documents d ON de.document_id = d.id
                    ORDER BY de.embedding <=> %s::vector
                    LIMIT %s
                """, (query_embedding, query_embedding, top_k))

                results = cursor.fetchall()

        elapsed_time = time.time() - start_time
        print(f"🔍 Búsqueda completada en {elapsed_time:.3f}s: {len(results)} resultados")

        return [(content, float(similarity)) for content, _, similarity in results]

    def get_document_count(self) -> int:
        """Obtener número total de documentos"""
        with self.pool.connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute("SELECT COUNT(*) FROM documents")
                return cursor.fetchone()[0]

    def get_stats(self) -> Dict[str, Any]:
        """Obtener estadísticas de la base de datos"""
        with self.pool.connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute("SELECT COUNT(*) FROM documents")
                doc_count = cursor.fetchone()[0]

                cursor.execute("SELECT COUNT(*) FROM document_embeddings")
                emb_count = cursor.fetchone()[0]

                cursor.execute("""
                    SELECT COUNT(DISTINCT embedding_model)
                    FROM documents WHERE embedding_model IS NOT NULL
                """)
                model_count = cursor.fetchone()[0]

                return {
                    'documents': doc_count,
                    'embeddings': emb_count,
                    'models': model_count,
                    'current_model': self.embedding_model,
                    'dimension': self.embedding_dim
                }

    def close(self):
        """Cerrar pool de conexiones"""
        if self.pool:
            self.pool.close()
            print("🔌 Pool de conexiones cerrado")

if __name__ == "__main__":
    # Test básico de la clase
    db = PostgreSQLVectorDatabase()
    stats = db.get_stats()
    print(f"📊 Estadísticas: {stats}")
    db.close()
```

#### Comandos:
```bash
# 5.1 Crear archivo de clase
# (Crear el archivo postgresql_database_experimental.py con el contenido de arriba)

# 5.2 Test de conexión
python postgresql_database_experimental.py
# Esperado: 🔌 PostgreSQLVectorDatabase inicializado + 📊 Estadísticas
```

### ✅ Paso 6: Ingestión con Modelo Base (60 minutos)

#### Checklist:
- [ ] Cargar modelo all-MiniLM-L6-v2
- [ ] Cargar dataset experimental
- [ ] Generar embeddings para todos los chunks
- [ ] Insertar en PostgreSQL con metadata completa
- [ ] Validar que no hay duplicados
- [ ] Registrar tiempos de ingestión

#### Archivo: `ingest_fase1.py`
```python
#!/usr/bin/env python3
"""
Ingestión Fase 1: Modelo base all-MiniLM-L6-v2
"""
import os
import json
import time
import hashlib
from sentence_transformers import SentenceTransformer
import numpy as np

from postgresql_database_experimental import PostgreSQLVectorDatabase

def ingest_base_model():
    """Ingestar dataset con modelo base"""

    print("🚀 Iniciando ingestión Fase 1: Modelo Base")
    print(f"🤖 Modelo: {os.getenv('EMBEDDING_MODEL')}")
    print(f"📏 Dimensión: {os.getenv('EMBEDDING_DIM')}")

    # Cargar dataset experimental
    print("\n📂 Cargando dataset experimental...")
    with open("second_brain/plan/experimental_dataset.json", 'r', encoding='utf-8') as f:
        dataset = json.load(f)

    print(f"✅ Dataset cargado: {len(dataset)} chunks")

    # Inicializar modelo de embeddings
    print("\n🤖 Cargando modelo de embeddings...")
    model_start = time.time()
    model = SentenceTransformer(os.getenv('EMBEDDING_MODEL'))
    model_load_time = time.time() - model_start
    print(f"✅ Modelo cargado en {model_load_time:.2f}s")

    # Inicializar base de datos
    print("\n🔌 Inicializando PostgreSQL...")
    db = PostgreSQLVectorDatabase()

    # Preparar documentos para ingestión
    print("\n📋 Preparando documentos para ingestión...")
    documents_to_ingest = []

    for i, chunk_data in enumerate(dataset):
        content = chunk_data['content']

        # Metadata enriquecida
        metadata = {
            'source_document': chunk_data['source_document'],
            'source_hash': chunk_data['source_hash'],
            'chunking_strategy': 'experimental',
            'chunk_index': chunk_data['chunk_index'],
            'semantic_title': f"Experimental chunk {i+1}",
            'semantic_summary': content[:100] + "..." if len(content) > 100 else content,
            'additional_metadata': {
                'phase': 'fase_1',
                'model': os.getenv('EMBEDDING_MODEL'),
                'dataset_type': 'experimental'
            }
        }

        documents_to_ingest.append((content, None, metadata))  # Embedding se generará después

    print(f"✅ {len(documents_to_ingest)} documentos preparados")

    # Generar embeddings en batch
    print(f"\n🧮 Generando embeddings para {len(documents_to_ingest)} chunks...")
    embedding_start = time.time()

    texts = [doc[0] for doc in documents_to_ingest]
    embeddings = model.encode(texts, convert_to_numpy=True)

    embedding_time = time.time() - embedding_start
    embedding_per_chunk = embedding_time / len(embeddings)

    print(f"✅ Embeddings generados en {embedding_time:.2f}s")
    print(f"⏱️ Tiempo por chunk: {embedding_per_chunk:.3f}s")

    # Combinar con embeddings
    documents_with_embeddings = []
    for (content, _, metadata), embedding in zip(documents_to_ingest, embeddings):
        documents_with_embeddings.append((content, embedding.tolist(), metadata))

    # Ingestar en base de datos
    print(f"\n📥 Ingestando {len(documents_with_embeddings)} documentos en PostgreSQL...")
    ingestion_start = time.time()

    db.add_documents_with_metadata(documents_with_embeddings)

    ingestion_time = time.time() - ingestion_start
    print(f"✅ Ingestión completada en {ingestion_time:.2f}s")

    # Validar resultados
    print(f"\n📊 Validando resultados...")
    stats = db.get_stats()
    print(f"📈 Estadísticas finales:")
    for key, value in stats.items():
        print(f"   {key}: {value}")

    # Validar no duplicados
    print(f"\n🔍 Verificando duplicados...")
    with db.pool.connection() as conn:
        with conn.cursor() as cursor:
            cursor.execute("""
                SELECT source_hash, COUNT(*) as count
                FROM documents
                GROUP BY source_hash
                HAVING COUNT(*) > 1
            """)
            duplicates = cursor.fetchall()

            if duplicates:
                print(f"⚠️ Se encontraron {len(duplicates)} hashes duplicados:")
                for hash_val, count in duplicates:
                    print(f"   {hash_val[:16]}...: {count} veces")
            else:
                print("✅ No se encontraron duplicados")

    # Guardar métricas
    metrics = {
        'phase': 'fase_1',
        'model': os.getenv('EMBEDDING_MODEL'),
        'dimension': int(os.getenv('EMBEDDING_DIM')),
        'dataset_size': len(dataset),
        'model_load_time': model_load_time,
        'embedding_time': embedding_time,
        'embedding_per_chunk': embedding_per_chunk,
        'ingestion_time': ingestion_time,
        'ingestion_per_chunk': ingestion_time / len(documents_with_embeddings),
        'final_stats': stats,
        'duplicates_found': len(duplicates),
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    }

    metrics_file = "second_brain/plan/logs/fase_1_metrics.json"
    with open(metrics_file, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=2)

    print(f"\n📄 Métricas guardadas en: {metrics_file}")

    # Test de búsqueda básica
    print(f"\n🔍 Probando búsqueda básica...")
    test_query = "¿Qué es un sistema?"
    test_embedding = model.encode([test_query])[0].tolist()

    results = db.search_similar(test_embedding, top_k=3)
    print(f"✅ Búsqueda test: {len(results)} resultados")
    for i, (content, similarity) in enumerate(results):
        print(f"   Resultado {i+1}: similarity={similarity:.3f}, content='{content[:50]}...'")

    db.close()
    print(f"\n🎉 Fase 1 completada exitosamente!")

if __name__ == "__main__":
    ingest_base_model()
```

#### Comandos:
```bash
# 6.1 Crear script de ingestión
# (Crear el archivo ingest_fase1.py con el contenido de arriba)

# 6.2 Ejecutar ingestión Fase 1
python ingest_fase1.py
# Esperado: 🚀 Ingestión completada con métricas detalladas
```

### ✅ Paso 7: Validación y Documentación (15 minutos)

#### Checklist:
- [ ] Verificar que todos los chunks se insertaron
- [ ] Confirmar que no hay duplicados
- [ ] Validar búsquedas funcionan
- [ ] Documentar tiempos y métricas
- [ ] Preparar resumen para Fase 2

#### Comandos:
```bash
# 7.1 Validar estado final de la base de datos
psql -d rag_experiments -c "
SELECT
    COUNT(*) as total_documents,
    COUNT(DISTINCT source_hash) as unique_hashes,
    COUNT(DISTINCT embedding_model) as models,
    AVG(LENGTH(content)) as avg_content_length
FROM documents;
"

# 7.2 Validar embeddings
psql -d rag_experiments -c "
SELECT
    COUNT(*) as total_embeddings,
    embedding_model,
    COUNT(DISTINCT document_id) as unique_docs
FROM document_embeddings de
JOIN documents d ON de.document_id = d.id
GROUP BY embedding_model;
"

# 7.3 Revisar métricas guardadas
cat second_brain/plan/logs/fase_1_metrics.json | python -m json.tool
```

#### Validación Final:
```bash
# 7.4 Test final de búsqueda
python -c "
from postgresql_database_experimental import PostgreSQLVectorDatabase
from sentence_transformers import SentenceTransformer

# Inicializar
db = PostgreSQLVectorDatabase()
model = SentenceTransformer('all-MiniLM-L6-v2')

# Test query
query = '¿Qué es Docker?'
embedding = model.encode([query])[0].tolist()

results = db.search_similar(embedding, top_k=3)
print(f'Query: {query}')
print(f'Resultados: {len(results)}')
for i, (content, score) in enumerate(results):
    print(f'{i+1}. [{score:.3f}] {content[:80]}...')

db.close()
"
```

## Criterios de Éxito de Fase 1

### ✅ Éxito si:
- PostgreSQL conecta y pgvector funciona
- Schema paramétrico creado con EMBEDDING_DIM=384
- 10-20 chunks ingeridos sin errores
- 0 duplicados detectados
- Búsquedas devuelven resultados relevantes
- Tiempos de ingestión < 5 segundos por chunk
- Métricas guardadas correctamente

### ❌ Fracaso si:
- PostgreSQL no conecta o pgvector no funciona
- Error en creación de schema paramétrico
- Embeddings no se generan o insertan
- Duplicados detectados en base de datos
- Búsquedas no devuelven resultados
- Tiempos de ingestión > 10 segundos por chunk
- Errores críticos sin resolver

## Entregables de Fase 1

1. **Base de datos PostgreSQL** con schema paramétrico
2. **Dataset experimental** de 10-20 chunks
3. **PostgreSQLVectorDatabase** funcional
4. **15 chunks ingeridos** con modelo base
5. **Métricas de rendimiento** guardadas
6. **Validación de búsqueda** básica
7. **Logs y documentación** completa

## Preparación para Fase 2

Al completar Fase 1 exitosamente, tendrás:
- Base de datos PostgreSQL limpia funcionando
- Dataset controlado listo para experimentos
- Clase base para ingestión y búsqueda
- Métricas baseline para comparación
- Pipeline completo validado

**Siguiente paso:** Proceder a `Fase_2_Experimento_Embedders.md` para probar embedders adicionales.