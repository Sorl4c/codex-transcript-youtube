# Fase 2: Experimento con Embedders Candidatos

## Objetivo del DÃ­a

Comparar rendimiento y calidad entre diferentes embedders usando un corpus sintÃ©tico multitemÃ¡tico con 3 categorÃ­as.

**DuraciÃ³n estimada:** 4-5 horas
**Resultado esperado:** Matriz de ingestiÃ³n completada para 3 embedders con mÃ©tricas objetivas de rendimiento usando dataset con tres dominios (tecnologÃ­a, aprendizaje/productividad, deporte/bienestar) y chunks sintÃ©ticos.

## Checklist Completo

### âœ… Paso 1: PreparaciÃ³n y Cache Invalidation (30 minutos)

#### Checklist:
- [ ] Validar que Fase 1 completÃ³ exitosamente
- [ ] Limpiar cache de sentence-transformers
- [ ] Limpiar base de datos PostgreSQL
- [ ] Configurar variables de entorno para embedder #2
- [ ] Validar que todo estÃ¡ limpio

#### Comandos:
```bash
# 1.1 Validar que Fase 1 completÃ³
if [ ! -f "second_brain/plan/logs/fase_1_metrics.json" ]; then
    echo "âŒ Fase 1 no completada. Ejecutar Fase 1 primero."
    exit 1
fi
echo "âœ… Fase 1 completada detectada"

# 1.2 Limpiar cache de sentence-transformers (CRÃTICO)
echo "ğŸ§¹ Limpiando cache de sentence-transformers..."
rm -rf ~/.cache/torch/sentence_transformers/
rm -rf ~/.cache/huggingface/
echo "âœ… Cache limpiada"

# 1.3 Limpiar completamente base de datos PostgreSQL
echo "ğŸ—‘ï¸ Limpiando base de datos PostgreSQL..."
psql -d rag_experiments -c "
DROP TABLE IF EXISTS document_embeddings CASCADE;
DROP TABLE IF EXISTS documents CASCADE;
"
echo "âœ… Tablas eliminadas"

# 1.4 Validar que la BD estÃ¡ vacÃ­a
psql -d rag_experiments -c "
SELECT table_name FROM information_schema.tables
WHERE table_schema = 'public' AND table_name LIKE '%document%';
"
# Esperado: 0 filas

# 1.5 Configurar variables para embedder #2 (MPNet)
echo "âš™ï¸ Configurando para embedder #2: all-mpnet-base-v2"
cat > .env.fase2 << EOF
# PostgreSQL Configuration
DATABASE_BACKEND=postgresql
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=rag_experiments
POSTGRES_USER=postgres
POSTGRES_PASSWORD=tu_password

# Embedding Configuration - Fase 2 (MPNet)
EMBEDDING_MODEL=all-mpnet-base-v2
EMBEDDING_DIM=768

# Experiment Configuration
EXPERIMENT_MODE=true
SAVE_RAW_OUTPUTS=true
LOG_LEVEL=DEBUG
PHASE=fase_2
EOF

# Usar variables de Fase 2
export $(cat .env.fase2 | xargs)
echo "âœ… Variables configuradas:"
echo "   EMBEDDING_MODEL: $EMBEDDING_MODEL"
echo "   EMBEDDING_DIM: $EMBEDDING_DIM"
```

### âœ… Paso 2: Setup Schema para Nueva DimensiÃ³n (30 minutos)

#### Checklist:
- [ ] Modificar script de schema para nueva dimensiÃ³n
- [ ] Recrear tablas con EMBEDDING_DIM=768
- [ ] Validar que la nueva dimensiÃ³n funciona
- [ ] Verificar constraints de dimensiÃ³n
- [ ] Test bÃ¡sico de inserciÃ³n

#### Comandos:
```bash
# 2.1 Recrear schema con nueva dimensiÃ³n
echo "ğŸ”§ Creando schema para dimensiÃ³n 768..."
python setup_schema.py
# Esperado: ğŸ”§ Creando schema... âœ… Schema creado exitosamente

# 2.2 Validar nueva dimensiÃ³n en schema
psql -d rag_experiments -c "
SELECT column_name, data_type, character_maximum_length
FROM information_schema.columns
WHERE table_name = 'document_embeddings' AND column_name = 'embedding';
"
# Esperado: embedding | vector | 768

# 2.3 Validar constraint de dimensiÃ³n
psql -d rag_experiments -c "
SELECT conname, pg_get_constraintdef(oid)
FROM pg_constraint
WHERE conrelid = 'document_embeddings'::regclass;
"
# Esperado: Constraint CHECK array_length(embedding, 1) = 768
```

### âœ… Paso 3: IngestiÃ³n con Embedder #2 (MPNet - 768d) (90 minutos)

#### Checklist:
- [ ] Descargar modelo all-mpnet-base-v2
- [ ] Cargar dataset experimental de Fase 1
- [ ] Generar embeddings de 768 dimensiones
- [ ] Insertar en PostgreSQL validando nueva dimensiÃ³n
- [ ] Registrar mÃ©tricas detalladas
- [ ] Validar que no hay duplicados

#### Archivo: `ingest_fase2_mpnet.py`
```python
#!/usr/bin/env python3
"""
IngestiÃ³n Fase 2a: Modelo all-mpnet-base-v2 (768 dimensiones)
"""
import os
import json
import time
import psutil
import hashlib
from sentence_transformers import SentenceTransformer
import numpy as np

from postgresql_database_experimental import PostgreSQLVectorDatabase

def clear_model_cache():
    """Limpiar cache de modelos completamente"""
    import torch
    import gc

    # Limpiar cache de PyTorch
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    # Forzar garbage collection
    gc.collect()

    print("ğŸ§¹ Cache de modelos limpiada")

def ingest_mpnet_model():
    """Ingestar dataset con modelo MPNet (768 dimensiones)"""

    print("ğŸš€ Iniciando ingestiÃ³n Fase 2a: Modelo MPNet (768d)")
    print(f"ğŸ¤– Modelo: {os.getenv('EMBEDDING_MODEL')}")
    print(f"ğŸ“ DimensiÃ³n: {os.getenv('EMBEDDING_DIM')}")

    # Limpiar cache completamente
    clear_model_cache()

    # Cargar corpus sintÃ©tico multitemÃ¡tico
    print("\nğŸ“‚ Cargando corpus sintÃ©tico multitemÃ¡tico...")
    with open("second_brain/plan/data/corpus_multitematico.json", 'r', encoding='utf-8') as f:
        dataset = json.load(f)

    print(f"âœ… Corpus cargado: {len(dataset)} chunks (3 categorÃ­as: tecnologÃ­a, aprendizaje/productividad, deporte/bienestar)")
    print(f"ğŸ¯ Objetivo: Medir relevancia cruzada entre dominios")

    # Registrar uso de recursos antes de cargar modelo
    initial_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
    print(f"ğŸ’¾ Memoria inicial: {initial_memory:.1f} MB")

    # Inicializar modelo MPNet
    print(f"\nğŸ¤– Cargando modelo {os.getenv('EMBEDDING_MODEL')}...")
    model_start = time.time()

    try:
        model = SentenceTransformer(os.getenv('EMBEDDING_MODEL'))
    except Exception as e:
        print(f"âŒ Error cargando modelo: {e}")
        print("ğŸ’¡ Intentando descarga manual...")
        # Forzar descarga
        model = SentenceTransformer(os.getenv('EMBEDDING_MODEL'), cache_folder='./cache')

    model_load_time = time.time() - model_start
    print(f"âœ… Modelo cargado en {model_load_time:.2f}s")

    # Registrar memoria despuÃ©s de cargar modelo
    after_model_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
    model_memory = after_model_memory - initial_memory
    print(f"ğŸ’¾ Memoria usada por modelo: {model_memory:.1f} MB")

    # Validar dimensiÃ³n del modelo
    test_embedding = model.encode(["test"])
    actual_dim = len(test_embedding[0])
    expected_dim = int(os.getenv('EMBEDDING_DIM'))

    if actual_dim != expected_dim:
        raise ValueError(
            f"âŒ DimensiÃ³n incorrecta: modelo genera {actual_dim}, "
            f"configurado para {expected_dim}"
        )
    print(f"âœ… DimensiÃ³n validada: {actual_dim}")

    # Inicializar base de datos
    print("\nğŸ”Œ Inicializando PostgreSQL...")
    db = PostgreSQLVectorDatabase()

    # Preparar documentos (mismo formato que Fase 1)
    print("\nğŸ“‹ Preparando documentos para ingestiÃ³n...")
    documents_to_ingest = []

    for i, chunk_data in enumerate(dataset):
        content = chunk_data['content']

        metadata = {
            'source_document': chunk_data['source_document'],
            'source_hash': chunk_data['source_hash'],
            'chunking_strategy': 'synthetic',
            'chunk_index': chunk_data['chunk_index'],
            'semantic_title': chunk_data.get('titulo', f"MPNet chunk {i+1}"),
            'semantic_summary': chunk_data.get('resumen', content[:100] + "..."),
            'additional_metadata': {
                'phase': 'fase_2a',
                'model': os.getenv('EMBEDDING_MODEL'),
                'dimension': actual_dim,
                'dataset_type': 'synthetic_multitematic',
                'categoria': chunk_data.get('categoria', 'tecnologia'),
                'fuente': chunk_data.get('fuente', 'sintetico'),
                'cross_domain_relevance': True
            }
        }

        documents_to_ingest.append((content, None, metadata))

    print(f"âœ… {len(documents_to_ingest)} documentos preparados")

    # Generar embeddings en batch (monitorear recursos)
    print(f"\nğŸ§® Generando embeddings {actual_dim}D para {len(documents_to_ingest)} chunks...")
    embedding_start = time.time()

    # Medir recursos durante generaciÃ³n
    peak_memory = initial_memory

    texts = [doc[0] for doc in documents_to_ingest]

    # Generar en batches mÃ¡s pequeÃ±os para controlar memoria
    batch_size = 5
    all_embeddings = []

    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        print(f"   Procesando batch {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1} ({len(batch_texts)} chunks)")

        batch_embeddings = model.encode(batch_texts, convert_to_numpy=True)
        all_embeddings.extend(batch_embeddings)

        # Monitorear uso de memoria
        current_memory = psutil.Process().memory_info().rss / 1024 / 1024
        peak_memory = max(peak_memory, current_memory)
        print(f"   Memoria actual: {current_memory:.1f} MB (pico: {peak_memory:.1f} MB)")

    embeddings = np.array(all_embeddings)
    embedding_time = time.time() - embedding_start
    embedding_per_chunk = embedding_time / len(embeddings)

    print(f"âœ… Embeddings generados en {embedding_time:.2f}s")
    print(f"â±ï¸ Tiempo por chunk: {embedding_per_chunk:.3f}s")
    print(f"ğŸ’¾ Pico de memoria: {peak_memory:.1f} MB")

    # Preparar para ingestiÃ³n
    documents_with_embeddings = []
    for (content, _, metadata), embedding in zip(documents_to_ingest, embeddings):
        documents_with_embeddings.append((content, embedding.tolist(), metadata))

    # Ingestar en base de datos
    print(f"\nğŸ“¥ Ingestando {len(documents_with_embeddings)} documentos en PostgreSQL...")
    ingestion_start = time.time()

    try:
        db.add_documents_with_metadata(documents_with_embeddings)
        ingestion_time = time.time() - ingestion_start
        print(f"âœ… IngestiÃ³n completada en {ingestion_time:.2f}s")
    except Exception as e:
        print(f"âŒ Error en ingestiÃ³n: {e}")
        raise

    # Validar resultados
    print(f"\nğŸ“Š Validando resultados...")
    stats = db.get_stats()
    print(f"ğŸ“ˆ EstadÃ­sticas finales:")
    for key, value in stats.items():
        print(f"   {key}: {value}")

    # Validar no duplicados
    print(f"\nğŸ” Verificando duplicados...")
    with db.pool.connection() as conn:
        with conn.cursor() as cursor:
            cursor.execute("""
                SELECT source_hash, COUNT(*) as count
                FROM documents
                GROUP BY source_hash
                HAVING COUNT(*) > 1
            """)
            duplicates = cursor.fetchall()

            if duplicates:
                print(f"âš ï¸ Se encontraron {len(duplicates)} hashes duplicados:")
                for hash_val, count in duplicates:
                    print(f"   {hash_val[:16]}...: {count} veces")
            else:
                print("âœ… No se encontraron duplicados")

    # Validar dimensiÃ³n guardada
    print(f"\nğŸ” Validando dimensiÃ³n guardada...")
    with db.pool.connection() as conn:
        with conn.cursor() as cursor:
            cursor.execute("""
                SELECT array_length(embedding, 1) as dim, COUNT(*) as count
                FROM document_embeddings
                GROUP BY array_length(embedding, 1)
            """)
            dimensions = cursor.fetchall()

            for dim, count in dimensions:
                print(f"   {dim} dimensiones: {count} embeddings")

    # Guardar mÃ©tricas detalladas
    metrics = {
        'phase': 'fase_2a',
        'model': os.getenv('EMBEDDING_MODEL'),
        'dimension': actual_dim,
        'dataset_size': len(dataset),
        'model_load_time': model_load_time,
        'model_memory_mb': model_memory,
        'peak_memory_mb': peak_memory,
        'embedding_time': embedding_time,
        'embedding_per_chunk': embedding_per_chunk,
        'ingestion_time': ingestion_time,
        'ingestion_per_chunk': ingestion_time / len(documents_with_embeddings),
        'final_stats': stats,
        'duplicates_found': len(duplicates),
        'dimensions_found': dimensions,
        'batch_size': batch_size,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    }

    metrics_file = "second_brain/plan/logs/fase_2a_mpnet_metrics.json"
    with open(metrics_file, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=2)

    print(f"\nğŸ“„ MÃ©tricas guardadas en: {metrics_file}")

    # Test de bÃºsqueda bÃ¡sica
    print(f"\nğŸ” Probando bÃºsqueda bÃ¡sica...")
    test_query = "Â¿QuÃ© es un sistema?"
    test_embedding = model.encode([test_query])[0].tolist()

    results = db.search_similar(test_embedding, top_k=3)
    print(f"âœ… BÃºsqueda test: {len(results)} resultados")
    for i, (content, similarity) in enumerate(results):
        print(f"   Resultado {i+1}: similarity={similarity:.3f}, content='{content[:50]}...'")

    db.close()
    clear_model_cache()
    print(f"\nğŸ‰ Fase 2a (MPNet) completada exitosamente!")

if __name__ == "__main__":
    ingest_mpnet_model()
```

#### Comandos:
```bash
# 3.1 Crear script de ingestiÃ³n MPNet
# (Crear el archivo ingest_fase2_mpnet.py con el contenido de arriba)

# 3.2 Ejecutar ingestiÃ³n MPNet
python ingest_fase2_mpnet.py
# Esperado: ğŸš€ IngestiÃ³n completada con mÃ©tricas de 768 dimensiones
```

### âœ… Paso 4: Cache Invalidation y PreparaciÃ³n Embedder #3 (30 minutos)

#### Checklist:
- [ ] Validar que MPNet completÃ³ exitosamente
- [ ] Limpiar cache de sentence-transformers
- [ ] Limpiar base de datos PostgreSQL
- [ ] Configurar variables para embedder #3 (MultilingÃ¼e)
- [ ] Validar configuraciÃ³n para 384 dimensiones

#### Comandos:
```bash
# 4.1 Validar MPNet completado
if [ ! -f "second_brain/plan/logs/fase_2a_mpnet_metrics.json" ]; then
    echo "âŒ Fase 2a (MPNet) no completÃ³ exitosamente"
    exit 1
fi
echo "âœ… Fase 2a (MPNet) completada detectada"

# 4.2 Limpiar cache (CRÃTICO para cambiar de modelo)
echo "ğŸ§¹ Limpiando cache para embedder #3..."
rm -rf ~/.cache/torch/sentence_transformers/
rm -rf ~/.cache/huggingface/
rm -rf ./cache/
echo "âœ… Cache limpiada"

# 4.3 Limpiar base de datos
echo "ğŸ—‘ï¸ Limpiando base de datos para embedder #3..."
psql -d rag_experiments -c "
DROP TABLE IF EXISTS document_embeddings CASCADE;
DROP TABLE IF EXISTS documents CASCADE;
"

# 4.4 Configurar para embedder #3 (MultilingÃ¼e)
echo "âš™ï¸ Configurando para embedder #3: multilingual-MiniLM"
cat > .env.fase2b << EOF
# PostgreSQL Configuration
DATABASE_BACKEND=postgresql
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=rag_experiments
POSTGRES_USER=postgres
POSTGRES_PASSWORD=tu_password

# Embedding Configuration - Fase 2b (Multilingual)
EMBEDDING_MODEL=paraphrase-multilingual-MiniLM-L12-v2
EMBEDDING_DIM=384

# Experiment Configuration
EXPERIMENT_MODE=true
SAVE_RAW_OUTPUTS=true
LOG_LEVEL=DEBUG
PHASE=fase_2b
EOF

# Usar variables de Fase 2b
export $(cat .env.fase2b | xargs)
echo "âœ… Variables configuradas:"
echo "   EMBEDDING_MODEL: $EMBEDDING_MODEL"
echo "   EMBEDDING_DIM: $EMBEDDING_DIM"
```

### âœ… Paso 5: IngestiÃ³n con Embedder #3 (MultilingÃ¼e - 384d) (60 minutos)

#### Checklist:
- [ ] Descargar modelo multilingÃ¼e
- [ ] Cargar mismo dataset experimental
- [ ] Generar embeddings de 384 dimensiones
- [ ] Insertar en PostgreSQL
- [ ] Registrar mÃ©tricas comparativas
- [ ] Validar mejora en espaÃ±ol

#### Archivo: `ingest_fase2_multilingual.py`
```python
#!/usr/bin/env python3
"""
IngestiÃ³n Fase 2b: Modelo MultilingÃ¼e (384 dimensiones)
"""
import os
import json
import time
import psutil
from sentence_transformers import SentenceTransformer
import numpy as np

from postgresql_database_experimental import PostgreSQLVectorDatabase

def clear_model_cache():
    """Limpiar cache de modelos completamente"""
    import torch
    import gc

    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    gc.collect()
    print("ğŸ§¹ Cache de modelos limpiada")

def ingest_multilingual_model():
    """Ingestar dataset con modelo multilingÃ¼e (384 dimensiones)"""

    print("ğŸš€ Iniciando ingestiÃ³n Fase 2b: Modelo MultilingÃ¼e (384d)")
    print(f"ğŸ¤– Modelo: {os.getenv('EMBEDDING_MODEL')}")
    print(f"ğŸ“ DimensiÃ³n: {os.getenv('EMBEDDING_DIM')}")

    clear_model_cache()

    # Cargar corpus sintÃ©tico multitemÃ¡tico
    print("\nğŸ“‚ Cargando corpus sintÃ©tico multitemÃ¡tico...")
    with open("second_brain/plan/data/corpus_multitematico.json", 'r', encoding='utf-8') as f:
        dataset = json.load(f)

    print(f"âœ… Corpus cargado: {len(dataset)} chunks (3 categorÃ­as: tecnologÃ­a, aprendizaje/productividad, deporte/bienestar)")
    print(f"ğŸ¯ Objetivo: Medir relevancia cruzada entre dominios")

    # Recursos iniciales
    initial_memory = psutil.Process().memory_info().rss / 1024 / 1024
    print(f"ğŸ’¾ Memoria inicial: {initial_memory:.1f} MB")

    # Cargar modelo multilingÃ¼e
    print(f"\nğŸ¤– Cargando modelo {os.getenv('EMBEDDING_MODEL')}...")
    model_start = time.time()

    try:
        model = SentenceTransformer(os.getenv('EMBEDDING_MODEL'))
    except Exception as e:
        print(f"âŒ Error cargando modelo: {e}")
        print("ğŸ’¡ Intentando con descarga explÃ­cita...")
        model = SentenceTransformer(os.getenv('EMBEDDING_MODEL'), cache_folder='./cache_multilingual')

    model_load_time = time.time() - model_start
    print(f"âœ… Modelo cargado en {model_load_time:.2f}s")

    after_model_memory = psutil.Process().memory_info().rss / 1024 / 1024
    model_memory = after_model_memory - initial_memory
    print(f"ğŸ’¾ Memoria usada por modelo: {model_memory:.1f} MB")

    # Validar dimensiÃ³n
    test_embedding = model.encode(["test"])
    actual_dim = len(test_embedding[0])
    expected_dim = int(os.getenv('EMBEDDING_DIM'))

    if actual_dim != expected_dim:
        raise ValueError(
            f"âŒ DimensiÃ³n incorrecta: modelo genera {actual_dim}, "
            f"configurado para {expected_dim}"
        )
    print(f"âœ… DimensiÃ³n validada: {actual_dim}")

    # Test de calidad en espaÃ±ol (importante para modelo multilingÃ¼e)
    print(f"\nğŸŒ Probando calidad en espaÃ±ol...")
    spanish_queries = [
        "Â¿QuÃ© es la inteligencia artificial?",
        "CÃ³mo funciona el aprendizaje automÃ¡tico",
        "Sistemas de bases de datos relacionales"
    ]

    spanish_embeddings = model.encode(spanish_queries)
    print(f"âœ… {len(spanish_queries)} queries espaÃ±oles procesadas")

    # Inicializar base de datos
    print("\nğŸ”Œ Inicializando PostgreSQL...")
    db = PostgreSQLVectorDatabase()

    # Preparar documentos
    print("\nğŸ“‹ Preparando documentos para ingestiÃ³n...")
    documents_to_ingest = []

    for i, chunk_data in enumerate(dataset):
        content = chunk_data['content']

        metadata = {
            'source_document': chunk_data['source_document'],
            'source_hash': chunk_data['source_hash'],
            'chunking_strategy': 'synthetic',
            'chunk_index': chunk_data['chunk_index'],
            'semantic_title': chunk_data.get('titulo', f"Multilingual chunk {i+1}"),
            'semantic_summary': chunk_data.get('resumen', content[:100] + "..."),
            'additional_metadata': {
                'phase': 'fase_2b',
                'model': os.getenv('EMBEDDING_MODEL'),
                'dimension': actual_dim,
                'language_support': 'multilingual',
                'dataset_type': 'synthetic_multitematic',
                'categoria': chunk_data.get('categoria', 'tecnologia'),
                'fuente': chunk_data.get('fuente', 'sintetico'),
                'cross_domain_relevance': True
            }
        }

        documents_to_ingest.append((content, None, metadata))

    print(f"âœ… {len(documents_to_ingest)} documentos preparados")

    # Generar embeddings
    print(f"\nğŸ§® Generando embeddings {actual_dim}D para {len(documents_to_ingest)} chunks...")
    embedding_start = time.time()

    texts = [doc[0] for doc in documents_to_ingest]
    embeddings = model.encode(texts, convert_to_numpy=True)

    embedding_time = time.time() - embedding_start
    embedding_per_chunk = embedding_time / len(embeddings)

    print(f"âœ… Embeddings generados en {embedding_time:.2f}s")
    print(f"â±ï¸ Tiempo por chunk: {embedding_per_chunk:.3f}s")

    # Preparar para ingestiÃ³n
    documents_with_embeddings = []
    for (content, _, metadata), embedding in zip(documents_to_ingest, embeddings):
        documents_with_embeddings.append((content, embedding.tolist(), metadata))

    # Ingestar
    print(f"\nğŸ“¥ Ingestando {len(documents_with_embeddings)} documentos en PostgreSQL...")
    ingestion_start = time.time()

    db.add_documents_with_metadata(documents_with_embeddings)
    ingestion_time = time.time() - ingestion_start
    print(f"âœ… IngestiÃ³n completada en {ingestion_time:.2f}s")

    # Validar resultados
    print(f"\nğŸ“Š Validando resultados...")
    stats = db.get_stats()
    print(f"ğŸ“ˆ EstadÃ­sticas finales:")
    for key, value in stats.items():
        print(f"   {key}: {value}")

    # Validar no duplicados
    print(f"\nğŸ” Verificando duplicados...")
    with db.pool.connection() as conn:
        with conn.cursor() as cursor:
            cursor.execute("""
                SELECT source_hash, COUNT(*) as count
                FROM documents
                GROUP BY source_hash
                HAVING COUNT(*) > 1
            """)
            duplicates = cursor.fetchall()
            print(f"âœ… Duplicados encontrados: {len(duplicates)}")

    # Guardar mÃ©tricas
    metrics = {
        'phase': 'fase_2b',
        'model': os.getenv('EMBEDDING_MODEL'),
        'dimension': actual_dim,
        'dataset_size': len(dataset),
        'model_load_time': model_load_time,
        'model_memory_mb': model_memory,
        'embedding_time': embedding_time,
        'embedding_per_chunk': embedding_per_chunk,
        'ingestion_time': ingestion_time,
        'ingestion_per_chunk': ingestion_time / len(documents_with_embeddings),
        'final_stats': stats,
        'duplicates_found': len(duplicates),
        'spanish_queries_tested': len(spanish_queries),
        'language_support': 'multilingual',
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    }

    metrics_file = "second_brain/plan/logs/fase_2b_multilingual_metrics.json"
    with open(metrics_file, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=2)

    print(f"\nğŸ“„ MÃ©tricas guardadas en: {metrics_file}")

    # Test de bÃºsqueda en espaÃ±ol
    print(f"\nğŸ” Probando bÃºsqueda en espaÃ±ol...")
    test_query = "Â¿QuÃ© es un sistema de bases de datos?"
    test_embedding = model.encode([test_query])[0].tolist()

    results = db.search_similar(test_embedding, top_k=3)
    print(f"âœ… BÃºsqueda test espaÃ±ol: {len(results)} resultados")
    for i, (content, similarity) in enumerate(results):
        print(f"   Resultado {i+1}: similarity={similarity:.3f}, content='{content[:50]}...'")

    db.close()
    clear_model_cache()
    print(f"\nğŸ‰ Fase 2b (MultilingÃ¼e) completada exitosamente!")

if __name__ == "__main__":
    ingest_multilingual_model()
```

#### Comandos:
```bash
# 5.1 Crear script de ingestiÃ³n multilingÃ¼e
# (Crear el archivo ingest_fase2_multilingual.py con el contenido de arriba)

# 5.2 Ejecutar ingestiÃ³n multilingÃ¼e
python ingest_fase2_multilingual.py
# Esperado: ğŸš€ IngestiÃ³n completada con soporte multilingÃ¼e
```

### âœ… Paso 6: Crear Matriz de IngestiÃ³n (45 minutos)

#### Checklist:
- [ ] Recopilar mÃ©tricas de los 3 embedders
- [ ] Crear matriz comparativa de rendimiento
- [ ] Analizar diferencias significativas
- [ ] Identificar patrones de rendimiento
- [ ] Preparar recomendaciones iniciales

#### Archivo: `create_ingestion_matrix.py`
```python
#!/usr/bin/env python3
"""
Crear matriz comparativa de ingestiÃ³n para los 3 embedders
"""
import json
import os
from datetime import datetime

def create_ingestion_matrix():
    """Crear matriz comparativa de rendimiento de ingestiÃ³n"""

    print("ğŸ“Š Creando matriz comparativa de ingestiÃ³n...")

    # Cargar mÃ©tricas de las 3 fases
    metrics_files = {
        'MiniLM (Control)': 'second_brain/plan/logs/fase_1_metrics.json',
        'MPNet (Alta Calidad)': 'second_brain/plan/logs/fase_2a_mpnet_metrics.json',
        'Multilingual (EspaÃ±ol)': 'second_brain/plan/logs/fase_2b_multilingual_metrics.json'
    }

    all_metrics = {}
    missing_files = []

    for name, file_path in metrics_files.items():
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                all_metrics[name] = json.load(f)
            print(f"âœ… Cargado: {name}")
        else:
            missing_files.append(file_path)
            print(f"âŒ No encontrado: {file_path}")

    if missing_files:
        print(f"\nâš ï¸ Archivos faltantes: {missing_files}")
        print("AsegÃºrate de haber completado todas las fases de ingestiÃ³n.")
        return

    # Crear matriz de ingestiÃ³n
    print(f"\nğŸ“‹ Creando matriz de ingestiÃ³n...")

    matrix_data = []
    for name, metrics in all_metrics.items():
        # Extraer mÃ©tricas clave
        ingestion_performance = {
            'Embedder': name,
            'Model': metrics.get('model', 'N/A'),
            'Dimension': metrics.get('dimension', 'N/A'),
            'Dataset_Size': metrics.get('dataset_size', 'N/A'),
            'Model_Load_Time_s': round(metrics.get('model_load_time', 0), 2),
            'Model_Memory_MB': round(metrics.get('model_memory_mb', 0), 1),
            'Embedding_Time_s': round(metrics.get('embedding_time', 0), 2),
            'Embedding_Per_Chunk_s': round(metrics.get('embedding_per_chunk', 0), 3),
            'Ingestion_Time_s': round(metrics.get('ingestion_time', 0), 2),
            'Ingestion_Per_Chunk_s': round(metrics.get('ingestion_per_chunk', 0), 3),
            'Peak_Memory_MB': round(metrics.get('peak_memory_mb', 0), 1),
            'Duplicates_Found': metrics.get('duplicates_found', 'N/A'),
            'Success_Rate_%': 100 if metrics.get('duplicates_found', 0) == 0 else 0,
            'Timestamp': metrics.get('timestamp', 'N/A')
        }

        matrix_data.append(ingestion_performance)

    # Mostrar matriz en consola
    print(f"\nğŸ“ˆ MATRIZ DE INGESTIÃ“N COMPARATIVA")
    print("="*120)

    # Header
    header = [
        "Embedder",
        "Dim",
        "Model_Load",
        "Embed_Time",
        "Ingest_Time",
        "Mem_MB",
        "Duplicates",
        "Success_%"
    ]
    print(f"{header[0]:<20} {header[1]:<6} {header[2]:<12} {header[3]:<12} {header[4]:<12} {header[5]:<8} {header[6]:<10} {header[7]:<9}")
    print("-"*120)

    # Data rows
    for row in matrix_data:
        print(f"{row['Embedder']:<20} {row['Dimension']:<6} {row['Model_Load_Time_s']:<12} "
              f"{row['Embedding_Time_s']:<12} {row['Ingestion_Time_s']:<12} "
              f"{row['Model_Memory_MB']:<8} {row['Duplicates_Found']:<10} {row['Success_Rate_%']:<9}")

    print("="*120)

    # AnÃ¡lisis comparativo
    print(f"\nğŸ” ANÃLISIS COMPARATIVO")

    # Tiempos
    fastest_model = min(matrix_data, key=lambda x: x['Embedding_Per_Chunk_s'])
    slowest_model = max(matrix_data, key=lambda x: x['Embedding_Per_Chunk_s'])

    print(f"âš¡ Embedding mÃ¡s rÃ¡pido: {fastest_model['Embedder']} "
          f"({fastest_model['Embedding_Per_Chunk_s']}s por chunk)")
    print(f"ğŸŒ Embedding mÃ¡s lento: {slowest_model['Embedder']} "
          f"({slowest_model['Embedding_Per_Chunk_s']}s por chunk)")

    # Memoria
    lightest_model = min(matrix_data, key=lambda x: x['Model_Memory_MB'])
    heaviest_model = max(matrix_data, key=lambda x: x['Model_Memory_MB'])

    print(f"ğŸ’¾ Modelo mÃ¡s ligero: {lightest_model['Embedder']} "
          f"({lightest_model['Model_Memory_MB']} MB)")
    print(f"ğŸ‹ï¸ Modelo mÃ¡s pesado: {heaviest_model['Embedder']} "
          f"({heaviest_model['Model_Memory_MB']} MB)")

    # Duplicados (problema que queremos resolver)
    successful_models = [m for m in matrix_data if m['Success_Rate_%'] == 100]
    print(f"âœ… Modelos sin duplicados: {len(successful_models)}/{len(matrix_data)}")
    for model in successful_models:
        print(f"   - {model['Embedder']}")

    # Calcular scores de rendimiento (ponderado)
    print(f"\nğŸ“Š PUNTUACIONES DE RENDIMIENTO")

    def calculate_performance_score(row):
        """Calcular score de rendimiento 0-100"""
        # Factores (menos es mejor para tiempos y memoria)
        time_score = max(0, 100 - (row['Embedding_Per_Chunk_s'] * 100))  # 0.01s = 99, 0.1s = 90
        memory_score = max(0, 100 - (row['Model_Memory_MB'] / 10))  # 100MB = 90, 500MB = 50
        success_score = row['Success_Rate_%']

        # PonderaciÃ³n: 40% tiempo, 30% memoria, 30% Ã©xito
        total_score = (time_score * 0.4) + (memory_score * 0.3) + (success_score * 0.3)
        return round(total_score, 1)

    for row in matrix_data:
        row['Performance_Score'] = calculate_performance_score(row)
        print(f"   {row['Embedder']}: {row['Performance_Score']}/100")

    # RecomendaciÃ³n basada en rendimiento puro
    best_performance = max(matrix_data, key=lambda x: x['Performance_Score'])
    print(f"\nğŸ† Mejor rendimiento (tiempo + memoria + Ã©xito): {best_performance['Embedder']}")

    # Guardar matriz completa
    matrix_complete = {
        'generated_at': datetime.now().isoformat(),
        'analysis': {
            'fastest_embedding': fastest_model['Embedder'],
            'slowest_embedding': slowest_model['Embedder'],
            'lightest_model': lightest_model['Embedder'],
            'heaviest_model': heaviest_model['Embedder'],
            'successful_models_count': len(successful_models),
            'best_performance': best_performance['Embedder']
        },
        'matrix_data': matrix_data,
        'raw_metrics': all_metrics
    }

    matrix_file = "second_brain/plan/results/ingestion_matrix.json"
    with open(matrix_file, 'w', encoding='utf-8') as f:
        json.dump(matrix_complete, f, indent=2)

    print(f"\nğŸ“„ Matriz completa guardada en: {matrix_file}")

    # Crear resumen para Fase 3
    summary = {
        'experiment_completed': True,
        'models_tested': len(matrix_data),
        'all_successful': len(successful_models) == len(matrix_data),
        'recommendations': {
            'proceed_to_phase_3': len(successful_models) >= 2,  # Al menos 2 modelos funcionan
            'best_performance': best_performance['Embedder'],
            'consider_language': any('multilingual' in m['Embedder'].lower() for m in matrix_data),
            'memory_constraints': heaviest_model['Model_Memory_MB'] > 500
        }
    }

    summary_file = "second_brain/plan/results/fase_2_summary.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2)

    print(f"ğŸ“„ Resumen para Fase 3 guardado en: {summary_file}")

    return matrix_complete

if __name__ == "__main__":
    create_ingestion_matrix()
```

#### Comandos:
```bash
# 6.1 Crear script de matriz
# (Crear el archivo create_ingestion_matrix.py con el contenido de arriba)

# 6.2 Ejecutar anÃ¡lisis de matriz
python create_ingestion_matrix.py
# Esperado: ğŸ“Š Matriz comparativa con anÃ¡lisis completo
```

### âœ… Paso 7: ValidaciÃ³n Final y PreparaciÃ³n Fase 3 (15 minutos)

#### Checklist:
- [ ] Verificar que los 3 embedders completaron exitosamente
- [ ] Confirmar que la matriz de ingestiÃ³n estÃ¡ creada
- [ ] Validar que no hay duplicados en ningÃºn caso
- [ ] Preparar resumen para Fase 3
- [ ] Documentar aprendizajes

#### Comandos:
```bash
# 7.1 Validar archivos de mÃ©tricas
echo "ğŸ” Validando archivos de mÃ©tricas..."
ls -la second_brain/plan/logs/fase_*_metrics.json
# Esperado: 3 archivos (fase_1, fase_2a_mpnet, fase_2b_multilingual)

# 7.2 Validar matriz de ingestiÃ³n
echo "ğŸ“Š Validando matriz de ingestiÃ³n..."
if [ -f "second_brain/plan/results/ingestion_matrix.json" ]; then
    echo "âœ… Matriz creada exitosamente"
    python -c "
    import json
    with open('second_brain/plan/results/ingestion_matrix.json', 'r') as f:
        matrix = json.load(f)
    print(f'ğŸ“ˆ Modelos analizados: {len(matrix[\"matrix_data\"])}')
    best = matrix['analysis']['best_performance']
    print(f'ğŸ† Mejor rendimiento: {best}')
    "
else
    echo "âŒ Matriz no encontrada"
fi

# 7.3 Validar estado final de PostgreSQL
echo "ğŸ” Validando estado final de PostgreSQL..."
psql -d rag_experiments -c "
SELECT
    COUNT(*) as total_docs,
    COUNT(DISTINCT embedding_model) as models,
    COUNT(DISTINCT source_hash) as unique_hashes
FROM documents;
"

# 7.4 Resumen preparaciÃ³n Fase 3
echo "ğŸ“‹ PreparaciÃ³n para Fase 3..."
python -c "
import json
with open('second_brain/plan/results/fase_2_summary.json', 'r') as f:
    summary = json.load(f)

print('ğŸ¯ Estado Fase 2:')
print(f'   âœ… Experimento completado: {summary[\"experiment_completed\"]}')
print(f'   âœ… Modelos probados: {summary[\"models_tested\"]}')
print(f'   âœ… Todos exitosos: {summary[\"all_successful\"]}')
print(f'   âœ… Recomendar Fase 3: {summary[\"recommendations\"][\"proceed_to_phase_3\"]}')
print(f'   ğŸ† Mejor rendimiento: {summary[\"recommendations\"][\"best_performance\"]}')
"
```

## Criterios de Ã‰xito de Fase 2

### âœ… Ã‰xito si:
- Los 3 embedders se probaron exitosamente
- Matriz de ingestiÃ³n creada con mÃ©tricas comparativas
- 0 duplicados en al menos 2 de los 3 embedders
- Tiempos de ingestiÃ³n medidos y comparados
- Uso de memoria medido y documentado
- Recomendaciones claras para Fase 3

### âŒ Fracaso si:
- Menos de 2 embedders completan exitosamente
- Todos los embedders generan duplicados
- Tiempos de ingestiÃ³n no se pueden medir
- Matriz comparativa no se puede crear
- Errores crÃ­ticos sin resoluciÃ³n

## Entregables de Fase 2

1. **MÃ©tricas de MPNet** (768 dimensiones) - `fase_2a_mpnet_metrics.json`
2. **MÃ©tricas de MultilingÃ¼e** (384 dimensiones) - `fase_2b_multilingual_metrics.json`
3. **Matriz de ingestiÃ³n comparativa** - `ingestion_matrix.json`
4. **Resumen y recomendaciones** - `fase_2_summary.json`
5. **AnÃ¡lisis de rendimiento** con scores ponderados
6. **ValidaciÃ³n de cache invalidation** exitosa

## Aprendizajes Clave Esperados

1. **Impacto de dimensiÃ³n**: 384 vs 768 en rendimiento y memoria
2. **Cache invalidation**: Importancia crÃ­tica al cambiar modelos
3. **Rendimiento relativo**: CuÃ¡l embedder es mÃ¡s eficiente
4. **Soporte multilingÃ¼e**: Beneficios para contenido en espaÃ±ol
5. **Patrones de duplicaciÃ³n**: Si el problema es del modelo o del sistema

## PreparaciÃ³n para Fase 3

Al completar Fase 2 exitosamente, tendrÃ¡s:
- **3 embedders probados** con el mismo dataset
- **MÃ©tricas objetivas** de rendimiento y recursos
- **Matriz comparativa** para toma de decisiones
- **Base de datos PostgreSQL** funcionando con el mejor modelo
- **Pipeline validado** para experimentos controlados

**Siguiente paso:** Proceder a `Fase_3_Validacion_Decision.md` para comparar calidad de bÃºsqueda y tomar decisiÃ³n final.