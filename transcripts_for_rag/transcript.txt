Título: This AI Web Scraper Reads Websites Like a Human (n8n Tutorial)
URL: https://www.youtube.com/watch?v=1gFh3Zl05eo

Hola, chicos. Entonces, en este video les mostraré firecrawl, que es un tipo diferente de raspador web al que le dan una URL y pueden decirle en un lenguaje sencillo qué información quieren extraer de la página. Esto es importante porque los raspadores web comunes te devuelven todo el contenido de la página y luego tienes que buscar en todo ese lío HTML para encontrar la información específica que realmente estás buscando. Digamos que estás espiando el anuncio de Facebook de tu competidor y quieres analizar sus páginas de destino y páginas de productos a gran escala. Con Firecrawl, puedes decir simplemente: "Envíame el título, los precios y las garantías mencionadas en esta página, y el programa escaneará todo el contenido y te devolverá exactamente esa información en un formato limpio y organizado". Ahora, esto tendrá más sentido a medida que avancemos en el vídeo. Y este es el ejemplo que vamos a utilizar. Y hablaré más sobre esto , por supuesto, pero haremos todo esto dentro de N8N. Esta es una automatización muy simple, pero realmente puede extraer mucha información del sitio web de su competidor. Por cierto, si quieres acceder a esta plantilla y a todas mis otras plantillas de forma gratuita, asegúrate de visitar la comunidad escolar gratuita . El enlace está debajo de este vídeo. Contamos con más de 1400 personas en esta comunidad que están aprendiendo a utilizar automatizaciones y agentes de IA. Mucha gente está aquí por el marketing, por los anuncios, por el contenido generado por el usuario (CGU), gente del tipo creativo. Entonces, nuevamente, tenemos 1400 personas aquí y puedes ingresar al aula, buscar el video y lo que encontrarás son todas las plantillas debajo del video. Entonces, nuevamente, si quieres esta plantilla gratis, simplemente haz clic en el enlace debajo del video. Entonces déjame ejecutar esto una vez y te mostraré dónde irá la salida. Será esta hoja de Google, y tenemos una URL aquí, y esta es la página de inicio de Ridge para su equipo militar. Entonces, voy a hacer clic en la automatización y Firecrawl escaneará esta URL y con el mensaje que estoy pasando a través de la automatización. Será capaz de extraer automáticamente estos fragmentos de información muy específicos por sí solo. Así que déjame entrar en N8N. Voy a ejecutar el flujo de trabajo. Esto podría tomar no sé, unos 30 segundos más o menos. Entonces Firecrawl va a raspar la página. Vamos a esperar 10 segundos para asegurarnos de que esté terminado. Luego vamos a ir a buscar el resultado final. y vamos a escribir los resultados en esa hoja de Google. Entonces, cuando regreso a Hojas de cálculo de Google, puedes ver que Firecrawl, sin ningún nodo adicional ni ningún paso de IA, pudo obtener exactamente lo que solicité. Entonces, tiene el título de la página, el subtítulo, el CTA, compra la colección, incorporó alguna prueba social, la copia de descuento y la copia garantizada. y extrae todo eso desde la URL de esa página de destino. Entonces, en este video, quiero hablar un poco sobre Firecrawl y por qué creo que este raspador es diferente a los raspadores web tradicionales. Y luego te mostraré cómo configurar una automatización simple como esta y un caso de uso específico para especialistas en marketing y personas que ejecutan anuncios de Facebook y cómo creo que puedes integrar esto en tu flujo cuando estás espiando los anuncios de Facebook de tu competencia. Muy bien, hablemos de un caso de uso específico de esto. Entonces, digamos que estás espiando los anuncios de Facebook de tus competidores. Quizás estés raspando sus anuncios. Hace un tiempo publiqué un tweet sobre esto que se volvió viral en X con más de 3 millones de visitas. Y estaba mostrando un flujo de trabajo que te permite rastrear los anuncios de Facebook de tus competidores. Y lo que creo que podemos hacer en este vídeo es construir sobre eso. Porque en mi video anterior lo que estábamos sacando era la transcripción del anuncio de video. Estábamos atrayendo los ganchos y los ángulos y el público objetivo y todas esas cosas buenas. Pero una cosa que no estábamos mirando eran las páginas de destino de esos anuncios. Entonces, capturamos todo lo del anuncio en sí, pero no miramos las páginas de destino reales, ¿verdad? ¿A dónde enviaban realmente nuestros competidores a la gente desde sus anuncios? Y eso es lo que vamos a hacer en este vídeo. Y aquí voy a echar un vistazo a Ridge Wallets. Así que aquí está uno de sus anuncios de Facebook que acaban de empezar a publicar. Y entonces el caso de uso aquí es que ahora también podremos analizar las páginas de destino. ¿ Bueno? Entonces, vamos a utilizar esta página de destino nuevamente en su billetera. Entonces, si eres un comercializador o trabajas en comercio electrónico o publicas anuncios para clientes, creo que esto es algo que puedes agregar a tu conjunto de herramientas y que realmente puede ayudar a analizar y espiar a tus competidores. Está bien. Primero, quiero realmente apreciar por qué pienso que Firecrawl es tan genial. Y para que quede constancia, este no es un video patrocinado, quiero mostrarles cómo se ve un raspador de solicitudes HTTP normal. Y lo que es más importante, los datos que realmente genera. Entonces, dentro de N8N, simplemente vamos a tomar un nodo de solicitud HTTP aquí. Y voy a poner la URL de esa cresta aquí. Ahora, cuando hago clic en ejecutar paso, esto raspará la página, ¿ verdad? Entonces, está haciendo el mismo trabajo, extrayendo toda la información. Pero cuando veamos el resultado aquí, veremos que en realidad no es tan útil. Está bien. Entonces, si miramos la vista JSON, podemos ver lo absolutamente horrible que es esta salida . Pienso que la mesa debería ser un poco mejor. Mi computadora literalmente se está congelando ahora mismo. Así que estos son los datos extraídos de esta página. Así que es básicamente todo el HTML, todo el código que ocurre detrás de escena para renderizar esta página. Entonces, nuevamente, es posible en NN con solo un nodo de solicitud HTTP básico extraer esta información. Y déjenme mostrarles un ejemplo. Entonces, si simplemente busco este término aquí, servicio activo, si mi computadora no se congela, déjame intentar buscar aquí. Está bien. Entonces, servicio activo, ¿verdad? Entonces, sólo para mostrarte que en realidad está extrayendo el texto y el contenido de la página, pero también está extrayendo todo el CSS, todo el HTML. Entonces, realmente no hay mucho que podamos hacer sólo con esto, ¿verdad? Si quisiéramos extraer la información que obtuvimos, como el titular y el CTA, en la prueba social, necesitaríamos varios nodos adicionales. Tenemos que limpiar los datos. Probablemente tendríamos que enviarlo a un paso de IA y luego generarlo allí. E incluso entonces, puede ser que no pueda conseguirlo. Entonces, solo quiero mostrarles cómo se ve un raspador HTTP normal dentro de Naden. Está bien. Entonces, con Firecrawl, en realidad tienen un par de herramientas diferentes de las que no voy a hablar, pero estoy en la página de inicio aquí y lo que voy a hacer es ir al panel de control. Y con lo que quiero jugar es este extracto. ¿ Bueno? Y es una herramienta cada vez más nueva. Entonces vas a hacer clic en extraer. Y lo primero que puedo hacer es mostrar esto en el patio de recreo. Entonces déjame hacer clic en intentar extraer el patio de juegos. Lo que puedes hacer aquí es proporcionar la URL de un sitio web con un mensaje. Y sabes que es más fácil simplemente mostrártelo. Y nuevamente puedes hablar con este raspador en inglés sencillo, directamente en lenguaje natural. Puedes ver el ejemplo aquí. Se dice en todas las páginas de firecrol dev.dev dev. Quiero extraer el nombre de la empresa, la misión y si es de código abierto. Entonces, puedes hablar con tu raspador web. Entonces, en nuestro ejemplo, tengo esto en mi portapapeles. Lo que estoy diciendo es que desde esta URL (que proporciono), ¿puedes extraer el título, el subtítulo, la CTA, la prueba social, la copia de descuento y la copia de garantía? Y la única razón por la que lo tengo formateado o escrito de esa manera es porque literalmente lo acabo de pegar desde mi hoja de Google. No es necesario formatearlo de esa manera. Así que aquí está nuestro mensaje. Y lo siguiente que queremos hacer es hacer clic en generar parámetros. Y te mostraré cómo configurar esto dentro de N8N en un segundo. Sólo quiero mostrarles cómo funciona en el patio de juegos para mostrarles lo que realmente está sucediendo aquí. Y ahora puedes ver los resultados. Entonces, nos dará el esquema aquí. Por cierto, puedes editar todo esto. Entonces, tiene el título principal, el subtítulo, el CTA, la prueba social y el texto de descuento. El mensaje también aparece aquí. Entonces, es como obtener una vista previa de la búsqueda antes de ejecutarla. Pero de nuevo, puedes cambiar esto si lo deseas. Puedes cambiar el mensaje. Y puedes ver que mi URL está guardada aquí. Ahora, si desea obtener una vista previa de los resultados y cómo se verá, puede simplemente hacer clic en ejecutar. Y esto tomará un segundo. Y simplemente mostrará los resultados aquí abajo. Y puedes ver que tardó unos 5 segundos. Y nos da toda nuestra información bien formateada aquí. Entonces, tenemos el texto, la prueba social, el subtítulo, el texto con descuento , el título principal y el texto garantizado. Entonces, si simplemente estás tratando de jugar con él en el patio de juegos, puedes hacerlo aquí. Puede ingresar varias URL, puede modificar su solicitud y, nuevamente, puede modificar el esquema aquí. Ahora, volveremos a esta página un poco más adelante, pero ahora quiero mostrarles los nodos dentro de N8N. De esta manera, puedes determinar el disparador si quieres hacer esto según un cronograma o si quieres hacerlo manualmente. Pero lo que estoy haciendo en este ejemplo aquí, y nuevamente la idea aquí es que si estás extrayendo anuncios de Facebook de tus competidores, en esta parte del proceso queremos obtener las URL de las páginas de destino para extraerlas. Entonces, tengo una hoja de Google aquí y la única entrada que proporciono es la URL de la página de destino y mis otras columnas aquí coinciden con todos los campos que estoy tratando de extraer. Entonces, dentro de N8N tengo un nodo de hoja de Google, una hoja de lectura y simplemente lo conecté a mi cuenta de Google. Seleccioné la hoja de cálculo y cuando ejecuto el paso puedes ver todos los datos que ingresan aquí. Ahora, una cosa que olvidé mencionar es que tengo una columna de identificación. Es la primera columna. Necesitaremos eso más tarde. Entonces, agregaría una columna de identificación y la numeraría hasta el final. Y puedes ver que la identificación se muestra aquí. Pero mi hoja de Google muestra toda mi columna. Está bien. Lo siguiente que tendremos que hacer es usar una solicitud HTTP para acceder a la API de Fire Crawl. Y para este, lo haré desde cero sólo para mostrártelo. Entonces, déjame agregar una solicitud HTTP de nodo . De hecho, déjame conectarlo aquí para no obtener ningún error. Y te mostraré cómo configurar esto con firecrawl. Entonces, antes de siquiera abrir esto, lo que quieres hacer (y es bueno adquirir el hábito de mirar la documentación de la API) es lo siguiente. Como puedes ver, estoy en los documentos de la API de firecrawl y, en las características de agentic, estoy en extraer. Así que aquí es donde debes estar para ver los documentos. Ahora lo que queremos hacer es llegar a este ejemplo de uso y tiene este bonito ejemplo de curl que podemos importar directamente a N8N. Entonces lo que quieres hacer es simplemente copiar este comando curl aquí. Regresa a N8N. Está bien. Ahora déjame abrir la nueva solicitud HTTP. Y lo que puedes hacer es simplemente hacer clic en importar curl. Pégalo. Pulsa Importar. Y ahora mucho de esto ya estará completo para ti. Está bien. Así que tenemos nuestra URL. Ahora, para la autenticación, realmente tienes dos opciones aquí. Puedes ver que está completando el encabezado para nosotros ahora mismo. Entonces tenemos los encabezados de envío activados. El nombre ya está rellenado con autorización. El valor se completa previamente con el portador. Y todo lo que necesitas hacer después de esto es ingresar tu clave API. Y puedes obtener tu clave API en tu panel de control. No voy a revelar el mío en el video, pero puedes tomarlo dentro de tu cuenta, regresar aquí, eliminarlo y luego pegarlo. Entonces, esa es una forma en que puedes autenticarte. Ahora, la forma más fácil de hacerlo, y especialmente si va a utilizar firecrawl en el futuro, es desactivar estos encabezados de envío y autenticarse aquí. Así que puedes verlo en el menú desplegable de autenticación. Lo que desea hacer clic aquí es el tipo de credencial genérica y luego, para el tipo O, haga clic en el encabezado. Ahora ya tengo el mío guardado y esto es lo que voy a mostrarles cómo hacer. De esta manera podrás guardar esto cada vez para firecraw. Así que no tienes que pasar por este proceso cada vez. Puedes ver que el mío está ahí. Pero si aún no lo tienes, simplemente debes hacer clic en crear nueva credencial. Y de nuevo ¿dónde conseguir estos campos? Si regresas a la documentación de firecrawl, puedes ver este encabezado desactivado, ¿verdad? Entonces, tenemos el encabezado allí. Y luego la autorización es lo que vas a copiar en el campo de nombre. Y luego, para el valor, vas a copiar. Copiaré todo aquí. Y para el campo de valor, puedes hacer clic y ejecutar la expresión para asegurarte de que realmente puedas verlo. Entonces, puedes ver que tenemos el portador y nuevamente tu clave API. Entonces, lo que vas a querer hacer es eliminar esta clave API y, cualquiera que sea tu clave API, nuevamente estará en tu cuenta de firecrawl. Simplemente pégalo allí. Deberías cambiarle el nombre a firecrawl y luego hacer clic en guardar. Ahora bien, cuando hagas eso, ¿qué va a pasar? Voy a cerrar aquí. Pero lo que va a pasar es que debajo de este encabezado del menú desplegable, ahora podrás seleccionar el rastreo de fuego. Por lo tanto, no necesitas estos encabezados de envío. Pero de cualquier manera funciona, ya sea que desees guardar tus credenciales aquí o que desees enviar los encabezados cada vez. Está bien. Sin embargo, lo que necesitamos cambiar es este cuerpo y este JSON aquí abajo. Entonces déjame hacer clic aquí para abrir la expresión y expandir el campo aquí. Así que estos son los datos ficticios que Firecrawl va a importar para nosotros. Pero tenemos que cambiar básicamente casi todo aquí. Entonces, lo primero que vamos a querer cambiar son las URL que estamos extrayendo. Así que ahora puedes ver que hay tres URL aquí. En nuestro ejemplo, vamos a introducir una variable. Entonces lo que quiero hacer es eliminar estos tres y solo dejar la primera cita aquí y la última cita aquí. Y ahora puedes ver que tengo mis dos citas. Y lo que yo arrastro es mi variable. Entonces la mía se llama URL de página de destino. Y deberías verlo rellenarse en el lado derecho. Está bien. Así que ese es el paso número uno. El paso número dos es que queremos cambiar el mensaje. ¿Bien? Entonces, si volvemos a nuestro patio de juegos, puedes ver que guardó nuestro mensaje o realmente generó nuestro mensaje aquí. Entonces voy a copiar esto, regresar a NN y luego eliminar nuevamente el mensaje, todo lo que está dentro de estas comillas y pegar el mío. Muy bien. Así que ahora también he cambiado el mensaje. Y ahora lo que necesitamos cambiar es este esquema de aquí. Así que básicamente todo lo que está bajo esta parte del esquema. Entonces regresa nuevamente al patio de juegos y podrás ver que comienza aquí con este tipo de objeto. Entonces, si simplemente copiamos todo esto aquí y regresamos a nuestro nodo de solicitud HTTP, nuevamente lo que quiero capturar es ese primer corchete hasta el final porque si miras en el campo de juegos puedes ver que tenemos el primer corchete aquí y este formato podría no funcionar. Déjame pegar esto. A veces, el JSON puede fallar si el formato está un poco desfasado. Entonces, intentemos probar esto. Está bien. Entonces, se queja de un JSON no válido. Creo que es por la forma en que pegué desde el firewall, a N8N no le gusta el formato. Entonces, lo que puedes hacer es literalmente copiar esto, ir al chat GPT y decir: "Por favor, arregle este JSON para N8N". Entonces, el chat GPT nos dará una versión corregida aquí. Entonces, regresemos y peguemos eso e intentemos ejecutarlo nuevamente. Muy bien, eso funcionó. Así que era solo el formato. Entonces deberías recibir un mensaje que diga verdadero y un número de identificación aquí. Muy bien, el siguiente paso. Lo que está sucediendo en segundo plano es que Firecrawl saldrá y rastreará esa página para nosotros. Ahora, esto tomará entre cinco y veinticinco segundos, dependiendo de cuántas páginas estés raspando. Entonces, quieres agregar un nodo de peso para tu próximo nodo . Ahora, el mío aquí solo se configuró por 10 segundos porque literalmente estamos raspando solo una página, pero tenga en cuenta, ya sabe, cuántas páginas está raspando. Entonces, desea agregar un nodo de peso a continuación. Y luego lo que quieres hacer después de eso es agregar otro nodo de solicitud HTTP . Bueno, déjame abrir esto. De hecho, déjame ejecutar estos pasos anteriores para que podamos ver cómo llegan los datos . Muy bien. Entonces ejecuté el nodo anterior para que pudiéramos ver algunos datos que llegan a este segundo nodo de solicitud HTTP. Entonces, nuevamente agregué otro nodo de solicitud HTTP y lo que esto hará es verificar el estado de este anterior aquí. ¿Bien? Así que tenemos una identificación aquí. Entonces lo que tenemos que hacer es dejar esto fijo. Lo único que tenemos que hacer es comprobar esa ID para asegurarnos de que Fire Crawler haya terminado de raspar la página que está raspando. Entonces, este es un nuevo nodo de solicitud HTTP y para el método que desea obtener, luego debe autenticarse aquí y es por eso que guardar su fire crawl en el paso anterior es una buena idea porque puede simplemente seleccionar el tipo de credencial genérica para el encabezado de selección de tipo O y puede ver en mi menú desplegable que tengo mi firecrol guardado aquí y no necesitamos enviar ningún otro parámetro ahora, sin embargo, lo que debemos hacer es enviar una URL. ¿Y esto es un poco complicado por qué? Por el paso de espera de antes. Y esto me hizo tropezar. Tuve que hacer que Chat GPT solucionara esto por mí. Entonces, hablemos de la URL, solo de la URL en blanco. Entonces, déjame borrar lo que hay aquí ahora mismo. De hecho, volvamos a la documentación porque aquí se muestra cómo generar esa URL. Entonces, nuevamente, estoy en la documentación de la API. Estoy en extracto. Y si bajas un poquito aquí donde dice comprobación de estado. Ahora también puedes importar este comando curl y te pedirá que completes tu O manualmente. Pero lo que realmente quiero mostrarles es sólo esta URL. Entonces esta es la URL que estoy usando, ¿verdad? Es este extracto de API.firecrol.dev v1 con esta barra invertida. Y luego de eso, puedes ver que está pidiendo el ID de extracto y lo vamos a capturar , ¿verdad? Así que este es el ID de la solicitud de rastreo de incendios. Ahora es posible que tengas la tentación de extraer la identificación del nodo de espera y se volverá verde. El problema con el que te vas a encontrar es si cuando verificamos esto y no está listo, y hablaremos de esto en el siguiente paso, vamos a enviar esto de vuelta al nodo de peso, pero al nodo HTTP para verificar el estado de extracción. Lo que quiere es el ID que viene del nodo anterior, no el nodo de peso. Entonces, nuevamente le pregunté al chat GPT cómo hacer esto. Y lo que quieres hacer es poder volver a llamar a este nodo. Entonces, déjame pegar eso nuevamente allí para ti. Y entonces, puedes ver que tengo llaves dobles. Tengo PNS de signo de dólar. Y luego estoy llamando a esa solicitud de extracción de firecrawl cerrar cita cerrar parenthem.json. IDENTIFICACIÓN. Así que asegúrate de hacerlo. Nuevamente, su nodo puede tener un nombre diferente, pero si está siguiendo esto, le cambiaría el nombre porque esto será importante para poder verificar el estado. Entonces déjame ejecutar esto. Está bien. Así que ahora podéis ver que esto ha terminado, ¿verdad? Por tanto, el éxito es verdadero y el estado se completa. Y puedes ver que realmente está extrayendo los datos que solicitamos . Ahora este estado es importante para el siguiente paso porque nuevamente vamos a querer verificar para asegurarnos de que el estado esté realmente completo antes de seguir adelante. Así que ese es el siguiente nodo. Así que esto es sólo un nodo if. Bueno. Entonces, después de verificar el estado de extracción del nodo HTTP, agregue un nuevo nodo. Este es un nodo if. Y lo que vas a querer hacer aquí es que, para la condición, ahora déjame eliminar lo que está aquí. Entonces, para este campo, simplemente arrastré el campo de estado del paso anterior. Y luego, para la cadena, la configuro como igual a. Y luego lo que queremos hacer es tirar en completado. Entonces puedes arrastrarlo o escribirlo. Pero lo que esto básicamente hace es verificar que el rastreo de fuego haya terminado, ¿verdad? Para que el estado quede completado. Entonces ejecuto este paso. Ahora lo que vamos a hacer es que con ese nodo tendrás dos caminos, el camino verdadero o el camino falso. Entonces, si es falso, bien, lo que queremos hacer es arrastrar esta salida verdadera nuevamente al nodo de espera. Y lo que eso hará es esperar 10 segundos más o los segundos que usted configure. Va a esperar y luego intentará comprobarlo nuevamente. Y si todavía no está listo, va a esperar y luego va a intentar verificarlo nuevamente hasta que finalmente esté completo. Ahora, cuando esté completo, querremos registrar esto en Hojas de cálculo de Google. ¿Bueno? Entonces, eso es a lo que se conecta este verdadero camino. Entonces, para obtener tu ruta verdadera, debes conectarla a un nodo de Google Sheet. Así que esta es una hoja de actualización. Lo sentimos, esta es una fila de actualización en el nodo de la hoja. Está bien. Y lo que vamos a hacer es enviarlo a nuestra hoja de cálculo aquí. Entonces déjame simplemente borrar la información que tengo aquí. Nuevamente, esta es una hoja de cálculo de Google simple con solo unas pocas columnas que puedes ver. Y ahora, en el nodo de su hoja de Google, nuevamente, la operación será actualizar fila. Solo querrás tomar el título del documento desde aquí. Entonces, el mío es Firecrawl, anuncios de Facebook y es la hoja uno. Y ahora, para el modo de mapeo de columnas, vamos a mapear manualmente. La columna que debe coincidir es ID. Y es por eso que tenemos esta columna de identificación aquí. Y luego, para actualizar los valores, lo primero que debes hacer es seleccionar el ID con el que quieres que coincida. Y ahora puedes regresar al primer nodo de la hoja de Google y arrastrar el ID aquí. Y ahora todos estos otros campos que queremos completar, el título, el subtítulo, el texto de CTA y la prueba social, y todos estos otros campos en nuestra hoja de Google, podemos simplemente tomarlos de la verificación de estado o del nodo anterior. Bien, ahora puedes ver que todos los campos están aquí. Así que puedes arrastrarlos aquí uno por uno. Ahora una vez que hagamos eso, podemos hacer clic en ejecutar paso. Y ahí vamos. Regresaremos a Hojas de cálculo de Google y podrás ver que nuestra hoja se ha completado. Entonces, tenemos nuestro título principal, nuestro subtítulo, CTA, prueba social, descuento y nuestro texto de garantía. Bueno, de todos modos, espero que hayas disfrutado el vídeo. Creo que aquí hay algunos buenos casos de uso para las personas que se dedican al ámbito competitivo de los anuncios en Facebook, algo que mucha gente parece estar haciendo. Hazme un favor. Si te gustó este vídeo suscríbete al canal, dale me gusta al vídeo y deja un comentario. Realmente me ayuda. Me lleva horas hacer estos videos, así que realmente lo aprecio. Gracias chicos.


Título: ¡No construyas Agentes IA hasta que veas esto! [Secretos de Anthropic]
URL: https://www.youtube.com/watch?v=3kzen1qCMBY&t=2248s

El 90% de las empresas está implementando mal la inteligencia artificial y están perdiendo dinero ahora mismo sin saberlo. Soy Rodrigo Mora, fundador de Mind Workers, empresa de agentes de inteligencia artificial. Durante los últimos meses he visto como las empresas que implementan correctamente infraestructura de inteligencia artificial obtienen una ventaja competitiva exponencial. Después de analizar cientos de automatizaciones y estudiar a fondo las investigaciones y mejores prácticas para construir agentes de IA, descubrí los secretos que utiliza Antropic, que hace que los agentes de inteligencia artificial sean realmente efectivos y funcionen en entornos empresariales de alta demanda. Son seis patrones que marcan la diferencia entre gastar en IA y ganar con IA. Esto no es solo teoría, son arquitecturas probadas que están generando millones a quienes las implementan. Los ejemplos que les voy a mostrar son extractos de implementaciones que yo mismo utilizo para mi empresa y las de mis clientes y les ha ayudado a multiplicar sus ventas y ahorrar cientos de horas de trabajo. En este video te revelaré paso a paso cómo implementar estas seis arquitecturas sin código y de forma sencilla. Quédate porque el patrón número cuatro puede cambiar por completo cómo ves las automatizaciones. Empezamos. ¿Te imaginas reducir costos, acelerar procesos y lograr que tu equipo se enfoque en lo estratégico? Todo esto es posible con un sistema multiagente. La idea es simple. Creemos diferentes agentes especializados que se comunican y trabajan en conjunto desde el análisis de datos hasta la generación de contenido en redes sociales. Esto lo vamos a hacer sin código y en N8N. Te lo explico paso a paso. Vamos a revisar seis casos de uso aplicados a entornos empresariales para explicar cada uno de estos patrones. cadena de pensamiento. Vamos a examinar la arquitectura de un agente de IA de ventas que cuenta con una base de conocimientos donde extrae la información para después transformar su respuesta de un mensaje técnico a uno humano y amigable que puede enviar por WhatsApp y después lo va a categorizar según la intención y sentimiento del usuario. Ruteo. Vamos a tomar las transcripciones de reuniones y mediante un análisis de contenido vamos a determinar qué tipo de reunión fue y categorizarla en propuesta comercial, seguimiento de proyecto, consultoría uno a uno. Esto para generar minutas específicas y especializadas de cada reunión. Paralelización. Utilizaremos tres LLMs para crear en paralelo el copy de un vídeo de YouTube, generando título, texto de la miniatura y la descripción. Orquestador con trabajadores funciona como un director de marketing que va a coordinar la creación de anuncios de meta para publicidad al integrar los resultados de sus dos agentes subordinados, un director creativo y un copyrighter. Evaluador optimizador, analista de datos con inteligencia artificial que se conecta a la base de datos financiera, al que le podemos preguntar información y ejecutar consultas SQL para responder. Este va a tener un supervisor que va a verificar que la información obtenida sea la requerida y no contenga alucinaciones. Sistema de recuperación de errores o fall. Este no viene la documentación de Antropic, pero es bastante útil para crear infraestructuras de inteligencia artificial para entornos empresariales reales y de alta demanda. El primer patrón es el prompting o cadena de pensamiento. Un error común es pedirle demasiado a la IA. Imagina una línea de ensamblaje inteligente. Cada estación, en este caso cada LLM, se especializa en una tarea pequeña usando el resultado de la estación anterior. Esto aumenta la precisión y el control. ¿Cómo aplicamos esto? Imagínate un chatbot de ventas avanzado conectado a un CRM. El usuario pregunta algo. Primero, un agente de IA busca en su base de conocimientos, que en este caso es una base de datos vectorial, la información técnica relevante para responder a la pregunta. Esta información, a veces cruda, pasa a un segundo LLM. Su única tarea es darle personalidad, un tono amigable, hacerlo sonar humano para poder enviar el mensaje en partes por WhatsApp, como lo haría una persona. Finalmente, un tercer ll analiza la intención del usuario, quiere comprar, necesita soportes, se está quejando y el sentimiento, está feliz, frustrado o neutro. El resultado es una respuesta precisa con el tono correcto y que entiende la emoción del cliente. Ahora vamos a ver cómo funciona. Vamos a escribirle. Buenas tardes. ¿Me podrías dar información sobre los servicios? Vemos que procesa, lo pasa al primer agente que es el agente vendedor. Este revisa su base de conocimientos para poder obtener la información. Haces la primer consulta a la base de conocimientos y ahora lo pasa al LLM encargado de personalizar el mensaje y dividirlo en partes. Una vez que lo divide, pasa al siguiente LM, que es el que va a clasificar. Entonces, vamos a revisar qué es lo que pasó en cada uno. Nos vamos primeramente con el agente vendedor. Nos vamos a la parte de los logs y vamos a analizar. Aquí tenemos ya su prompt, ¿no? Eh, eres Delta, una gerente de perfilamiento band con más de 10 años de experiencia en ventas B2B. Le damos todo el todo el rol, todo el contexto, le damos un objetivo principal, realizar un descubrimiento inicial eh estructurado eh mediante la metodología Band, que es budget, authority, need and timeline. Y bueno, le vamos dando instrucciones y restricciones y consideraciones. Entonces, ya en su output nos dice, "Buenas tardes, soy Delta, especialista en soluciones en automatización con IA. Con gusto puedo brindarle más información sobre nuestros servicios. Permíteme consultar la base de conocimientos para darte detalles precisos. Este fue un pensamiento interno. Este no fue un usuario, un mensaje para usuario final. Entonces, a partir de ahí, revisa su base de conocimientos, pregunta, ¿qué servicios ofrece la empresa? Entonces, ya la base de conocimientos o la base vectorial le responde, "Basándome en el contexto proporcionado, puedo compartir lo siguiente sobre los servicios de la empresa. Mind Workers ofrece soluciones de automatización con inteligencia artificial, específicamente desarrollo de agentes empresariales de IA, personalizados, entre otras cosas. Entonces, esto regresa nuevamente a la gente y ya puede elaborar su respuesta final. Lo que hacemos es y bueno, da da lo que hacemos, los beneficios para nuestros clientes y me gustaría conocer un poco más sobre tus necesidades, que aquí es donde entra la parte de band. vemos que es un mensaje largo. Entonces este al momento de pasar a la siguiente cadena, que lo que hace justamente es acortar y dividir, nos vamos a los logs y vemos, ah, mira, aquí no es necesario, aquí en el en el mismo output podemos ver el mensaje original y luego el mensaje completo ya acortado. Esto lo divide en tres partes, como lo haría normalmente una persona que mande un WhatsApp, ¿no? manda de repente un WhatsApp con todo el mensaje completo como si fuera un pergamino, lo va dividiendo por mensajes. Entonces, este ya lo divide en tres partes. Gracias por tu interés. En Mind Workers desarrollamos agentes de ya personalizados para empresas con un enfoque diferente al software estándar. Mapeamos tus procesos de ventas, creamos soluciones a medida y nos enfocamos en lograr un retorno rápido de inversión. Eso es la parte uno. Parte dos, nuestros clientes obtenen más reservaciones, reducen gastos y automatizan completamente sus ventas con respuestas de alta calidad. Esta es la parte dos. Parte tres, para entender mejor tus necesidades, que ya es donde entra el band, ¿qué canales te gustaría automatizar? ¿Usas algún sistema actualmente? ¿Qué actividades específicas buscas automatizar? Entonces, esta ya es una respuesta que suena mucho más humana y más fácil también de procesar. Entonces, esto ya pasa a otro flujo. Aquí ya lo lo ponemos pues ya como como mandar en WhatsApp. Y ya por último consultamos o revisamos cuál fue la información o cuál fue el sentimiento y la intención del usuario con su consulta. Entonces este LLM lo que hace es categorizar cuál fue su intención y cuál es el sentimiento. ¿Cuál es la intención? Pues rápidamente consultar consultas sobre servicios. sentimiento neutro, pues no es positivo, no es negativo, pues es un es un sentimiento neutro. Si llegara a haber un sentimiento negativo, pues podemos poner una bandera para que se active, para que un humano pueda justamente revisar esa conversación. Pero bueno, así es como funciona una cadena de pensamientos o prompt chaining. No más respuestas robóticas. No satures de actividades a una sola. Construye una cadena de especialistas. Ahora que sabes encadenar, ¿qué pasa si la IA necesita elegir qué cadena escoger? Es aquí donde entra el segundo patrón, routing o enrutamiento. Un clasificador inteligente analiza la entrada y la dirige a la gente o LLM especializado. Correcto. Piensa en las reuniones, ¿cuántas horas invertidas y al final no se le da un seguimiento puntual? Porque pocos toman notas de lo hablado, a menos que tengas una secretaria que te haga la minuta. Esto ya no es necesario con esta arquitectura. Utilizamos un sistema que transcribe automáticamente el audio de la reunión, como puede ser read, AI, Tactic, entre otras herramientas que existen. Entonces, este audio o este texto ya transcrito pasa a un LLM clasificador. Su misión va a ser determinar qué tipo de reunión fue, una propuesta comercial, un seguimiento, un proyecto, una alianza estratégica, consultoría uno a uno. Según la clasificación, el texto se va a enviar a un LLM diferente entrenado para este tipo de minuta en particular. Por ejemplo, en el caso de una propuesta comercial, va a generar una minuta enfocada en próximos pasos, objeciones del prospecto, compromisos, calificar al prospecto según sus comentarios, hacer recomendaciones para concretar la venta. Para un seguimiento de proyecto necesitamos una minuta con las tareas asignadas, deadlines, bloqueos, hitos, entre otras cosas. En el caso de una alianza estratégica, vamos a requerir un resumen de los acuerdos, puntos claves, responsables, beneficios mutuos, entre otros. El resultado, minutas perfectas y relevantes en segundos y adaptadas al contexto. Okay, vamos a ver cómo funciona. Vamos, en este caso a abrir nuevamente el chat y le voy a pegar una transcripción que en algún momento se realizó. Entonces, el chat se lo va a pasar al LLM clasificador. Este va a pensar y va a evaluar para qué tipo de reunión y en base a eso lo va a mandar al LLM adecuado. En este caso le pasé una transcripción de una propuesta comercial, ¿no? Entonces aquí podemos ver ya la transcripción. Entonces esta transcripción justamente analizamos los caminos. Si es una clase, le explicamos en la descripción qué tendría que tomar en consideración para tomar esta esta ruta, ¿no? Si la reunión es una clase o explicación de un sistema, plataforma o metodología para propuesta comercial. Si la reunión está orientada, levantar requerimientos para una implementación, vender o presentar una propuesta de implementación o automatización. Y digo aquí continúa alianza o sociedad. Si el resultado de la reunión es hacer una sociedad o alianza para distribuir o vender servicios de agentes de IA o automatizaciones. Levantamiento de requerimientos, si es un levantamiento de requerimientos y o mapeo de procesos para iniciar un proyecto. En caso de que no, pues lo manda a otra a otra rama. Entonces ahora sí este roteador elige en base al modelo seleccionado, que en este caso es Antropic, cuál es la rama más adecuada. En este caso, como era una minuta comercial, escogió la de minuta comercial o generar minuta comercial. Entonces, lo manda a este. Este LLM tiene un prompt muy especializado en el cual extrae información específica para poder generar una propuesta, ¿no? Aquí le pasamos la transcripción como parte del contexto y ahora sí, ¿cuál es su output? Pues una síntesis detallada, necesidades del cliente, ¿no? Para una tienda de ropa, para vento de uniformes escolares, cuáles son sus problemas actuales, sus objetivos de negocio, cuáles son las plataformas y sistemas actuales, canales de comunicación que busca automatizar, sus requerimientos técnicos, limitaciones o restricciones, plazos, decisiones tomadas, próximos pasos acordados y un análisis de oportunidades. Básicamente esto es lo que tiene este prompt especificado que obtenga en base a la transcripción. Ahora le voy a pegar otra minuta de otra reunión que tuve, que en este caso fue una clase, por ejemplo. Le doy la información de la transcripción, la procesa y va a determinar qué tipo de contenido es. Identifica que es una clase. Entonces lo pasa al LLM. justamente que hace la documentación de la clase. Vamos a ver ahorita cómo la respuesta es distinta. Aquí ya termino de procesar. Entonces abrimos y vamos a revisar sus logs para verlo en formato markdown, que sea más fácil de leer. Aquí tenemos su system prompt, ¿no? Le damos las instrucciones. Entonces, en base a eso, aquí está, bueno, toda la transcripción de la clase. Fue una clase larga. Entonces, ahora sí, aquí ya nos da la información. Vemos que la clase fue de N8N, su interfaz, nodos esenciales y flujos de trabajo. Nivel técnico intermedio, temas principales, interfaz y navegación de N8N, tipos de nodos y funcionalidades, transformación de datos con JSON, web hooks y y gestión de APIs. Vemos una síntesis de la clase, conceptos fundamentales, N8N como plataforma de automatización, arquitectura de datos en N8N, técnicas específicas o los how to, que es qué fue lo que se hizo y cómo hacerlo. Configurar la instancia de N8N, paso uno, paso dos, paso 3. La versión autohospedada no permite administrar múltiples usuarios ni proyectos, o sea, consideraciones importantes. La parte dos, creación de un flujo básico, viene también por pasos, transformación de datos con los nodos edit fields, ¿no? Secretos e insights compartidos, ¿no? Parte de los data pinning, extensiones de Chrome, eh arquitectura multitenant, buenas prácticas y recomendaciones, eh herramientas y configuraciones. Como podemos ver, la respuesta que nos da este LLM tiene una estructura completamente distinto. ¿Por qué? Porque la minuta que recibió o la transcripción que recibió era distinta, era de otro contenido, de otro tema. Entonces, aquí podemos ver justamente un caso aplicado del roteador. ¿Por qué? Porque recibe la transcripción y determina cuál es el LLM más adecuado para poder procesar esta transcripción y que pueda arrojar un resultado preciso. ¿Okay? Hemos visto secuencias y elecciones, pero y si necesitamos hacer varias cosas a la vez, para esto vamos a utilizar el tercer patrón, paralelización. Piensa en dividir un gran proyecto en partes y que varios equipos trabajen en ella simultáneamente. Para esto, dividimos la tarea principal en subtareas, las ejecutamos todas al mismo tiempo y luego unimos los resultados. Esto que nos da velocidad y especialización. Vamos a ver un ejemplo concreto. Para crear contenido en YouTube, necesitamos varias piezas creativas rápido. Vamos a tener tres ramas. Una va a generar cinco opciones de títulos virales y llamativos. Otra va a generar el texto que va a estar en la miniatura, que es igualmente de importante que el título. Y la última va a redactar una descripción optimizada para SEO, incluyendo los capítulos y C to action. Al final todas se van a unir en un mismo documento. El resultado es que en segundos tienes todos los componentes claves de tu video generados en paralelo por IA especializadas. Listo para publicar. Multiplica tu productividad haciendo que inteligencias artificiales trabajen en equipo al mismo tiempo. Okay, vamos a ver cómo funcionan. Vamos a abrir el chat. Le voy a pegar la transcripción de uno de mis videos. En este caso es el primer video que hice. Se lo voy a dar. Y este lo va a pasar a los tres LLMs. Cada LLM está especializado, uno en crear las descripciones, otro en el en el texto del thumbnail o la miniatura y otro va a crear los títulos. Entonces, estos tres LLMs se van a ejecutar para poder crear un documento cada uno que posteriormente se va a juntar y va a crear un único documento. Vamos a analizarlo. Empezamos con el creador de descripciones, que fue el primero que se ejecutó. Nos vamos a ir a los logs. Aquí está el system prompt. Vamos a ver también la transcripción que le pasamos, de todo el video y ya está abajo la respuesta. Entonces, aquí ya nos da la descripción. Esta descripción ya está optimizada para SEO. Entonces, básicamente ya nos dice, "Descubre por qué los generalistas en IA dominarán en el 2025 las cuatro habilidades clave para transformar empresas tradicionales en potencias digitales automatizadas. ya nos da los time stamps. En este caso, eh como la transcripción que le di no los incluía, pues en este caso sí los inventó. Si la transcripción hubiera tenido los time stamps, eh, los hubiera tomado directamente de ahí, pero aquí lo está dividiendo por capítulos. Los detalles aprende cómo una sola persona puede realizar el trabajo de un equipo completo utilizando IA, ¿no? ¿Qué puede hacer? construir software sin código, automatizar procesos, delegación de agentes, las herramientas mencionadas, recursos y eh cómo conectar conmigo, básicamente, ¿no? Entonces, ahí está justamente una descripción optimizada para YouTube. Luego nos pasamos el siguiente, que es el generador de el texto para la miniatura. Nos va a dar una, dos, 3, cuatro versiones distintas, ¿no? Que es la versión directa 2025. Los generalistas de IA ganarán. Versión provocativa, no serás relevante en 2025 si no haces esto. Versión beneficio, cuatro habilidades de IA que reemplazan 10 empleados y versión aspiracional de principiante a experto de en IA en semanas, ¿no? Que este sería el texto que va a ir en la miniatura y ya pues nos da nos dice cuáles son los patrones identificados, elementos de alto impacto y las consideraciones. Y ya para el título, de igual manera nos vamos a esta a la al último modelo o al último LLM y ya nos dice opción uno, los generalistas de IA dominarán 2025. Cuatro habilidades para ser imparable. El secreto para reemplazar equipos completos con IA en 2025. Cuatro herramientas de IA para automatizar tu empresa en 2025. Y ya pues bueno, nos da eh todas las especificaciones, las justificaciones y nos da una recomendación principal. Entonces, el trabajo de cada uno de estos lls pasa por este flujo para poder agregar todo, ¿no? En una sola en una sola variable y aquí incluso lo podemos descargar en un único documento, ¿no? Aquí ya le doy en download y aquí vemos el documento que nos descargó. Ya es un documento completo basado en el contenido del video. Propongo los siguientes títulos optimizados. Opción uno, ya nos da, opción uno, opción dos, opción tres, recomendación principal, las razones y luego ahora nos da lo del thumbnail, ¿no? Basado en en el análisis del contenido, te propongo los siguientes textos optimizados para el thumbnail o miniatura y ya nos los da. Y si le seguimos bajando, ahora ya nos da también aquí el contenido, ¿no? La parte de la descripción. Entonces, ya tenemos un único documento con títulos sugeridos, texto para el thumbnail y la descripción básicamente para escoger cuál, copiar y pegar. Okay, los equipos de IA son muy buenos, pero ¿quién los dirige? Aquí entra el patrón que puede cambiar negocios completos. Pon mucha atención. Este es el cuarto patrón, orquestador con trabajadores. Aquí no hay un flujo fijo, hay un director que es el orquestador que analiza la tarea y decide cuántos y cuáles trabajadores necesita en tiempo real. Es un gerente de proyectos con IA. Este es el patrón que automatiza departamentos completos. El orquestador es el cerebro estratégico. Los trabajadores son los especialistas ejecutando las microtareas. Imagina un director de marketing. El usuario le dice, "Necesito una campaña para redes sociales para el producto X. Quiero tres anuncios para Instagram y dos para LinkedIn." El orquestador recibe esto. Analiza producto X, campaña en redes, tres anuncios Insta, dos anuncios LinkedIn. Decide. Necesito un creativo para las imágenes o ideas visuales, un copywritter para los textos especializados, uno en Instagram y otro para LinkedIn. Entonces, el agente de director creativo genera cinco ideas visuales o borradores. El agente copywritter para meta genera tres copies para Instagram y el agente para LinkedIn genera dos copies para LinkedIn. Ya que tenemos el trabajo de todos los agentes, los unimos en un solo documento final, un departamento de marketing bajo demanda. Le pides lo que necesitas y el sistema se autoorganiza para entregarlo. ¿Ves el poder? Okay, vamos a revisar este flujo. Vamos a abrir nuevamente el chat y le vamos a pedir, crea tres anuncios de meta y uno de LinkedIn para Mindwers, empresa de automatización y agentes de IA. Entonces este mensaje del chat va a pasar directamente al orquestador, que en este caso sería el director de marketing. Aquí, en este caso, el orquestador tiene un modelo de razonamiento. ¿Por qué? Porque tiene que orquestar, entonces necesita más potencia mental o potencia cognitiva para poder identificar bien qué es lo que le va a pasar a cada uno. En este caso estamos utilizando el modelo O3 Mini. Entonces, una vez que el orquestador ya pasa los ítems o pasa el trabajo, ya lo va a delegar justamente a los a los subagentes. En este caso, como le pedimos tres anuncios de meta y uno de LinkedIn, estamos hablando que al director creativo le está pasando cuatro anuncios, los tres de meta y el de LinkedIn. Le está pasando tres al copywriter de meta y uno al copywriter de LinkedIn. Entonces, vamos a esperar ahora a que se ejecuten agentes para ver cuál va a ser el resultado final. Aquí vemos que el director creativo ya terminó. Vamos a meternos al log para ver qué fue lo que hizo. Tenemos cuatro llamadas al modelo. ¿Por qué? Una por cada anuncio. Entonces, vamos a revisar la primer llamada. Aquí está su system prompt. y vamos a ver su resultado. Nos da las especificaciones. Aquí podemos verlo. En este caso, especificaciones de anuncio, imagen única para mind workers, análisis de audiencia objetivo, especificaciones técnicas, estructura y composición, elementos visuales estratégicos, tratamiento cromático y tipográfico, mensaje clave y llamadas a la acción. Okay, viene toda la información. Luego nos pasamos al siguiente. Este es un carrusel. Entonces, en el carrusel nos da ya la estructura y la composición y nos va describiendo la imagen uno, la imagen dos, qué va a contener, la imagen tres, etcétera, etcétera. Luego en el siguiente anuncio podemos ver que es un video, ¿no? Entonces aquí ya nos da toda la información también para poder crear este video. Y en el cuarto anuncio, que es la imagen para LinkedIn, básicamente ya nos da también la información. Entonces, este agente lo que hizo es que por cada uno de los anuncios creó un resultado. Entonces, ahí tenemos los cuatro resultados. Ahora vamos a irnos al de meta. Nos vamos a los logs y vamos a ver cuál fue su output. Mira, aquí en este primero pues está pidiendo más información. Aquí habría que cambiar su prompt es la que va a utilizar para que no solicite más información. En la segunda vuelta podemos ver que aquí sí sacó el el anuncio, ¿no? Entonces aquí ya desarrolla el copy para cada diapositiva. Así como el director creativo en el segundo anuncio es un carrusel, esa misma instrucción llega también al copywriter de meta para que cree el copy de cada diapositiva, ¿no? Entonces aquí tenemos las cinco diapositivas y justamente cada una con su descripción. Y en el tercer anuncio tenemos ya el copy para el video, ¿no? Que básicamente también eh es parte de la instrucción que dio el orquestador o el director de marketing. Si nos pasamos al último, que es el agente de LinkedIn, que se le pide un anuncio, nos vamos a los logs y revisamos aquí que tiene el copy enfocado a LinkedIn. Si vemos es otro formato distinto que los que estaban siendo utilizados para meta. Entonces, pues bueno, aquí ya nos da toda la información. Liderazgo estratégico en automatización e inteligencia artificial, nuestra propuesta de valor, ¿por qué elegirnos, etcétera, etcétera. Entonces, ya que se procesaron todos los anuncios, en este caso los cuatro diseños, tres copies de meta y un copy de LinkedIn pasan al nodo merch en donde aquí los vamos a agrupar para pasarlos a una sola lista. Y aquí en este nodo de código lo único que hacemos es esa lista concatenarla en una sola cadena de texto para poder descargarla, ¿no? Aquí no voy a hacer tanto hincapié. Entonces aquí yo ya si me voy a este último nodo, voy a tener ya el informe completo que voy a poder descargar. Justamente aquí le doy en download y podemos ver cuál es el documento que nos redactó. Aquí ya tenemos en diseño las especificaciones del anuncio uno, que es la imagen única para mind workers. nos nos aparece ya el diseño dos, que son las especificaciones del anuncio de carrusel, que también es para meta. Seguimos bajando, nos aparece el diseño tres, que es el anuncio en video, también para meta. Y si seguimos bajando, tenemos el diseño 4, que es justamente la imagen para Linked. Seguimos bajando. Tenemos el copy de meta, el copy copy 2, el copy 3 y por último tenemos el copy de LinkedIn. Entonces, tenemos ya en un solo documento toda la información requerida para crear la campaña. Ya nada más esto se lo pasamos a diseñador. Podríamos también meter un agente que se encargue de crear los diseños con inteligencia artificial a partir de las instrucciones del director creativo. Pero bueno, ese ya sería otro flujo y ya podríamos tener una automatización más avanzada de todo el departamento de marketing, ¿no? Este es nada más un ejemplo de lo que podemos lograr con este tipo de arquitectura, que es el patrón orquestador con trabajadores. Deja de construir flujos rígidos, construye directores de IA que creen sus propios equipos al instante. Tenemos sistemas inteligentes y flexibles, pero ¿cómo nos aseguramos que no se equivoque? Aquí entra el quinto patrón, el evaluador optimizador. Es como tener un supervisor de calidad para tu agente de inteligencia artificial. Esto implementa un ciclo de feedback para que la IA revise su propio trabajo garantizando precisión y fiabilidad. Una ya genera una respuesta y luego otra ya, que es el evaluador la revisa con criterios específicos como precisión, tono, formato. Si no cumple, le da feedback a la primera ya y lo intenta de nuevo. Este es un ciclo de mejora continua. Vamos a ver un ejemplo de negocio que requiere fiabilidad máxima y tiene poca o nula tolerancia de errores. Hacer un análisis financiero. El director de la compañía le dice a la gente de IA, "Dame las ventas totales del último trimestre por categoría de producto." Paso uno, tenemos a un agente analista especializado con acceso a la base de datos financiera que traduce la petición a una consulta SQL. Ejecuta el query y obtiene los datos. Paso dos, pasa esto información a un agente o LLM auditor, es decir, antes de mostrar algo, un LLM lo revisa. Uno, la información obtenida responde directamente a la pregunta del usuario. Dos, los datos provienen exclusivamente de la base de datos financiera autorizada. Esto para evitar alucinaciones. Paso tres es el ciclo del feedback. Si cumple, se muestra el resultado. Si no cumple, el evaluador da feedback específico. Por ejemplo, incluiste datos del trimestre dos o la consulta no filtró bien por categoría o la más crítica. La información que diste no proviene de la base de datos, que muchas veces pasa en el tema justamente de las alucinaciones, que los agentes dan respuestas en base a su entrenamiento y no en base a la a la información o a la base de datos que le proporcionamos. Entonces, esto vuelve al generador para que refine la consulta SQL y repita esto. Asegura que las decisiones críticas se basen en información correcta y verificada. Imprescindible en finanzas, legal o cualquier área sensible. Okay, vamos a probar ahora este flufo. Vamos a abrir nuevamente el chat y le vamos a escribir, quiero que me des la información de ventas, utilidades y promedio de ventas por cliente de los últimos 3 años. Entonces, una vez que se mande el chat, el mensaje, se va primeramente a My Sequel a obtener el esquema de la base de datos. Una vez que se obtiene el esquema, nos se va a un nodo de agregar que este lo va a convertir en un JSON para que posteriormente este esquema en un en un string se le pueda pasar al agente financiero, que es el que va a hacer las consultas en SQL. Entonces, aquí ya está procesando el agente financiero la consulta. vemos que le pasa la información al validador de alucinaciones, que en este caso es el evaluador, y el evaluador va a determinar si es correcto o no es correcto. Vamos a revisar cómo fue el flujo. Vamos a revisar primeramente el log del agente financiero. Vemos aquí que el query que utiliza, aquí le pasamos, aquí le pasamos todo el esquema y dice que va a dividir esta consulta en partes para obtener la información solicitada. Entonces hace ya aquí la consulta SQL, utiliza su herramienta de SQL Query, entonces ya la responde ventas totales, utilidades totales, promedio de ventas por cliente y número de clientes. Entonces elabora ya su respuesta final. Su respuesta final basado en los resultados obtenidos. Aquí está el análisis de la información solicitada de los últimos 3 años, del 7 de abril del 2022 al 7 de abril del 2025. Este es el monto total de las ventas. ya nos da las ventas, las utilidades y el promedio de ventas, número de clientes y ya hace unas observaciones y análisis en base a esta información. ¿Okay? Entonces pareciera pareciera que es correcto. Si nos vamos ya al siguiente, que es el validador de alucinaciones, vemos en sus logs que recibe la información y nos dice que es válido y su justificación es la consulta de SQL es sintácticamente correcta y obtiene exactamente la información solicitada. Ventas totales, utilidades y promedio de ventas por cliente para los últimos 3 años. La consulta maneja correctamente la exclusión de ventas canceladas, calcula las utilidades restando los gastos asociados de las ventas y obtiene el promedio de ventas por cliente de manera precisa. La respuesta del agente se basa estrictamente en los datos obtenidos de la consulta SQL y proporciona un análisis detallado que se deriva directamente de los resultados numéricos obtenidos. Entonces, pareciera que la información es correcta. Entonces, como ya lo validó, eh pasa al siguiente nodo, que es un un if, que es si está validado, entonces ya retorna la respuesta final. En caso de que no, ya se volvería a pasar la información al agente financiero con el feedback del validador de alucinaciones. Aquí voy a hacer otra consulta para ver el proceso nuevamente. Dame las utilidades por cliente de los últimos 3 meses y dime cuál es el cliente más rentable. A ver qué nos dice. Pasa la información al agente financiero, está elaborando la consulta. Pasa ahora al validador de alucinaciones para poder verificar que sea correcta la respuesta del agente anterior. Y nuevamente es correcta esta respuesta. Vamos a revisar exclusivamente ahora el validador de alucinaciones. ¿Qué es lo que nos dice? Dice que es true. La consulta SQL es sintácticamente correcta y lógicamente apropiada para responder la pregunta. Incluye todos los elementos necesarios. Uno, filtra por los primeros tr meses, por los últimos tr meses. Dos, calcula correctamente las utilidades. Ventas menos gastos por cliente. Tres, ordena los resultados por utilidad para identificar al más rentable. Cuatro, incluye información relevante como ventas totales y gastos. La respuesta proporcionada se deriva exclusivamente de los resultados. Pareciera que es correcto. Aquí yo podría seguir haciendo múltiples preguntas y en el momento en el que el agente financiero ejecutara una respuesta que no consultara en su base de conocimientos financiera o que la estructura NSQL en la cual hace eh esta consulta fuera incorrecta, automáticamente la va a regresar el validador y le va a dar retroalimentación para que vuelva a a crear una nueva consulta ya con la información adecuada. Entonces, aquí ya vemos que la respuesta validada pasó por los dos por los dos filtros, en este caso el validador de alucinaciones y el agente financiero. Vamos a intentar una más. En este caso se la voy a poner un poco más difícil. Dame las ventas del día de hoy y los clientes que no han comprado en los últimos dos años. Vemos que ejecuta ya su query en SQL. En esta consulta parece que tiene un error en su query, en su segunda iteración. Tercera iteración, lo pasa ya al validador de alucinaciones y parece que es correcta. Nuevamente aquí vemos que tuvo cinco iteraciones. Vamos a revisar cuál fue su proceso. La primera obtiene una respuesta vacía de la herramienta. La segunda es donde le da el error. La tercera, respuesta vacía, la cuarta ya obtiene el total de clientes, que son cinco, y total de ventas 62. Y en la última obtiene aquí justamente es la última compra de cada cliente. Entonces con estas seis iteraciones pudo obtener la respuesta correcta. Entonces, si nos vamos al validador de alucinaciones, vamos a ver sus logs, recibe toda la información de los pasos intermedios, en este caso del del agente. Y ya nos dice que la que es validado true. Las consultas SQL ejecutadas son correctas y responden adecuadamente a la pregunta del usuario. La primera consulta busca las ventas del día actual, incluyendo detalles relevantes del cliente y producto. La segunda consulta identifica clientes sin compras en los últimos 2 años. Los resultados obtenidos, ninguna venta hoy y todos los clientes con compras recientes son considerados con los datos y la respuesta proporcionada refleja fielmente esta información sin sin añadir inferencias no respaldadas por datos. Pues bueno, entonces ya la información final justamente que nos da es todos los clientes han realizado compras recientemente, siendo la última fecha de compra para todos el 4 de marzo del 2025, ¿okay? No hay clientes que no hayan comprado en los últimos 2 años y no hay ventas registradas para el día de hoy. Entonces, las respuestas fueron correctas. En dado caso que hubiera habido una respuesta errónea, inmediatamente el validador de alucinaciones hubiera saltado y le hubiera dado el feedback. Entonces, de esta manera podemos tener la certeza de que lo que le hagamos o las preguntas que le hagamos a este agente, sobre todo en cuestiones financieras que son críticas, van a ser correctas. No confíes ciegamente en la IA. Implementa sistemas que se autocorrijan. Hemos visto arquitecturas complejas, pero a veces lo más importante es que el sistema nunca falle. Sexto patrón, bueno, un bonus crítico para entornos empresariales reales, el fall de recuperación de errores. ¿Qué pasa si tu superagente de IA falla por un momento? ¿Se detiene tu negocio? No, siempre debes tener un plan B. Puede ser otro modelo de ya más simple o más robusto, un flujo más básico o incluso una notificación a un humano. La clave es la continuidad. El objetivo de este patrón es tener un sistema de respaldo que asegure la operación si el principal falla. Negocio que vamos a utilizar es un sistema de almacenamiento de documentos inteligente para RAJ. Vamos a procesar documentos largos y críticos como contratos o transcripciones de llamadas para extraer información añadiendo metadatos y dividiendo el documento en secciones coherentes. Vamos a tener un LLM principal, inteligente, pero económico, que intente dividir el documento en partes. Esto es conocido como chonking. Analiza cada parte, extrae datos clave y agrega metadata, contexto, time stamp, categorías, etcétera. ¿Qué pasa si falla? entre el sistema Foldback, lo que activa automáticamente un LLM con otro modelo, en este caso más robusto y por lo tanto más caro, que sea capaz de procesar el documento de forma fiable. ¿Qué pasa si falla el segundo LLM? Entra otro modelo de respaldo con las mismas instrucciones, pero con razonamiento avanzado, que es más caro, pero es nuestra última alternativa. De esta manera aseguramos que el documento se procese completo evitando cuellos de botella críticos. Okay, vamos a ver en funcionamiento. Entonces este flujo, aquí le vamos a pasar en un input array ahorita para términos prácticos, un documento bastante grande que fue previamente preprocesado en 117 partes, es decir, 117 chunks. Entonces, cada uno de estos chunks o cada una de estas partes va a pasar a al LLM para que se procese. Entonces este LLM lo que va a hacer es agarrar la metadata o crearle la metadata y ver si puede dividirlo en partes más pequeñas que tengan sentido, coherencia o que semánticamente puedan estar completas. Entonces, es una tarea bastante exhaustiva para unas para una cadena. Y entonces es un caso perfecto para este sistema fallack que es cuando puede fallar entra el que sigue. Entonces aquí voy a ejecutar el flujo y vamos a tener en nuestra primera cadena vamos a tener un modelo que es el Yemini 2.0 flash, que es un modelo económico y potente. En dado caso de que este fallara, pasaría a la siguiente, que va a estar utilizando un modelo que es O3 Mini de Open AI, que es un modelo mucho más caro y mucho más potente, pero es un modelo de backup. Entonces este nada más entraría en dado caso de que falle el anterior. Y si este llegara a fallar tenemos un tercer modelo que está utilizando Antropic 3.7 Cloud Sonet, que también queda como backup para que en caso de que fallen los anteriores dos, este pueda entrar y rescatar el flujo. Vamos a ver ahorita, por ejemplo, vamos en este en la octava iteración y va bien. Normalmente estos flujos en lo que pueden fallar comúnmente es en el output parser, es decir, en la manera en la que entregan el JSON final. Entonces esto es común en este tipo de cadenas, por eso utilizamos este sistema. Aquí vemos, por ejemplo, que ya falló. Ahorita está programado para que intente tres veces. En si en esas tres veces sigue fallando, ya se va a pasar al siguiente modelo. Aquí vemos que ya pasa el siguiente modelo. Entonces ya para esta iteración ya va a utilizar Open AI, el modelo de Out Mini. Entonces este es un modelo mucho más potente, como mencionábamos también es más caro, pero vemos que de 12 entra una con este modelo. Vamos a ver en qué resuelve. Listo. Este ya pudo parsear correctamente el chunk y vuelve otra vez a loop y otra vez vuelve a ser procesado por el primer modelo hasta que vuelva a tener un error. Si en este caso eh el segundo modelo fallara, entra en vigor el tercer modelo que tenemos como backup, que normalmente pues para eso tenemos pues estos tres. Si vemos que es un proceso crítico, podríamos poner un loop para que se repita tres veces o cuatro o cinco las que decidamos este mismo proceso. Aquí ya lo voy a parar hasta aquí que digo se entiende, se entiende cuál es la idea, ¿no? Voy a correr los 117 chunks que pues bueno, son bastantes. Ahorita lo que queríamos mostrar justamente es qué pasaba si el primer modelo, la primer cadena fallaba y cómo entraba eh la segunda cadena para poder rescatar el proceso en producción. La empresa con IA más inteligente es la que tiene un buen plan B. Ahí tienes los seis patrones multiagente. Estos no son solo conceptos, son arquitecturas probadas que separan a los que juegan con la IA de los que construyen el futuro de las empresas con ellas. Implementar IA sin entender estas estructuras es como navegar sin mapa. Ahora tienes el mapa. Úsalo para evitar errores costosos y empezar a obtener resultados reales. Sé que esto puede parecer mucho, pero empezar con uno o dos de estos patrones en procesos clave de tu negocio puede tener un impacto gigante. Si este video te gustó, suscríbete a mi canal porque seguiré revelando más estrategias avanzadas de IA para empresas. Si quieres aprender en serio cómo crear agentes de IA avanzados, estoy por terminar mi curso. Puedes registrarte en el link de la descripción y te avisaré en cuanto esté listo. Ahora, si quieres que te ayude a implementar agentes de IA avanzados en tu empresa o en la de tus clientes, mándame un correo y con gusto platicamos. También está en la descripción. Déjame un comentario sobre qué te gustaría que hiciera mi próximo video. Nos vemos. M.


Título: Turn Any Website Into LLM Ready Data in Seconds with n8n & Firecrawl
URL: https://www.youtube.com/watch?v=Ee9WtEEd300

Entonces, fir crawl nos permitirá convertir cualquier sitio web en datos listos para llm en cuestión de segundos y, como pueden ver aquí, también es de código abierto. Una vez que accedan a fir crawl, hagan clic en este botón y podrán obtener 500 créditos gratis para experimentar. Como pueden ver, hay cuatro cosas diferentes que podemos hacer con fir crawl: podemos raspar, podemos rastrear, podemos mapear o podemos hacer este nuevo extracto, que básicamente significa que podemos darle a firec una URL y también un mensaje como "¿puedes extraer el nombre de la empresa y los servicios que ofrece?" y un rompehielos de esta URL. Así que hay algunos casos de uso realmente interesantes que podemos hacer con fir crawl. En este video, veremos principalmente la extracción, pero también les mostraré la diferencia entre raspar y extraer. Y vamos a entrar en NN y conectarnos para que puedan ver cómo funciona esto. Pero el área de juegos será un muy buen lugar para comprender la diferencia entre estos diferentes puntos finales. Bien, para el bien de este video, este es el sitio web que estamos... Vamos a ver lo que se llama " citas para raspar" y, como pueden ver, tiene como 10 en esta primera página y también tiene diferentes páginas de diferentes categorías de citas y, como pueden ver, si hacemos clic en ellas, hay diferentes citas. Entonces, lo que voy a hacer es volver a la pantalla principal y voy a copiar la URL de este sitio web y vamos a ir a NN. Vamos a abrir un nuevo nodo que va a ser una solicitud HTTP y esto es solo para mostrarles cómo se ve una solicitud git estándar a un sitio web estático. Así que vamos a pegar el paso de prueba de acierto de URL y, en el lado derecho, vamos a obtener todo el HTML del sitio web de citas para raspar. Como dije, lo que estamos viendo aquí es un trozo desagradable de HTML que es bastante difícil de leer para nosotros, pero básicamente lo que está sucediendo aquí es que este es el código que va al sitio web para que tenga estilo y diferentes fuentes y diferentes colores, así que aquí lo que estamos viendo es toda la primera página de esto. sitio web, así que si buscáramos a Harry, si copio esto, volvemos a niden y controlamos F. Aquí pueden ver que está la cita exacta que tiene la palabra Harry, así que todo, desde el sitio web, está aquí, simplemente está envuelto en una especie de feo trozo de HTML. Ahora, volviendo al patio de juegos de firec, usando el punto final de scrape, podemos reemplazar esa misma URL, ejecutaremos esto y va a generar formato markdown, así que ahora podemos ver que en realidad tenemos todo lo que estamos buscando con las diferentes comillas y es mucho más legible para un humano, así que eso es lo que es un web scrape, ¿verdad? Obtenemos la información, ya sea HTML o markdown, pero luego normalmente la introduciríamos en algún tipo de llm para extraer la información que estamos buscando. En este caso, estaríamos buscando diferentes citas, pero lo que podemos hacer con extract es darle la URL y luego también decir "oye, obtén todas las citas aquí" y usando este método podemos decir no solo estas primeras 10 en esta página, quiero que rastrees todo el sitio. y básicamente obtener todas estas citas, todas estas citas, todas estas citas, todas estas citas, así que va a ser realmente genial, así que voy a mostrar cómo funciona esto en fir craw y luego lo vamos a conectar a nadn, muy bien, entonces lo que estamos haciendo aquí es decir que extraigamos todas las citas y autores de este sitio web. Le di el sitio web y ahora lo que está haciendo es generar los diferentes parámetros que el llm buscará extraer del contenido del sitio web. Bien, aquí está la ejecución que estamos a punto de ejecutar, tenemos la URL y luego tenemos nuestro esquema para lo que el llm va a buscar y está buscando texto que sería la cita y es una cadena y luego también va a buscar al autor de esa cita que también es una cadena y luego el mensaje que estamos alimentando aquí al llm es extraer todas las citas y sus autores correspondientes del sitio web, así que vamos a presionar ejecutar y vamos a ver que no solo va a Vaya a esa primera URL, básicamente tomará ese dominio principal, que es " scrape.on", también una conexión rápida, continúe y use el código herk 10 para obtener un 10% de descuento en los primeros 12 meses en su plan de rastreo de abeto. Bien, acaba de terminar, como puede ver, tenemos 79 citas, así que aquí abajo tenemos una respuesta JSON donde será un objeto llamado citas y allí tenemos un montón de elementos diferentes, que es, ya sabe, autor de texto, autor de texto, autor de texto y tenemos prácticamente todo de ese sitio web ahora, bien, genial, pero lo que queremos hacer es ver cómo podemos hacer esto en N-end para que tengamos, ya sabe, una lista de 20, 30 o 40 URL de las que queremos extraer información. Podemos simplemente recorrer y enviar esa automatización en lugar de tener que venir aquí y escribirla en el rastreo de abeto. Bien, entonces lo que vamos a hacer es volver a idn y me disculpo porque puede haber algunos saltos por aquí, pero básicamente vamos a borrar esta solicitud http y tomar una nueva. Ahora, lo que estamos haciendo es Vamos a ir a la documentación de fir craws. Solo tenemos que importar el comando curl para extraer Endo, en lugar de intentar averiguar cómo completar estos diferentes parámetros. De vuelta en fir craw, una vez que configures tu cuenta, en la parte superior derecha verás un botón llamado " docs" en el que debes hacer clic. Ahora podemos ver una guía de inicio rápido. Tenemos diferentes puntos finales. Lo que vamos a hacer es, a la izquierda, desplazarnos hacia abajo hasta "características" y hacer clic en "extraer". Esto es lo que buscamos. Tenemos información aquí. Lo primero que debemos observar es que cuando usamos el extracto, podemos extraer datos de estructura de una o varias URL, incluyendo comodines. Lo que hicimos fue no solo raspar una sola página, sino que básicamente raspamos todas las páginas que tenían el dominio base principal de "quotes scrape.on". Analiza todas las URL que puede descubrir y luego extrae los datos solicitados. Podemos ver que así es como funciona. Si volvemos a la solicitud que acabamos de hacer, podemos ver aquí que agregó una barra con Un asterisco después de cerrar el scrape. En esto, básicamente, se completará el método, que será una solicitud de publicación, se completará el punto final, se completará el tipo de contenido y se nos mostrará cómo configurar nuestra autorización. Luego, tendremos una solicitud de cuerpo a la que tendremos que hacer algunos cambios menores. En la parte superior derecha, haré clic en copiar. Volveré a Ed y presionaré "Importar curl", pegaré eso allí y presionaré "Importar". Como pueden ver, prácticamente todo se completó. Como dije, el método será una publicación. Ya tenemos el punto final configurado y lo que quiero hacer es mostrarles cómo configurar esta autorización para que podamos mantenerla guardada para siempre en lugar de tener que ponerla aquí en el panel de configuración cada vez. Primero, regresen a su navegador, vayan a " Claves de API" en el lado izquierdo y solo querrán copiar esa clave de API. Una vez que la hayan copiado, regresen a NN y ahora veamos cómo configuramos esto. Normalmente, lo que hacen es... tiene esto como parámetro de encabezado no todas las autorizaciones son encabezados pero este es un encabezado y la clave o el nombre es autorización y el valor es Portador espacio su clave API así que lo que normalmente hace es pegar su clave API ahí mismo y estará listo pero lo que queremos hacer es guardar nuestra credencial fir crraw de la misma manera que guardaría ya sabe una credencial de Hojas de cálculo de Google o una credencial de Slack así que vamos a entrar en autenticación haga clic en genérico vamos a hacer clic en tipo genérico y elegir encabezado porque sabemos aquí abajo es un encabezado desactivado y luego puede ver que tengo otras credenciales ya guardadas vamos a crear una nueva solo voy a nombrar este fir crraw para mantenernos organizados para el nombre vamos a poner autorización y para el valor vamos a escribir Portador con b mayúscula espacio y luego pegar nuestra clave API y presionaremos guardar y esto va a ser exactamente lo mismo que acabamos de hacer abajo excepto por ahora lo tenemos guardado así que en realidad podemos quitar este campo nosotros No necesitamos enviar encabezados porque los estamos enviando aquí mismo y ahora solo necesitamos averiguar cómo configurar esta solicitud de cuerpo. Bien, voy a cambiar esto a una expresión y abrirla solo para que podamos echarle un vistazo. Lo primero que notamos es que, de forma predeterminada, hay tres URL aquí que estaríamos extrayendo. No queremos hacer eso aquí, así que voy a tomar todo dentro de la matriz, pero voy a mantener las dos comillas. Ahora, todo lo que necesitamos hacer es poner la URL de la que queremos extraer información entre estas comillas. Aquí solo pongo las comillas para raspar. Después de eso, rastreará todas las páginas, no solo la primera página, que solo tendría como nueve o diez comillas. Y ahora el resto va a ser muy fácil de configurar porque ya lo hicimos en el área de juegos, así que sabemos exactamente qué va dónde. Voy a hacer clic de nuevo en nuestro ejemplo de área de juegos. Lo primero es que esta es la cita que envió el rastreo de referencia, así que la copiaré y volveré a edn y solo voy a reemplazar las indicaciones aquí mismo no queremos la misión de la empresa bla bla bla queremos pegar esto aquí y estamos buscando extraer todas las citas y sus autores correspondientes del sitio web y el siguiente es básicamente decirle al llm qué estás recuperando así que le acabamos de decir que está recuperando citas y autores así que necesitamos hacer que el esquema aquí abajo en el cuerpo de la solicitud coincida con la indicación así que todo lo que tenemos que hacer es volver a nuestro patio de juegos aquí mismo está el esquema que enviamos en nuestro ejemplo y solo voy a hacer clic en Vista Json y voy a copiar todo esto que está envuelto en llaves volveremos en ID final y comenzaremos después del esquema dos puntos espacio reemplazar todo esto con lo que acabamos de tener en um fir crawl y de hecho creo que he notado la forma en que esto copiado no va a funcionar así que déjenme mostrarles eso muy rápido si presionamos el paso de prueba dirá que el parámetro Json debe ser Json válido así que lo que voy a hacer es Voy a copiar todo esto ahora, llegué al chat GPT y solo digo que arreglen este JSON, lo que va a hacer es básicamente empujarlos cuando lo copien desde fir craw, los alinea a la izquierda, pero no quieren eso, así que como pueden ver, básicamente empujó todo, copiaremos esto en nuestro nadn allí mismo y todo lo que hizo fue empujar todo una vez y ahora deberíamos estar listos, así que muy rápido antes de probar esto, solo voy a llamar a esto extraer y luego presionaremos el paso de prueba y deberíamos ver que va a estar extrayendo y nos dará un mensaje que dice um verdadero y nos da un ID y ahora lo que tenemos que hacer a continuación es extraer este idid para ver si nuestra solicitud ya se ha cumplido, así que estoy de vuelta en la documentación y ahora vamos a ver aquí abajo la extracción asincrónica y la verificación de estado, así es como verificamos el estado de una solicitud como vieron, acabamos de hacer una, así que aquí voy a hacer clic en Copia este comando curl. Regresaremos a NN y agregaremos otra solicitud HTTP y la importaremos allí. Puedes ver que este será un comando git, tendrá un punto final diferente. Lo que debemos hacer, si revisas la documentación, es que al final de la barra de extracción, debemos colocar el ID de extracción cuyo estado buscamos verificar. En NN, el ID vendrá del lado izquierdo del nodo anterior cada vez. Simplemente cambiaré el campo URL a una expresión, pondré una barra invertida y luego tomaré el ID, lo colocaré allí y listo. Necesitamos configurar nuestras credenciales. Por eso es genial. Ya configuramos esto como genérico como encabezado. Ahora podemos extraer fácilmente nuestro paso de prueba de detección de incendios y de impacto. Lo que sucede ahora es que nuestra solicitud aún no se ha realizado, como puedes ver, regresa como procesamiento y los datos son una matriz vacía. Voy a configurar rápidamente algo llamado sondeo, donde básicamente estamos verificando un ID específico, que es este de aquí, y vamos a verificar si está vacío, si el campo de datos está vacío, eso significa que vamos a esperar una cierta cantidad de tiempo y volver e intentarlo nuevamente, así que después de la solicitud voy a agregar un "si es así", esto básicamente nos ayudará a crear nuestro filtro, así que estamos arrastrando json. dat, que como puedes ver es una matriz vacía y simplemente diremos que está vacía, pero una cosa que debes tener en cuenta es que esto no coincide, como puedes ver, estamos arrastrando una matriz y estábamos tratando de hacer un filtro de una cadena, así que tenemos que ir a la matriz y luego decir que está vacía y presionaremos el paso de prueba y esto dirá verdadero, el campo de datos está vacío y, por lo tanto, si es verdadero, lo que queremos hacer es agregar un peso y esto esperará, ya sabes, en este caso, solo diremos 5 segundos, así que si presionamos el paso de prueba, esperará 5 segundos y, en realidad, desearía haber cambiado la lógica para que esto estuviera en la parte inferior, pero lo que sea, y luego simplemente arrastraríamos esto de regreso aquí y lo intentaríamos de nuevo, así que ahora, después de que hayan pasado 5 segundos o el tiempo que sea, intentaríamos esto de nuevo y ahora podemos ver que tenemos nuestro elemento de vuelta y el campo de datos ya no está vacío porque tenemos nuestro objeto de comillas que tiene 83 comillas, así que incluso obtuvo más que eso Esta vez lo hicimos en el patio de juegos y creo que esto es solo porque, como sabes, el extracto aún está en versión beta, por lo que puede que no sea muy consistente, pero sigue siendo mucho mejor que si solo hiciéramos una solicitud de obtención simple y luego, como puedes ver ahora, si ejecutamos este siguiente paso, esto saldría, ah, pero esto es interesante, así que antes de saber qué está extrayendo el json. El campo DAT es una matriz, por lo que podemos configurarla como si la matriz estuviera vacía, pero ahora es un objeto, así que no podemos pasarla por el mismo filtro porque estamos viendo un filtro para una matriz. Así que lo que estoy pensando aquí es que podríamos configurar esto para continuar usando la salida de error. Debido a que este nodo generaría un error, podríamos hacer un paso de prueba y podríamos ver que ahora bajará por la rama falsa, lo que básicamente significa que nos permitirá continuar con el proceso y podríamos hacer lo que queramos aquí. Obviamente, esto no es perfecto porque acabo de configurar esto para mostrárselo y me encontré con eso, pero esa es normalmente la forma en que pensaríamos: ¿cómo podemos hacer esto un poco más dinámico? Porque tiene que lidiar con matrices vacías o potencialmente con objetos llenos. De todos modos, lo que quería mostrarles ahora es, de vuelta en nuestra solicitud, si nos deshiciésemos de este Aster, ¿qué sucedería? Así que simplemente vamos a ejecutar todo este proceso de nuevo. Haré un flujo de trabajo de prueba y ahora enviará esa solicitud solo a una URL en lugar de... el otro Aha y me alegro de que estemos haciendo pruebas en vivo porque cometí el error de poner esto como json. ID que no existe si extraemos del nodo de peso, por lo que todo lo que tenemos que hacer aquí es deshacernos del json. Identificación y extracción, básicamente, una variable de referencia de nodo, por lo que haremos dos llaves, extraeremos del nodo de extracción y ahora solo queremos decir elemento. json. ID y deberíamos estar listos ahora, así que solo voy a actualizar esto y lo haremos completamente de nuevo, así que prueba el flujo de trabajo, estamos haciendo exactamente lo mismo, todavía no está listo, así que vamos a esperar 5 segundos y luego vamos a volver a verificar, con suerte deberíamos ver bien, todavía no está listo, así que vamos a esperar 5 segundos más, volver a verificar y luego, cuando esté listo, ahora como puedes ver, baja por esta rama y podemos ver que realmente recuperamos nuestros elementos y lo que ves aquí es que esta vez solo obtuvimos 10 comillas, um, sabes que dice nueve, pero las computadoras cuentan desde cero, pero solo obtuvimos 10 comillas porque um no pusimos un asterisco después de la URL, así que para el rastreo no sabía que necesitaba raspar todo de toda esta URL base. Solo voy a raspar en esta página específica, que es esta de aquí, que de hecho solo tiene 10 comillas y, por cierto, una plantilla súper simple aquí, pero si quieres probarla y simplemente conecta tu clave API y diferentes URL que puedes obtener en la Comunidad Escolar gratuita, entrarás allí, harás clic en Recursos de YouTube y harás clic en la publicación asociada con este video y tendrás el JSON allí mismo para descargar una vez que lo descargues, todo lo que tienes que hacer es importarlo desde un archivo aquí arriba y tendrás el flujo de trabajo, por lo que hay muchos casos de uso interesantes para el rastreo de abetos. Sería genial poder extraer de una hoja, por ejemplo, 30, 40 o 50 URL que queremos revisar y luego actualizar en función de los resultados. Puedes hacer cosas realmente interesantes aquí, como investigar un montón de empresas y luego hacer que también cree un alcance inicial para ti, así que avísame si quieres ver ese tipo de cosas. Además, si buscas un enfoque más práctico con naden y aprender a construir automatizaciones de IA, definitivamente echa un vistazo a mi Comunidad paga, el enlace para eso también está en la descripción. Tenemos una gran comunidad de miembros que se dedican a aprender NN compartiendo los problemas con los que se encuentran y tenemos una gran sección de aula con temas de inmersión profunda como agentes de construcción API de bases de datos vectoriales y solicitudes HT CP y compilaciones paso a paso también tenemos cinco llamadas en vivo por semana como puedes ver hemos estado trayendo algunos oradores invitados geniales y las llamadas siempre se graban si no puedes asistir, así que me encantaría verte en esas llamadas en vivo y en la comunidad pero eso será todo por este video sé que fue más rápido y fue un estilo un poco diferente les mostré algunos tipos de resolución de problemas en vivo diferentes problemas que vimos pero espero que lo hayan disfrutado si lo hicieron por favor dejen un me gusta siempre me ayuda y realmente aprecio que hayan llegado al final del video los veré en el próximo gracias a todos

Título: n8n + Crawl4AI - Scrape ANY Website in Minutes with NO Code
URL: https://www.youtube.com/watch?v=c5dw_jsGNBk

La semana pasada les presenté Crawl for AI, un web scraper de código abierto y amigable con LLM que hace que sea muy fácil rastrear cualquier sitio y formatearlo para una base de conocimiento RAG para su agente de IA. Incluso hice un video de seguimiento donde construí un agente RAG AI completo aprovechando esta base de conocimiento que construimos con Crawl 4 AI. La cosa es que hicimos todo con código Python y muchos de ustedes me pidieron que hiciera lo mismo en N8N para que podamos raspar sitios y construir una implementación RAG completa sin código. Me lo pidieron y ahora estoy entregando este video, así que ahora mismo les mostraré cómo implementar Craw for AI de manera súper fácil con Docker y aprovecharlo en sus flujos de trabajo N8N para raspar páginas del sitio web en solo segundos y vamos a hacer esto sin escribir una sola línea de código y vamos a construir un agente de IA simple que aprovecha Crawl for AI para construirse una base de conocimiento en Superbase para que pueda ser un experto en Pantic AI, que es mi marco de agente favorito en este momento. Un pequeño ejemplo ordenado y este es el tipo de flujo de trabajo final. que realmente puedes robar para ti mismo y extender para que se ajuste a cualquier caso de uso que tengas. Hay muchas maneras de rastrear sitios web, pero la mayoría son lentas, costosas y no son muy fáciles de usar. Pero Crawl for AI es todo lo contrario: es rápido, intuitivo y completamente gratuito porque es de código abierto. Quiero decir, lo único que realmente tienes que pagar es la máquina en la nube para alojar tu rastreador, eso es solo si no lo estás ejecutando en tu propia computadora. Por lo tanto, es una plataforma fantástica y la recomiendo, aunque hay muchas maneras específicamente dentro de NN para rastrear sitios web, pero en mi opinión, todas se quedan cortas ante Crawl for AI. Así que estoy muy emocionado de mostrarte esto ahora mismo. Así que con eso, vamos a sumergirnos en ello. Bien, este es el flujo de trabajo completo de 8 extremos que vamos a repasar en este video. Tenemos la parte inferior aquí, que es lo que realmente aprovecha Crawl for AI para raspar un montón de páginas y ponerlas en una tienda de vectores de superbase para Rag. Y luego, la parte superior aquí es una implementación de prueba de concepto muy simple para un agente de IA que aprovecha esta base de conocimiento. Entonces, esto es básicamente seguir lo que construimos dentro de Python en los últimos videos en mi canal, así que antes construimos todo directamente dentro de un script de Python aquí, así que extraemos todas las páginas de Pantic AI, las raspamos todas usando Crawl for AI y luego las colocamos en nuestro almacenamiento Vector de superbase tal como lo haremos en n8n, pero lo haremos ahora sin ningún código, así que diga adiós a este código, lo estamos haciendo todo en n8n y solo como revisión, voy a usar exactamente la misma documentación aquí para Pantic AI como ejemplo, es solo un gran recurso para usar porque es el tipo de sitio que estamos bien raspando, es muy importante tener en cuenta que tengo que presentar este video, si desea raspar éticamente, hay muchos sitios web que bloquean los raspadores y le solicitan que no raspe, y es importante seguir esas reglas, la forma en que puede verificar generalmente es si va a un sitio web como youtube.com y agrega / robots. texto verás algunas reglas aquí que te ayudan a entender cuándo y cómo puedes raspar muchas veces te dirán que no raspes en absoluto los términos de uso también es otro buen lugar para verificar cualquier sitio web o plataforma solo para asegurarte de que estás siguiendo las reglas y raspando éticamente así que con eso en mente lo que podemos hacer para la documentación de Pantic AI quiero una forma rápida de obtener todas las páginas web disponibles aquí, así que básicamente toda la navegación que tenemos aquí quiero poder extraer todas estas URL para mí dentro del flujo de trabajo N8 y la forma en que vamos a hacerlo es con lo que se llama un sitemap.xml así que si agregas SLS sitemap.xml al final de muchos sitios web como páginas de documentación y tiendas de comercio electrónico obtendrás este XML aquí que te da la mayor parte del tiempo cada página web que está disponible en este sitio y así podemos analizar esto dentro de nuestro flujo de trabajo n8n tal como lo hicimos con Python antes para obtener todas las páginas web y luego rasparlas una a la vez con Crawl 4 AI, de modo que ese es realmente el comienzo de este flujo de trabajo, pero antes de siquiera comenzar a desarrollar este flujo de trabajo n-end, y lo repasaré con bastante detalle para usted, primero necesitamos implementar Crawl 4 AI para que podamos usarlo como un punto final de API dentro de n8n, así que cuando tenía el código de Python antes, simplemente importamos Crawl for AI como una biblioteca, pero ahora necesitamos llamar a un punto final de API donde tenemos Crawl for AI alojado externamente con Docker, así que antes de entrar en esto, le mostraré exactamente cómo hacerlo y no se preocupe, es súper fácil. Aquí estamos en la documentación de Crawl 4 AI. Ahora, cuando usaba Python y no n8n, solo tenía que ir a la pestaña de instalación para comenzar. Usé pip, que es el administrador de paquetes de Python para instalar Crawl for AI y luego lo importé directamente en mis scripts de Python y lo usé allí, pero en n8n no tenemos la misma opción, no hay un lugar para ejecutar este comando de instalación de pip y no es como si pudiéramos usarlo directamente dentro de cualquier Los scripts de Python no son una dependencia disponible para nosotros, por lo que para n8n tenemos que implementar Crawl for AI de una manera completamente diferente, y es por eso que mencioné Docker hace un minuto. Docker es nuestro boleto para implementar Crawl for AI de una manera en la que realmente podemos interactuar con él como un punto final de API, y eso es lo que haremos dentro de nuestro flujo de trabajo de n8n, así que lo repasaremos rápidamente y, de nuevo, repasaré este flujo de trabajo con mucho detalle en un momento. Estos nodos de solicitudes HTTP son donde interactuamos con Crawl for AI, así que tengo este punto final de API configurado que he alojado en la nube y les mostraré cómo hacerlo con Docker. Esto solo está esperando que enviemos solicitudes para rastrear un sitio web específico, así que le damos una URL para rastrear y devolverá el Markdown y los enlaces a las imágenes y tendrá todo el HTML de vuelta para nosotros para que luego podamos procesarlo y ponerlo en nuestra base de conocimiento para Rag, y eso es lo que haremos ahora mismo: configurar Docker y es importante tener en cuenta que hay algunas formas diferentes en las que podemos instalar Crawl for AI con Docker la primera opción es que podríamos incluir Crawl for AI en la imagen de Docker dentro de nuestro kit de inicio de AI local que es una opción fantástica que probablemente cubriré en otro video, así que déjame saber en los comentarios si estás interesado en ver eso la otra solución que voy a mostrar ahora mismo es no hacer eso solo porque quiero algo un poco más universal y llegaré a eso como mi última solución aquí la segunda opción es que puedes ejecutar Crawl for AI simplemente ejecutando este contenedor localmente o en la instancia de la nube que ya tienes ejecutando n8n si estás alojando n8n por tu cuenta, así que es una forma fantástica de hacerlo si no quieres gastar más para tener una instancia dedicada para Crawl for AI porque si ya estás pagando para que la máquina aloje n8n por tu cuenta o simplemente lo estás ejecutando localmente en tu máquina, no tienes que pagar nada ahora hay una cosa a tener en cuenta y es que el contenedor Docker de Crawl 4 AI es un poco de Una CU que consume muchos recursos ejecuta un navegador sin interfaz gráfica para rastrear la web por ti, por lo que requiere una buena cantidad de CPU y RAM, y por eso recomendaría alojarlo por separado porque si tu rastreo para la instancia de AI se está atascando mucho, no quieres que esté alojado en el mismo lugar que tu n8n porque eso también podría ralentizarlo, así que recomendaría alojarlo como una instancia completamente separada en la nube y eso es lo que te voy a mostrar cómo hacer ahora mismo, así que si solo quieres alojarlo directamente en tu computadora o directamente en una instancia que ya tienes ejecutando n8n, puedes ejecutar estos comandos aquí mismo, puedes extraer la imagen de Docker, obviamente, siempre que ya tengas Docker instalado y luego puedes ejecutarlo y también puedes ejecutarlo opcionalmente con un token secreto para que puedas proteger tu punto final para que no todos puedan usarlo para usar tu rastreo para AI y entonces sí, estos dos comandos son todo lo que necesitas para comenzar, es muy fácil ahora la configuración más complicada es cuando realmente quieres tenerlo como una API dedicada punto final y eso es lo que vamos a cubrir ahora mismo hay muchas plataformas para ejecutar un contenedor Docker como un punto final de API, aunque la que he encontrado más fácil con diferencia es digital ocean. Cubro digital ocean bastante en mi canal, me encanta su producto no patrocinado por ellos de ninguna manera solo los uso para mis instancias en la nube y también cosas como esta y estoy a punto de mostrarte las implementaciones de Docker y entonces la forma en que puedes implementar crawl para AI dentro de digital ocean es usando su plataforma de aplicaciones así que voy a hacer clic en esto para crear una nueva plataforma de aplicaciones y puedes extraer de un repositorio git que es súper ordenado pero en este caso voy a extraer de una imagen de contenedor aquí y el crawl 4 imágenes de contenedor de AI están alojadas aquí en dockerhub así que acabo de ir a hub. deer.com busca crawl for AI y esto me da todas las imágenes que están actualmente disponibles para las diferentes arquitecturas que tenemos aquí, así que voy a seleccionar Docker Hub y luego el repositorio que quiero. Solo voy a mirar la versión que quiero ahora mismo para implementar en Digital Ocean. La versión que quieres específicamente es Basic AMD64, que es solo la arquitectura de Linux que tienen para Digital Ocean, así que esta es la que vas a querer usar, así que específicamente el repositorio va a ser Uncle Code SLC Craw para AAI y luego la etiqueta de la imagen va a ser Basic AMD64 y puedo obtenerla mirando aquí mismo en dockerhub para que pueda ver que es Uncle Code SL Craw para AI. Uncle Code es el nombre de usuario de GitHub del creador de Crawl for AI. Por cierto, un tipo genial. Y luego sí, Basic AMD64 es la etiqueta que tenemos después de los dos puntos, así es como averiguamos estas dos cosas. Luego seguiré adelante y haré clic en siguiente. Ahora podemos identificar el recurso que Queremos y esto realmente depende de ti. Voy a hacer clic en editar para el tamaño del recurso aquí, puedes ir tan barato como $ 5 al mes para que puedas obtener algunas plataformas de aplicaciones muy baratas aquí en Digital Ocean, solo ten en cuenta que probablemente necesitarás más RAM y vcpus para ejecutar Crawl 4 AI CU nuevamente, está ejecutando un navegador completo bajo el capó, por lo que incluso podrías necesitar hasta 4 GB de RAM si estás creando un montón de páginas a la vez. Solo tenlo en cuenta y podemos ajustar las cosas en el flujo de trabajo N para tener tamaños de lote más pequeños y cosas para trabajar con instancias más pequeñas si lo deseas, así que solo voy a seleccionar este y son $ 50 al mes, pero solo voy a eliminarlo después del día, así que solo estoy pagando un par de dólares para preparar mi base de conocimientos y luego puedo eliminar esta plataforma de aplicaciones y es muy fácil implementarla nuevamente si quiero y moveré los contenedores a uno en lugar de dos, por lo que es solo una base de $ 50 al mes y luego para el puerto aquí mismo Necesito cambiarlo de 8080 al puerto real que tenemos para Docker, que es este 11235, así que seguiré adelante y pegaré eso allí y luego haré clic en guardar y eso es todo lo que tenemos que hacer para nuestra configuración de recursos, así que continúe y haga clic en siguiente y ahora lo único que realmente tenemos que definir aquí es nuestra variable de entorno para seguridad, así que nuevamente podríamos no incluir esta variable de entorno para que nuestro punto final de API para rastreo para IA no esté protegido, pero definitivamente quiero incluir esa protección aquí y los animo a hacer lo mismo, y entonces nuestra variable de entorno se llama token de API de rastreo para IA, así que voy a agregar esto como una variable de entorno global aquí y luego el valor será el que desee que sea su token de portador, así que solo voy a decir prueba desactivada, obviamente voy a eliminar esto después de grabar este video, así que solo estoy codificando esto, lo estoy haciendo visible para ustedes, este será mi token de portador para mi punto final de rastreo para IA, así que seguiré adelante y haré clic en siguiente, tenemos nuestras variables de entorno definidas ahora y luego podemos revisar nuestro información, asegúrese de que todo se vea bien, lo cual seguro hace, y luego continúe y haga clic en crear recurso y es así de fácil, en un par de minutos aquí irá y realmente construirá este contenedor para mí, configurará todo en mi entorno en solo un par de minutos tendremos Crawl for AI disponible para nosotros y la plataforma de la aplicación aquí en Digital Ocean nos brinda automáticamente un punto final https completo protegido por SSL listo para que lo accedamos dentro de nuestra na y flujo de trabajo, allí vamos, solo un par de minutos después estamos completamente en vivo y tenemos esta URL para que accedamos a la implementación de nuestra aplicación ahora y, de hecho, si voy a la página de inicio, simplemente me lleva directamente a la página de inicio de la documentación de Crawl for AI y también puedo llegar al punto final de salud aquí, que es solo una parte del contenedor y puedo ver sí, tengo un estado de saludable que me da algunas cosas para el uso de la CPU y la memoria también es súper ordenado, tan fácil de configurar esto con Digital Ocean y nuevamente, muchas otras plataformas para hacerlo. Solo te estoy mostrando aquí como un ejemplo de algo que recomendaría, también todo esto es opcional si solo vas a ejecutar el contenedor tú mismo o donde Estás alojando n8n y hablaré sobre cómo puedes usarlo también en el flujo de trabajo de n8n, cómo ajustarías la URL para que funcione con el host local y la documentación para la implementación de Docker también lo cubre porque te muestra, quiero decir, esta es la URL que usarías en lugar de lo que te voy a mostrar cuando simplemente lo tienes alojado junto con tu instancia de NN o en tu computadora localmente, así que con eso podemos sumergirnos en nuestro flujo de trabajo final de N8 y realmente aprovechar nuestro nuevo rastreo para el punto final de IA. El patrocinador del video de hoy es 10 agent, un marco de código abierto que te ayuda a crear agentes de IA interactivos de voz en unos 20 minutos, imagina tener tu propio agente de IA que puede verte y entenderte a través de tu cámara y tu pantalla en tiempo real y procesar comandos de voz en paralelo y esto no es solo un producto, es una herramienta de código abierto para que tú, como desarrollador, construyas estos agentes tú mismo con estas capacidades, eso es lo que me emocionó tanto de asociarme con ellos. 10 admite un montón de módulos prediseñados realmente útiles para voz a texto (llm) y texto a voz, por lo que es muy fácil iterar en su agente a medida que lo construye y puede elegir cualquier tipo de modelo que desee, como Open AI en tiempo real o Gemini 2.0, y se integran con muchas cosas diferentes como RTC, lo que hace que sea muy fácil para usted configurar su agente con baja latencia y buena interrumpibilidad, que son muy importantes para la aplicación en tiempo real y este es el tipo de cosas que no son fáciles de configurar desde cero, así que en su campo de juego en el agente. el. Podemos probar 10 agentes ahora mismo, es genial, y esta es una plataforma para desarrolladores diseñada para ayudarte a crear agentes. Así que lo que estamos viendo aquí no es un producto, es solo una prueba de concepto básica para mostrar lo que podemos crear con nuestros agentes. Voy a activar el sonido. Ya estoy compartiendo mi pantalla mostrando la página de NN H. Mira esta pequeña conversación. Es genial cómo puedo ver mi pantalla en tiempo real. En una oración, descríbeme lo que ves en mi pantalla. Bien, veo el sitio web n8n, que se describe como una plataforma segura de automatización de flujo de trabajo nativa de IA para equipos técnicos. Bien, estoy compartiendo una nueva pantalla. Ahora dime qué ves aquí. En una oración, en una oración, veo una plantilla de flujo de trabajo para que un agente de IA chatee con archivos en un almacenamiento basado en Super. Muy bien, muy genial. Si quieres configurar las cosas tú mismo, definitivamente puedes hacerlo para la conversión de voz a texto o texto a voz de LLM cuando lo alojas localmente y tienes acceso al área de juegos de 10 agentes allí mismo. Para los desarrolladores, 10 admite C++, Go Python y Node.js. funciona sin problemas en cualquier sistema operativo y dispositivos móviles si está interesado en crear agentes de IA multimodales o simplemente aprender más sobre ellos, definitivamente revisaría el repositorio de GitHub que habré vinculado en la descripción de este video y este realmente es un marco de código abierto único en su tipo por lo que he visto, así que estoy muy impresionado, definitivamente recomiendo revisarlo. Todo este flujo de trabajo de n8n que estoy a punto de repasar con usted ahora mismo lo tengo en un repositorio de GitHub que habré vinculado en la descripción de este video para que pueda descargarlo usted mismo ahora mismo, llevarlo a su propia instancia de NN y luego personalizarlo al contenido de su corazón para hacer lo que quiera para crawl para IA porque tenga en cuenta lo que estoy construyendo aquí con usted, esto es solo una prueba de concepto para sentar las bases para que use crawl para IA dentro de n8n y el resto de las cosas en ese repositorio de GitHub que es el código de Python, la versión de Python de lo que estamos construyendo aquí mismo, así que definitivamente échele un vistazo también si tiene curiosidad y Además, la versión en Python de este agente, el experto en inteligencia artificial de Pantic, la he alojado para usted ahora mismo en el estudio del agente en vivo de Automator para que pueda ir al estudio. Automatizador Doai: Obtienes tokens gratis para jugar con estos agentes. Puedes ir al experto en IA de Pantic y probarlo ahora mismo sin tener que ejecutar nada. Puedes conversar con él para probarlo. Consulta la documentación de IA de Pantic y comprueba en qué te estás metiendo. Construyendo esto conmigo. Es la versión de Python de Pt. No es exactamente lo mismo que estamos construyendo aquí, pero tiene una funcionalidad muy similar. Vamos a construir nuestro flujo de trabajo N8N. Lo primero que queremos hacer es obtener una lista de URL que queremos rastrear con Crawl for AI. En este caso específico, lo hacemos a través del archivo Sitemap.xml que mostré antes para la documentación de IA de Pantic. Haré clic en " Probar paso" aquí mismo. Haré esto para cada paso del proceso. Así que podemos profundizar en cómo funciona este proceso. Y sí, obtengo el XML. Lo que vemos aquí es lo que tenemos. la salida de nuestro primer nodo y luego queremos convertir esto en Jong, esto es mucho mejor para trabajar dentro de n8n, así que ahora tenemos una lista de todas las URL para la documentación de Pantic AI ahora lo siguiente que tenemos que hacer si nota esto aquí, n8n todavía piensa que este es un solo elemento, pero realmente queríamos reconocer cada uno de estos aquí como un elemento separado, cada uno con una URL para una página para raspar y eso es lo que hace este nodo aquí, nos dividimos en función de este valor aquí, por lo que es el conjunto de URL. url y los estamos convirtiendo todos en elementos individuales, así que ahora n8n realmente los reconoce a cada uno como elementos individuales en una lista para que podamos recorrer cada uno de ellos ahora y entonces haré clic en el paso de prueba y hay un pequeño error en n8n donde a veces no mostrará el resultado cuando estés ejecutando el paso de prueba para solucionar esto solo tienes que actualizar así que podría simplemente cortar eso pero quería mostrar eso porque probablemente te va a pasar también mientras juegas con flujos de trabajo de na así que pensé que sería útil así que simplemente actualizas y luego haces clic en el paso de prueba para el paso actual del que quieres ver el resultado y entonces ese es este bucle aquí entonces cuando hacemos clic en el paso de prueba en un bucle solo nos mostrará el primer elemento en un bucle que es la página de inicio de la documentación de pantic AI y luego el segundo elemento sería la segunda página que vemos aquí el tercer elemento sería este y así sucesivamente y entonces sí tenemos la ubicación aquí y la última modificación como el primer elemento en nuestro Bucle y luego, por cierto, aquí es donde también puedes actualizar el tamaño del lote, así que si quieres procesar cinco URL o 10 URL al mismo tiempo, puedes ajustar esto aquí mismo. Solo asegúrate de que tu instancia pueda manejarlo porque recuerda que Crawl for AI puede consumir mucha CPU y RAM, así que ten mucho cuidado con esto y me estoy quedando con un tamaño de lote de uno para simplificarlo. Y creo que el resto de este flujo de trabajo en realidad tendría que modificarse un poco si tienes un tamaño de lote mayor que uno solo. Así que, de nuevo, esto es solo una aprobación del concepto para que comiences aquí con Crawl for AI. Quiero que ese sea el foco de este video, así que tenemos este elemento aquí mismo. Ahora podemos pasar al resto de este flujo de trabajo. El resto de lo que vemos aquí mismo se está ejecutando dentro del bucle, así que para cada uno de los 49 elementos que extrajimos aquí mismo, vamos a ejecutar todo para cada uno de ellos porque eso es lo que vamos a hacer para raspar y luego poner en super base para rag, así que estamos haciendo nuestra primera solicitud de API para Crawl 4 AI ahora y así Puedes ver que la URL que tengo aquí coincide exactamente con la que tenemos en la aplicación que configuramos en Digital Ocean. Si alojas Crawl for AI localmente, si ejecutas el contenedor Docker en tu computadora o en la misma instancia donde alojas N8N, solo tendrás que cambiar esto al puerto de host local 11235 y luego actualizarlo también a http. Volviendo a la página de implementación de Docker para Crawl for AI, si me desplazo hacia abajo hasta el primer ejemplo, en realidad te muestran esta URL. Si alojas el contenedor Docker en tu propia infraestructura, simplemente haciendo referencia a él con Local Host, esta es exactamente la URL que usarías en lugar de la que estoy usando aquí para conectarte a Digital Ocean. Quiero cubrir esas diferentes formas en que podrías alojar Crawl for AI cuando lo haces referencia en N8N. Esa es la URL. Y luego, para la autenticación, aquí mismo, si vuelvo a la documentación, la configuramos específicamente, así que tenemos este token de portador aquí. Volviendo a Digital Ocean, voy a mis variables de entorno. Aquí tengo configurada esta variable de entorno. Para el token de oso, solo tengo el valor de prueba desactivada y, volviendo a n8n, configuramos un tipo de credencial genérica y la tenemos como encabezado desactivado y la forma en que desea configurar esta credencial aquí, simplemente crearé nuevas credenciales para mostrarle un ejemplo, el nombre tiene que ser autorización porque ese es el nombre del encabezado para un token de portador y luego el valor aquí. Voy a ir a una vista de expresión para que pueda ver lo que estoy escribiendo. Debe escribir error y luego un espacio y luego cualquier valor que configure en su entorno aquí mismo o simplemente lo que especifique aquí mismo cuando ejecute el contenedor si lo está alojando localmente, así que dije prueba o para mi valor. Entonces, dentro de n8n es espacio de portador y luego prueba desactivada, así que voy a cerrar esto y voy a actualizar porque ya lo tengo configurado, así que déjeme volver a este nodo aquí mismo. Muy bien, tenemos nuestra URL, descubrimos nuestras credenciales y ahora tenemos el cuerpo de nuestra solicitud y, por lo tanto, nuevamente, volviendo a la documentación, no estoy haciendo nada. elegante aquí estoy siguiendo mucho la documentación y mostrándole cómo incorporarlo en n8n, así que el Json, esta es nuestra carga útil, tenemos dos valores aquí, en primer lugar tenemos las URL y no sé por qué esto es plural, solo da una URL aquí, de hecho, lo intenté con varias separadas por comas y no funcionó, así que solo da una URL aquí que desea raspar y luego también tiene una prioridad porque el rastreo para el contenedor Docker de AI admite la ejecución de todo esto en cu, por lo que le da los sitios que desea raspar en señales y puede definir prioridades para que pueda definir el orden en el que sus solicitudes son realmente manejadas por su rastreo para la implementación de AI No voy a entrar en eso aquí porque quiero mantener las cosas simples, pero solo sepa que hay tanta documentación que tienen aquí para tantas cosas que puede hacer para personalizar su rastreo para la implementación de AI también Lo mantendré simple, pero sí, solo sepa que hay mucho disponible para usted y, por lo tanto, volviendo a n8n para las URL del cuerpo, solo tengo el json. ubicación para que extraiga la URL del último nodo que se ejecuta y luego tenemos la prioridad de 10, así que no estoy usando esto aquí, pero realmente solo les estoy mostrando que estoy siguiendo la documentación exactamente al incluir esto aquí, así que sigamos adelante y ejecutemos esto y lo que obtenemos aquí no es exactamente lo que esperaríamos, obtenemos un ID de tarea, ¿qué diablos es una idea de tarea? Pensé que iba a raspar y me daría el marcado del HTML. Bueno, veamos la documentación, no nos adelantemos aquí, así que cuando raspamos la URL de rastreo de SL, no cuando la raspamos cuando la llamamos para que raspe por nosotros, nos devolverá un ID de tarea y la razón por la que hacemos esto es porque raspar un sitio web puede tomar más tiempo de lo que normalmente tomaría una solicitud HTTP, por lo que esencialmente está iniciando una tarea y luego se le da un ID de tarea para que pueda verificar el estado de la tarea más tarde para que pueda hacer ping a este punto final, así que en lugar de rastreo de SL, es una tarea de barra y Entonces, el ID de la tarea para esencialmente preguntarle a mi instancia de rastreo de IA sobre el estado de esta tarea de raspado específica que le di y, por lo tanto, si salgo de aquí y hago clic en esperar, esperará 5 segundos y luego tengo esta segunda solicitud que llamará al punto final de la tarea con el ID que obtuve de la primera solicitud para volver a preguntarle a Crawl for AI cuál es el estado de esta solicitud y si la solicitud se cumple, obtendremos el HTML y el enlace a todas las imágenes y el markdown y todo eso, así que déjame actualizar NN, nos topamos con ese pequeño error extraño nuevamente, así que voy a actualizar n8n, volveré y comenzaré desde este nodo aquí, así que haré clic en el paso de prueba y tendremos que esperar 5 segundos para ese paso de espera, pero iniciará la tarea de rastreo y luego, aquí mismo, hacemos ping a la tarea en función del ID de tarea que obtuvimos de nuestra primera solicitud HTTP, así que sí, obtenemos esa espera de 5 segundos y luego verificamos el estado y en 5 segundos la tarea de rastreo ya se completó, así que obtenemos instantáneamente todo lo que se extrae para nosotros, así que tenemos la URL, el HTML, hay un montón de información aquí porque nos da todos los enlaces y nos da el markdown si voy hasta el final, tenemos todo el markdown aquí, así que tenemos todo lo que podríamos querer de esa tarea de raspado y luego también tenemos el estado aquí, que está completado en este momento, pero si esto todavía se estaba procesando y creo que probablemente te lo mostraré más adelante también, dirá pendiente o procesando en lugar de completado y luego, justo después de esta solicitud aquí, tenemos esta declaración if donde básicamente verificamos si el estado está completado y si no está completado, si la declaración if es falsa, entonces lo que queremos hacer es retroceder, esperar otros 5 segundos y luego verificar nuevamente, así que estamos en este bucle infinito aquí, básicamente cada 5 segundos preguntando a nuestro rastreo por la implementación de IA, ¿la tarea está completa? ¿La tarea está completa todavía? ¿La tarea está completa todavía? y luego, finalmente, una vez que el estado esté completado, para que esta declaración if sea verdadera y luego podemos hacer el último paso aquí, que es tomar el markdown que extrajimos de este trabajo que ejecutamos y agregarlo a nuestra base de conocimiento, así que voy a hacer clic en esta tarea aquí mismo, llegó a la rama verdadera porque el estado está completado y ahora eso nos guiará al paso final de insertar en nuestra base de conocimiento, finalmente hemos llegado a la súper base ahora, lo cual es súper emocionante, así que para el modelo de incrustación, solo estoy usando la incrustación de texto de Open AI 3 pequeños conectados, mi clave API, es súper fácil de hacer, por cierto, solo tienes que hacer clic en abrir documentos aquí mismo y te guiará a través de cómo obtener tu clave API, que básicamente puedes hacer para cualquier cosa en NN, simplemente hace que sea muy fácil obtener todas tus credenciales y luego, para el cargador de datos, solo estoy usando el cargador de datos predeterminado y luego los datos específicamente los obtengo por resultado. Markdown, por lo que desde la salida del último nodo es JSON. res. markdown y luego también para los metadatos aquí no es súper importante, pero estoy especificando la página de donde proviene este fragmento, lo cual es bueno tenerlo en la base de datos para poder ver exactamente de qué página de la documentación de Pantic AI proviene este fragmento específico que se creó y luego para el divisor, solo estoy usando un divisor de texto con un tamaño de fragmento de 5000. No pensé mucho en cómo estoy fragmentando mis datos para rag aquí, definitivamente puedes jugar con esto para tu caso de uso específicamente y ver qué obtiene los mejores resultados para ti y luego para el nodo en sí con superbase, todo lo que tuve que hacer para conectar mi cuenta de superbase es simplemente darle mi host, que es Mi URL de superbase y luego mi secreto de servicio R, así que tengo este proyecto de superbase creado aquí mismo. Simplemente bajé en la parte inferior aquí a la configuración de mi proyecto a la pestaña API, copié mi URL que está aquí y luego revelé y copié mis secretos de servicio rooll, esas son las dos cosas para tus credenciales de superbase y luego la única otra cosa que tienes que hacer es configurar su tabla para almacenar la base de conocimiento en subit base y la forma en que puede hacerlo es que tienen una plantilla de inicio muy buena para usted. Si hago clic en los documentos aquí en el nodo de almacenamiento vectorial de Superbase y luego me desplazo hacia abajo hasta un inicio rápido para configurar su almacenamiento vectorial, hago clic aquí, encuentro este SQL para ejecutar aquí, copio todo comenzando en la línea 4 porque la extensión vectorial ya está habilitada en Superbase de forma predeterminada, simplemente copie esto aquí y luego vaya a Superbase, vaya al editor de SQL y luego pegue todo aquí, así que va a pegar esto y luego haga clic en ejecutar en la parte inferior derecha aquí y luego eso creará automáticamente su tabla de documentos y esta función de coincidencia de documentos, que es lo que realmente realiza el registro en su base de conocimiento de Superbase, así que ahora en el editor de tablas tengo esta tabla de documentos aquí que actualmente está vacía porque no he hecho nada para ejecutar realmente este flujo de trabajo, pero esa es la configuración completa de Superbase, así que me tomé un par de minutos para mostrar eso porque es muy importante asegurarse de que sabe cómo hacerlo y ahora podemos probar el paso, así que ejecutémoslo aquí y ¡Boom! Ahí vamos. Tenemos nuestros primeros tres elementos. Tomó la página de inicio de Pantic AI, la dividió en tres partes e insertó cada una en nuestra base de conocimiento. Haré clic en Actualizar. Tenemos el contenido de cada una y las incrustaciones. Podemos hacer la búsqueda con las matemáticas vectoriales. Tenemos los metadatos. También indica la página de la que proviene, que es la pequeña información que agregué. Eso es todo para este flujo de trabajo. Ahora, lo que tenemos que hacer es eliminar estos registros para empezar de cero. Ejecutaré este flujo de trabajo para procesar todo a la vez. Guardaré esto. Haré clic en Probar flujo de trabajo. Mira, ya está funcionando. Está haciendo la primera solicitud. Espera un poco. Parece que se completó. Hizo la primera tarea. Obtuvo los primeros tres fragmentos en la base de conocimiento. Ahora vuelve a funcionar. ¡ Boom! Ahí vamos. Tiene ocho. Entonces, si actualizo aquí, deberíamos tener ocho elementos en nuestra base de conocimientos. Miren eso porque ya procesa dos páginas. Así que lo que voy a hacer aquí es pausar y volver una vez que haya hecho todo porque tomará un poco de tiempo completar las 49 páginas. Bien, el procesamiento está completo. Me impacienté un poco, así que lo corté en 30. Solo porque quiero llegar a la última parte de mi video para ustedes aquí. Pero sí, pueden ver que ejecutó esta parte del bucle 56 veces porque algunas veces todavía estaba procesando la solicitud. De hecho, pueden hacer clic en la declaración if para ver que quiero mostrarles esto muy rápido. Así que aquí el estado está completo. Parece que falló para esta página. Déjenme ir a otra rápidamente. Aquí, eh, vamos. Volvamos aquí mismo. Es una rama verdadera porque el estado está completo. Extrajo todo correctamente. Pero luego, en otra ejecución como esta, vamos a la rama falsa porque el estado todavía se está procesando. Esto muestra que no todo el tiempo va a ser después. 5 segundos inmediatamente disponibles para nosotros, por eso tenemos que entrar en este bucle y, a veces, hacer ping una segunda o tercera vez después de 10 o 15 segundos porque tenemos que esperar a que termine esa tarea, así que ahora tenemos los 148 vectores insertados en nuestra base de datos, así que si me desplazo hacia abajo aquí, de hecho, puedes ver aquí mismo los 148 registros que tenemos en superbase y ahora podemos ir a nuestro agente para chatear con nuestra base de conocimientos. No voy a repasar esto porque es súper simple. Solo tengo el disparador de recepción de mensajes de chat y luego tengo el nodo del agente de IA. Ni siquiera tengo un mensaje del sistema ni nada aquí, así que es súper básico. Tengo mi historial de chat de superbase. Tengo mi modelo GPT 40 Mini y luego conecté la herramienta de almacenamiento de vectores de superbase para rag, así que es muy básico, solo uso GPT tanto para mi modelo de incrustación como para mi llm. Ahora abramos un chat, charlemos con esto y le haré una pregunta. Solo podría responder si realmente tiene la documentación de p pantic AI en su base de conocimiento, así que diré cuáles son los modelos compatibles con pantic AI. Bien, y entonces verá que comenzará con Open AI, irá a la herramienta de recuperación de almacén de vectores, procesará la solicitud y luego la enviará de vuelta al LLM para darme la respuesta final y esto es absolutamente perfecto y podemos hacer clic en Super Base aquí para ver la respuesta. Estos son todos los fragmentos que nos fueron devueltos para que el LLM los razone. Y sí, tiene todos los modelos aquí, esto es perfecto y luego, si hago otra pregunta como, ¿cómo se manejan las respuestas de la herramienta? Veremos si puede extraer de la página de herramientas esta vez, así que debe asegurarse de que realmente esté recuperando de las páginas correctas de la documentación dentro de la base de conocimiento aquí, así que haré clic aquí, desplácese hacia abajo hasta llegar a la URL y ahí vamos. Sí, va a la página / herramientas aquí mismo para obtener la respuesta a nuestra pregunta. Se ve bien, está bien, perfecto, esto está funcionando absolutamente de maravilla y, de nuevo, esto es mucho más Una prueba de concepto, pero te muestra cómo puedes implementar Crawl for AI con Docker y usarlo a través de un punto final de API en n8n para que ni siquiera tengas que usar pip y todo el paquete porque no puedes hacer eso con una N así que sí, súper genial, espero que puedas tomar esto, extenderlo a tus casos de uso y hacer algunas cosas increíbles con él así que ahí lo tienes, hemos integrado Crawl for AI directamente en n8n, lo que nos da un enfoque súper simple y completamente sin código para rastrear cualquier sitio web para Rag y ya sea que estés alojando Crawl for AI localmente o estés implementando un contenedor Docker en la nube como lo hice con Digi Ocean, sigue siendo exactamente el mismo proceso dentro de n8n para aprovechar Crawl for AI súper súper simple y es importante tener en cuenta que esto realmente es una prueba de concepto, este flujo de trabajo está destinado a darte la base para realmente combinar estas dos plataformas juntas de maneras increíbles así que definitivamente revisa la documentación de Crawl for AI si quieres ver cómo hacer las cosas más eficientes o configurar tu instancia de Crawl for AI y también hay muchas oportunidades para mejorar cosas como convertir nuestra IA agente en un enfoque de trapo agentico como lo hice con mi otro video con python, así que hay muchas cosas para que avance, pero espero que esto lo ayude a comenzar. Si apreció este video, realmente apreciaría un me gusta y una suscripción y con eso lo veré en el próximo video.

